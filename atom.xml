<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>不会魔法的小圆</title>
  
  <subtitle>世界如此可爱</subtitle>
  <link href="https://anti-entrophic.github.io/atom.xml" rel="self"/>
  
  <link href="https://anti-entrophic.github.io/"/>
  <updated>2023-12-11T08:42:23.018Z</updated>
  <id>https://anti-entrophic.github.io/</id>
  
  <author>
    <name>不会魔法的小圆</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>FDU Computer Graphics pj2</title>
    <link href="https://anti-entrophic.github.io/posts/10020.html"/>
    <id>https://anti-entrophic.github.io/posts/10020.html</id>
    <published>2023-12-11T08:30:56.000Z</published>
    <updated>2023-12-11T08:42:23.018Z</updated>
    
    <content type="html"><![CDATA[<div class="note success simple"><p>薪火相传</p></div><p>距离有点久远了，唉可惜自己比较懒，没能在做完作业后立马补一篇博客。现在差不多都忘啦，还是有点不好做，就记录下答案吧。pj3还要用webgl，有什么想起来的可能会记录一下。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 顶点着色器</span></span><br><span class="line"><span class="keyword">var</span> <span class="variable constant_">VSHADER_SOURCE</span> = </span><br><span class="line">    <span class="string">&#x27;attribute vec4 a_Position;\n&#x27;</span> + <span class="comment">// attribute 变量</span></span><br><span class="line">    <span class="string">&#x27;uniform mat4 u_ModelMatrix;\n&#x27;</span> + <span class="comment">//变换矩阵</span></span><br><span class="line">    <span class="string">&#x27;attribute vec4 a_Color;\n&#x27;</span> +</span><br><span class="line">    <span class="string">&#x27;varying vec4 v_Color;\n&#x27;</span> + <span class="comment">// varying 变量</span></span><br><span class="line">    <span class="string">&#x27;uniform int u_RenderMode;\n&#x27;</span> + <span class="comment">// 新增 uniform 变量，用于区分绘制模式(图形or边框线)</span></span><br><span class="line">    <span class="string">&#x27;void main() &#123;\n&#x27;</span> +</span><br><span class="line">    <span class="string">&#x27;    gl_Position = u_ModelMatrix * a_Position;\n&#x27;</span> + <span class="comment">// 设置顶点坐标</span></span><br><span class="line">    <span class="string">&#x27;    if(u_RenderMode == 0) &#123;\n&#x27;</span> +</span><br><span class="line">    <span class="string">&#x27;        v_Color = a_Color;\n&#x27;</span> +  <span class="comment">// 传递给片元着色器</span></span><br><span class="line">    <span class="string">&#x27;    &#125;\n&#x27;</span> + </span><br><span class="line">    <span class="string">&#x27;    else &#123;\n&#x27;</span>+</span><br><span class="line">    <span class="string">&#x27;        v_Color = vec4(1.0, 0.0, 0.0, 1.0);\n&#x27;</span> +</span><br><span class="line">    <span class="string">&#x27;    &#125;\n&#x27;</span> +</span><br><span class="line">    <span class="string">&#x27;&#125;\n&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 片元着色器</span></span><br><span class="line"><span class="keyword">var</span> <span class="variable constant_">FSHADER_SOURCE</span> =</span><br><span class="line">    <span class="string">&#x27;precision mediump float;\n&#x27;</span> + <span class="comment">// 精度</span></span><br><span class="line">    <span class="string">&#x27;varying vec4 v_Color;\n&#x27;</span> +    <span class="comment">// 接收varying变量</span></span><br><span class="line">    <span class="string">&#x27;void main() &#123;\n&#x27;</span> +</span><br><span class="line">    <span class="string">&#x27;  gl_FragColor = v_Color;\n&#x27;</span> +</span><br><span class="line">    <span class="string">&#x27;&#125;\n&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 旋转速度（度/秒）</span></span><br><span class="line"><span class="keyword">var</span> <span class="variable constant_">ANGLE_STEP</span> = <span class="number">45.0</span>;</span><br><span class="line"><span class="comment">// 缩放速度</span></span><br><span class="line"><span class="keyword">var</span> <span class="variable constant_">SCALE_SPEED</span> = <span class="number">0.2</span>;</span><br><span class="line"><span class="comment">// 缩放方向（变大or变小）</span></span><br><span class="line"><span class="keyword">var</span> <span class="variable constant_">SCALE_DIRECTON</span> = -<span class="number">1</span>;</span><br><span class="line"><span class="comment">// 是否开启动画</span></span><br><span class="line"><span class="keyword">var</span> <span class="variable constant_">ANIMATION</span> = <span class="literal">false</span>;</span><br><span class="line"><span class="comment">// 避免动画开始的瞬移 </span></span><br><span class="line"><span class="keyword">var</span> init = <span class="literal">false</span>;</span><br><span class="line"><span class="comment">// 判断是否正拖动点</span></span><br><span class="line"><span class="keyword">var</span> isDragging = <span class="literal">false</span>;</span><br><span class="line"><span class="comment">// 当前正在拖动的点</span></span><br><span class="line"><span class="keyword">var</span> <span class="title class_">DraggingVertex</span> = -<span class="number">1</span>;</span><br><span class="line"><span class="comment">// 是否显示边框</span></span><br><span class="line"><span class="keyword">var</span> frame = <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 三角形当前的旋转角度</span></span><br><span class="line"><span class="keyword">var</span> currentAngle = <span class="number">0.0</span>;</span><br><span class="line"><span class="keyword">var</span> currentScale = <span class="number">1.0</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">main</span>(<span class="params"></span>) &#123;</span><br><span class="line">    <span class="keyword">var</span> canvas = <span class="variable language_">document</span>.<span class="title function_">getElementById</span>(<span class="string">&quot;webgl&quot;</span>);</span><br><span class="line">    <span class="keyword">if</span>(!canvas) &#123; <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&quot;Fail to load canvas&quot;</span>); <span class="keyword">return</span> <span class="literal">false</span>;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置画布大小</span></span><br><span class="line">    canvas.<span class="title function_">setAttribute</span>(<span class="string">&quot;width&quot;</span>, canvasSize.<span class="property">maxX</span>);</span><br><span class="line">    canvas.<span class="title function_">setAttribute</span>(<span class="string">&quot;height&quot;</span>, canvasSize.<span class="property">maxY</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将canvas坐标转换为webgl坐标</span></span><br><span class="line">    <span class="keyword">var</span> rect = canvas.<span class="title function_">getBoundingClientRect</span>();</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">var</span> i=<span class="number">0</span>, len = vertex_pos.<span class="property">length</span>; i&lt;len; i++) &#123;</span><br><span class="line">        vertex_pos[i][<span class="number">0</span>] = ((vertex_pos[i][<span class="number">0</span>] - rect.<span class="property">left</span>) - canvas.<span class="property">width</span> / <span class="number">2</span>) / (canvas.<span class="property">width</span> / <span class="number">2</span>);</span><br><span class="line">        vertex_pos[i][<span class="number">1</span>] = (canvas.<span class="property">height</span> / <span class="number">2</span> - (vertex_pos[i][<span class="number">1</span>] - rect.<span class="property">top</span>)) / (canvas.<span class="property">height</span> / <span class="number">2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 获取webgl对象</span></span><br><span class="line">    <span class="keyword">var</span> gl = <span class="title function_">getWebGLContext</span>(canvas);</span><br><span class="line">    <span class="keyword">if</span>(!gl) &#123; <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&quot;Fail to get the rendering context for WebGL&quot;</span>); <span class="keyword">return</span> <span class="literal">false</span>;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化着色器</span></span><br><span class="line">    <span class="keyword">var</span> shader_main = <span class="title function_">initShaders</span>(gl, <span class="variable constant_">VSHADER_SOURCE</span>, <span class="variable constant_">FSHADER_SOURCE</span>);</span><br><span class="line">    <span class="keyword">if</span> (!shader_main) &#123;</span><br><span class="line">        <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&#x27;Failed to intialize shaders.&#x27;</span>);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建缓冲区对象</span></span><br><span class="line">    <span class="keyword">var</span> n = <span class="title function_">initVertexBuffers</span>(gl, canvas, rect);</span><br><span class="line">    <span class="keyword">if</span> (n &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&#x27;Failed to set the positions of the vertices&#x27;</span>);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置背景色</span></span><br><span class="line">    gl.<span class="title function_">clearColor</span>(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取渲染模式uniform变量</span></span><br><span class="line">    <span class="keyword">var</span> u_RenderMode = gl.<span class="title function_">getUniformLocation</span>(gl.<span class="property">program</span>, <span class="string">&#x27;u_RenderMode&#x27;</span>);</span><br><span class="line">    <span class="keyword">if</span> (!u_RenderMode) &#123; </span><br><span class="line">        <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&#x27;Failed to get the storage location of u_RenderMode&#x27;</span>);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取旋转矩阵uniform变量</span></span><br><span class="line">    <span class="keyword">var</span> u_ModelMatrix = gl.<span class="title function_">getUniformLocation</span>(gl.<span class="property">program</span>, <span class="string">&#x27;u_ModelMatrix&#x27;</span>);</span><br><span class="line">    <span class="keyword">if</span> (!u_ModelMatrix) &#123; </span><br><span class="line">        <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&#x27;Failed to get the storage location of u_ModelMatrix&#x27;</span>);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建旋转矩阵</span></span><br><span class="line">    <span class="keyword">var</span> modelMatrix = <span class="keyword">new</span> <span class="title class_">Matrix4</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 动画效果</span></span><br><span class="line">    <span class="keyword">var</span> tick = <span class="keyword">function</span>(<span class="params"></span>) &#123;</span><br><span class="line">        <span class="keyword">var</span> currentStage = <span class="title function_">animate</span>(currentAngle, currentScale);</span><br><span class="line">        currentAngle = currentStage[<span class="number">0</span>];  <span class="comment">// 更新旋转角度</span></span><br><span class="line">        currentScale = currentStage[<span class="number">1</span>];  <span class="comment">// 更新缩放大小</span></span><br><span class="line"></span><br><span class="line">        <span class="title function_">draw</span>(gl, n, currentAngle, currentScale, modelMatrix, u_ModelMatrix, u_RenderMode);   <span class="comment">// 渲染图形</span></span><br><span class="line">        <span class="keyword">if</span>(<span class="variable constant_">ANIMATION</span>)</span><br><span class="line">            <span class="title function_">requestAnimationFrame</span>(tick, canvas); <span class="comment">// 请求再次调用tick</span></span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="title function_">draw</span>(gl, n, currentAngle, currentScale, modelMatrix, u_ModelMatrix, u_RenderMode);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 注册键盘点击事件</span></span><br><span class="line">    <span class="variable language_">document</span>.<span class="title function_">addEventListener</span>(<span class="string">&quot;keydown&quot;</span>, <span class="keyword">function</span>(<span class="params">event</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (event.<span class="property">key</span> === <span class="string">&quot;t&quot;</span> || event.<span class="property">key</span> === <span class="string">&quot;T&quot;</span>) &#123; <span class="comment">// 开始/暂停动画</span></span><br><span class="line">            <span class="variable constant_">ANIMATION</span> = !<span class="variable constant_">ANIMATION</span>;</span><br><span class="line">            <span class="keyword">if</span>(<span class="variable constant_">ANIMATION</span>) &#123;</span><br><span class="line">                init = <span class="literal">true</span>;</span><br><span class="line">                <span class="title function_">tick</span>(); <span class="comment">// 调用 tick 函数</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (event.<span class="property">key</span> === <span class="string">&quot;e&quot;</span> || event.<span class="property">key</span> === <span class="string">&quot;E&quot;</span>) &#123; <span class="comment">// 复位</span></span><br><span class="line">            <span class="variable constant_">ANIMATION</span> = <span class="literal">false</span>;</span><br><span class="line">            currentAngle = <span class="number">0.0</span>;</span><br><span class="line">            currentScale = <span class="number">1.0</span>;</span><br><span class="line">            init = <span class="literal">true</span>;</span><br><span class="line">            <span class="title function_">tick</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (event.<span class="property">key</span> === <span class="string">&quot;b&quot;</span> || event.<span class="property">key</span> === <span class="string">&quot;B&quot;</span>) &#123; <span class="comment">// 显示边框</span></span><br><span class="line">            frame = !frame;</span><br><span class="line">            init = <span class="literal">true</span>;</span><br><span class="line">            <span class="title function_">tick</span>();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 注册鼠标点击事件</span></span><br><span class="line">    canvas.<span class="property">onmousedown</span> = <span class="keyword">function</span>(<span class="params">ev</span>)&#123; </span><br><span class="line">        isDragging = <span class="literal">true</span>;</span><br><span class="line">        <span class="title function_">click</span>(ev, gl, canvas, </span><br><span class="line">            currentAngle, currentScale,  <span class="comment">// 当前的状态，为了确定当前的点的位置</span></span><br><span class="line">            modelMatrix, u_ModelMatrix,</span><br><span class="line">            rect); </span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    canvas.<span class="property">onmousemove</span> = <span class="keyword">function</span>(<span class="params">ev</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (!isDragging) <span class="keyword">return</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="variable constant_">ANIMATION</span>) <span class="keyword">return</span>;</span><br><span class="line">        <span class="keyword">var</span> x = <span class="title function_">toW</span>(ev.<span class="property">clientX</span>, <span class="number">0</span>, canvas, rect); <span class="comment">// 鼠标x坐标</span></span><br><span class="line">        <span class="keyword">var</span> y = <span class="title function_">toW</span>(ev.<span class="property">clientY</span>, <span class="number">1</span>, canvas, rect); <span class="comment">// 鼠标y坐标</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 得到旋转矩阵</span></span><br><span class="line">        modelMatrix.<span class="title function_">setRotate</span>(currentAngle, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">        modelMatrix.<span class="title function_">scale</span>(currentScale, currentScale, currentScale);</span><br><span class="line">        <span class="comment">// 求逆</span></span><br><span class="line">        modelMatrix = modelMatrix.<span class="title function_">invert</span>();</span><br><span class="line">        <span class="comment">// 更新点（注意需要更新旋转前的坐标）</span></span><br><span class="line">        <span class="keyword">if</span>(<span class="title class_">DraggingVertex</span>!=-<span class="number">1</span>)&#123;</span><br><span class="line">            vertex_pos[<span class="title class_">DraggingVertex</span>][<span class="number">0</span>] = x * modelMatrix.<span class="property">elements</span>[<span class="number">0</span>] + y * modelMatrix.<span class="property">elements</span>[<span class="number">4</span>] + modelMatrix.<span class="property">elements</span>[<span class="number">12</span>];</span><br><span class="line">            vertex_pos[<span class="title class_">DraggingVertex</span>][<span class="number">1</span>] = x * modelMatrix.<span class="property">elements</span>[<span class="number">1</span>] + y * modelMatrix.<span class="property">elements</span>[<span class="number">5</span>] + modelMatrix.<span class="property">elements</span>[<span class="number">13</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 重新创建缓冲区对象</span></span><br><span class="line">        <span class="keyword">var</span> n = <span class="title function_">initVertexBuffers</span>(gl, canvas, rect);</span><br><span class="line">        <span class="keyword">if</span> (n &lt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&#x27;Failed to set the positions of the vertices&#x27;</span>);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="title function_">draw</span>(gl, n, currentAngle, currentScale, modelMatrix, u_ModelMatrix, u_RenderMode);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    canvas.<span class="property">onmouseup</span> = <span class="keyword">function</span>(<span class="params"></span>) &#123;</span><br><span class="line">        isDragging = <span class="literal">false</span>;</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">initVertexBuffers</span>(<span class="params">gl, canvas, rect</span>) &#123;</span><br><span class="line">    <span class="comment">// 创建缓冲区对象</span></span><br><span class="line">    <span class="keyword">var</span> vertexBuffer = gl.<span class="title function_">createBuffer</span>();</span><br><span class="line">    <span class="keyword">if</span>(!vertexBuffer) &#123;</span><br><span class="line">        <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&#x27;Failed to create the buffer object&#x27;</span>);</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将缓冲区对象绑定到目标</span></span><br><span class="line">    gl.<span class="title function_">bindBuffer</span>(gl.<span class="property">ARRAY_BUFFER</span>, vertexBuffer);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 向缓冲区对象中写入数据</span></span><br><span class="line">    <span class="keyword">var</span> n = <span class="number">4</span> * polygon.<span class="property">length</span>; <span class="comment">// 点的个数</span></span><br><span class="line">    <span class="keyword">var</span> vertices = <span class="keyword">new</span> <span class="title class_">Float32Array</span>(n * <span class="number">5</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">var</span> offset = <span class="number">0</span>; <span class="comment">// 偏移用于将每个四边形的顶点数据写入数组</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">var</span> i=<span class="number">0</span>, len = polygon.<span class="property">length</span>; i&lt;len; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">var</span> j=<span class="number">0</span>; j&lt;<span class="number">4</span>; j++) &#123;</span><br><span class="line">            vertices[offset++] = vertex_pos[polygon[i][j]][<span class="number">0</span>];</span><br><span class="line">            vertices[offset++] = vertex_pos[polygon[i][j]][<span class="number">1</span>];</span><br><span class="line">            vertices[offset++] = vertex_color[polygon[i][j]][<span class="number">0</span>]/<span class="number">255</span>;</span><br><span class="line">            vertices[offset++] = vertex_color[polygon[i][j]][<span class="number">1</span>]/<span class="number">255</span>;</span><br><span class="line">            vertices[offset++] = vertex_color[polygon[i][j]][<span class="number">2</span>]/<span class="number">255</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 向缓冲区写入数据</span></span><br><span class="line">    gl.<span class="title function_">bufferData</span>(gl.<span class="property">ARRAY_BUFFER</span>, vertices, gl.<span class="property">STATIC_DRAW</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> <span class="variable constant_">FSIZE</span> = vertices.<span class="property">BYTES_PER_ELEMENT</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取坐标attribute变量</span></span><br><span class="line">    <span class="keyword">var</span> a_Position = gl.<span class="title function_">getAttribLocation</span>(gl.<span class="property">program</span>, <span class="string">&#x27;a_Position&#x27;</span>);</span><br><span class="line">    <span class="keyword">if</span> (a_Position &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&#x27;Failed to get the storage location of a_Position&#x27;</span>);</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 将缓冲区对象分配给a_position变量</span></span><br><span class="line">    gl.<span class="title function_">vertexAttribPointer</span>(a_Position, <span class="number">2</span>, gl.<span class="property">FLOAT</span>, <span class="literal">false</span>, <span class="variable constant_">FSIZE</span> * <span class="number">5</span>, <span class="number">0</span>);</span><br><span class="line">    <span class="comment">// 连接a_Position变量与分配给它的缓冲区对象</span></span><br><span class="line">    gl.<span class="title function_">enableVertexAttribArray</span>(a_Position);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取颜色attribute变量</span></span><br><span class="line">    <span class="keyword">var</span> a_Color = gl.<span class="title function_">getAttribLocation</span>(gl.<span class="property">program</span>, <span class="string">&#x27;a_Color&#x27;</span>);</span><br><span class="line">    <span class="keyword">if</span>(a_Color &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&#x27;Failed to get the storage location of a_Color&#x27;</span>);</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    gl.<span class="title function_">vertexAttribPointer</span>(a_Color, <span class="number">3</span>, gl.<span class="property">FLOAT</span>, <span class="literal">false</span>, <span class="variable constant_">FSIZE</span> * <span class="number">5</span>, <span class="variable constant_">FSIZE</span> * <span class="number">2</span>);</span><br><span class="line">    gl.<span class="title function_">enableVertexAttribArray</span>(a_Color); </span><br><span class="line"></span><br><span class="line">    <span class="comment">// 解绑缓冲区</span></span><br><span class="line">    gl.<span class="title function_">bindBuffer</span>(gl.<span class="property">ARRAY_BUFFER</span>, <span class="literal">null</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> n;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">draw</span>(<span class="params">gl, n, currentAngle, currentScale, modelMatrix, u_ModelMatrix, u_RenderMode</span>) &#123;</span><br><span class="line">    <span class="comment">// 计算旋转矩阵</span></span><br><span class="line">    modelMatrix.<span class="title function_">setRotate</span>(currentAngle, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">    modelMatrix.<span class="title function_">scale</span>(currentScale, currentScale, currentScale);</span><br><span class="line">    <span class="comment">// 把旋转矩阵传给渲染器</span></span><br><span class="line">    gl.<span class="title function_">uniformMatrix4fv</span>(u_ModelMatrix, <span class="literal">false</span>, modelMatrix.<span class="property">elements</span>);</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 清除canvas</span></span><br><span class="line">    gl.<span class="title function_">clear</span>(gl.<span class="property">COLOR_BUFFER_BIT</span>);</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 画出图形</span></span><br><span class="line">    gl.<span class="title function_">uniform1i</span>(u_RenderMode, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">var</span> i=<span class="number">0</span>; i&lt;<span class="number">16</span>; i+=<span class="number">4</span>) &#123;</span><br><span class="line">        gl.<span class="title function_">drawArrays</span>(gl.<span class="property">TRIANGLE_FAN</span>, i, <span class="number">4</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(frame) &#123;</span><br><span class="line">        gl.<span class="title function_">uniform1i</span>(u_RenderMode, <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">var</span> i=<span class="number">0</span>; i&lt;<span class="number">16</span>; i+=<span class="number">4</span>) &#123;</span><br><span class="line">            gl.<span class="title function_">drawArrays</span>(gl.<span class="property">LINE_LOOP</span>, i, <span class="number">3</span>);</span><br><span class="line">            gl.<span class="title function_">drawArrays</span>(gl.<span class="property">LINE_LOOP</span>, i, <span class="number">4</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 上次调用tick函数的时间</span></span><br><span class="line"><span class="keyword">var</span> g_last = <span class="title class_">Date</span>.<span class="title function_">now</span>();</span><br><span class="line"><span class="keyword">function</span> <span class="title function_">animate</span>(<span class="params">angle, scale</span>) &#123;</span><br><span class="line">    <span class="comment">// 计算时间差距</span></span><br><span class="line">    <span class="keyword">var</span> now = <span class="title class_">Date</span>.<span class="title function_">now</span>();</span><br><span class="line">    <span class="keyword">var</span> elapsed;</span><br><span class="line">    <span class="keyword">if</span>(!init) &#123;</span><br><span class="line">        elapsed = now - g_last;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        elapsed = <span class="number">0</span>;</span><br><span class="line">        init = <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    g_last = now;</span><br><span class="line">    <span class="comment">// 根据时间间隔调整新角度</span></span><br><span class="line">    <span class="keyword">var</span> newAngle = (angle + (<span class="variable constant_">ANGLE_STEP</span> * elapsed) / <span class="number">1000.0</span>) % <span class="number">360</span>;</span><br><span class="line">    <span class="comment">// 根据时间间隔调整新缩放比例</span></span><br><span class="line">    <span class="keyword">var</span> update = scale + <span class="variable constant_">SCALE_DIRECTON</span> * (<span class="variable constant_">SCALE_SPEED</span> * elapsed) / <span class="number">1000.0</span>;</span><br><span class="line">    <span class="keyword">var</span> newSize;</span><br><span class="line">    <span class="keyword">if</span>(update &lt; <span class="number">0.2</span>) &#123;</span><br><span class="line">        <span class="variable constant_">SCALE_DIRECTON</span> *= -<span class="number">1</span>;</span><br><span class="line">        newSize = <span class="number">0.4</span> - update;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(update &gt; <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="variable constant_">SCALE_DIRECTON</span> *= -<span class="number">1</span>;</span><br><span class="line">        newSize = <span class="number">2</span> - update;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        newSize = update;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> [newAngle, newSize];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">click</span>(<span class="params">ev, gl, canvas, currentAngle, currentScale, modelMatrix, u_ModelMatrix, rect</span>) &#123;</span><br><span class="line">    <span class="keyword">var</span> x = <span class="title function_">toW</span>(ev.<span class="property">clientX</span>, <span class="number">0</span>, canvas, rect); <span class="comment">// 鼠标x坐标</span></span><br><span class="line">    <span class="keyword">var</span> y = <span class="title function_">toW</span>(ev.<span class="property">clientY</span>, <span class="number">1</span>, canvas, rect); <span class="comment">// 鼠标y坐标</span></span><br><span class="line">    <span class="comment">// 计算旋转矩阵</span></span><br><span class="line">    modelMatrix.<span class="title function_">setRotate</span>(currentAngle, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">    modelMatrix.<span class="title function_">scale</span>(currentScale, currentScale, currentScale);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> vertex_X;</span><br><span class="line">    <span class="keyword">var</span> vertex_Y;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> min_length = <span class="number">0.001</span>;</span><br><span class="line">    <span class="keyword">var</span> min_vertex = -<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">var</span> i=<span class="number">0</span>, len = vertex_pos.<span class="property">length</span>; i&lt;len; i++) &#123;</span><br><span class="line">        vertex_X = vertex_pos[i][<span class="number">0</span>] * modelMatrix.<span class="property">elements</span>[<span class="number">0</span>] + vertex_pos[i][<span class="number">1</span>] * modelMatrix.<span class="property">elements</span>[<span class="number">4</span>] + modelMatrix.<span class="property">elements</span>[<span class="number">12</span>];</span><br><span class="line">        vertex_Y = vertex_pos[i][<span class="number">0</span>] * modelMatrix.<span class="property">elements</span>[<span class="number">1</span>] + vertex_pos[i][<span class="number">1</span>] * modelMatrix.<span class="property">elements</span>[<span class="number">5</span>] + modelMatrix.<span class="property">elements</span>[<span class="number">13</span>];</span><br><span class="line">        <span class="keyword">if</span>((x - vertex_X)**<span class="number">2</span> + (y - vertex_Y)**<span class="number">2</span> &lt; min_length) &#123;</span><br><span class="line">            min_vertex = i;</span><br><span class="line">            min_length = (x - vertex_X)**<span class="number">2</span> + (y - vertex_Y)**<span class="number">2</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="title class_">DraggingVertex</span> = min_vertex;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">toW</span>(<span class="params">co, XorY, canvas, rect</span>)&#123; <span class="comment">// 变换至Webgl坐标</span></span><br><span class="line">    <span class="keyword">if</span>(!<span class="title class_">XorY</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> ((co - rect.<span class="property">left</span>) - canvas.<span class="property">width</span> / <span class="number">2</span>) / (canvas.<span class="property">width</span> / <span class="number">2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> (canvas.<span class="property">height</span> / <span class="number">2</span> - (co - rect.<span class="property">top</span>)) / (canvas.<span class="property">height</span> / <span class="number">2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">计算机图形学pj2，webgl</summary>
    
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/categories/Study/"/>
    
    <category term="Computer Graphics" scheme="https://anti-entrophic.github.io/categories/Study/Computer-Graphics/"/>
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/tags/Study/"/>
    
    <category term="Computer Graphics" scheme="https://anti-entrophic.github.io/tags/Computer-Graphics/"/>
    
    <category term="Homework" scheme="https://anti-entrophic.github.io/tags/Homework/"/>
    
  </entry>
  
  <entry>
    <title>Mininet安装全指南</title>
    <link href="https://anti-entrophic.github.io/posts/10019.html"/>
    <id>https://anti-entrophic.github.io/posts/10019.html</id>
    <published>2023-11-22T14:01:21.000Z</published>
    <updated>2023-12-11T08:29:57.370Z</updated>
    
    <content type="html"><![CDATA[<p>该说不说，对我们学校大部分课程的Lab都非常失望，连简单的实验文档都写不好，还需要学生自己摸索怎么做，浪费我那么多时间。</p><p>记录一下自己在安装mininet过程中遇到的问题，非常非常多，希望能帮到后来的同学。</p><p>应该会有很多图，希望都能加载出来。</p><h1 id="安装虚拟机"><a href="#安装虚拟机" class="headerlink" title="安装虚拟机"></a>安装虚拟机</h1><p>本次mininet需要用到图形界面，WSL本人试过了并不支持，无奈只能转向VirtualBox进行实验。</p><p>去VirtualBox官网下载即可：<a href="https://www.virtualbox.org/wiki/Downloads">https://www.virtualbox.org/wiki/Downloads</a></p><p>我是windows系统，直接点windows host下载即可。下载完后安装。</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picss.sunbangyan.cn/2023/11/22/d847fbc3124ed53e25aed79901f8ed26.jpeg" style="width:400px;"/></div></div><p>下载完后打开应该是这样：（我因为已经建好了所以列表里会有一个Ubuntu，原始状态应该是空的）</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picst.sunbangyan.cn/2023/11/22/d3b4657d59d70bc5d2ca1999fae0005f.jpeg" style="width:800px;"/></div></div><p>这个时候我们需要先去下载一个Ubuntu镜像，在 <a href="https://mirrors.tuna.tsinghua.edu.cn/ubuntu-releases/20.04/">https://mirrors.tuna.tsinghua.edu.cn/ubuntu-releases/20.04/</a> 这里即可下载，选择 ubuntu-20.04.6-desktop-amd64.iso</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picss.sunbangyan.cn/2023/11/22/a259e2bfd08beb71fad51b4c18bc3768.jpeg" style="width:400px;"/></div></div><p>下载完后我们回到VirtualBox，点击右上方蓝色的新建</p><p>进去以后界面如下，设置一个名称（Ubuntu就可以），文件夹在想要的位置自己建一个，然后虚拟光盘选择我们刚才下载的文件。下一步</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picss.sunbangyan.cn/2023/11/22/415d13eb9426abb55c23cdbbd4bf05f6.jpeg" style="width:800px;"/></div></div><p>下一步自己设置用户名和密码（请记住）。然后下面这个增强功能要勾选！（截图中没有选上）</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picdl.sunbangyan.cn/2023/11/22/04cb6d27ce0073cd853d42217eab62fd.jpeg" style="width:800px;"/></div></div><p>之后是分配内存和空间，默认即可（感觉空间不用那么多）</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picdl.sunbangyan.cn/2023/11/22/0b60d8486932d0d87d22fadfcdd0de05.jpeg" style="width:400px;"/></div></div><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picdl.sunbangyan.cn/2023/11/22/0b8f435b83c5e52744ddcb9eabe106a5.jpeg" style="width:400px;"/></div></div><p>然后它就会开始初始化系统，等待一段时间即可。</p><h1 id="配置Ubuntu"><a href="#配置Ubuntu" class="headerlink" title="配置Ubuntu"></a>配置Ubuntu</h1><h2 id="呼出命令行"><a href="#呼出命令行" class="headerlink" title="呼出命令行"></a>呼出命令行</h2><p>随后就可以进入Ubuntu界面，那么，该如何呼出命令行呢？</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picdl.sunbangyan.cn/2023/11/22/146af73c4b003f2be0f83e70a8ae9032.jpeg" style="width:400px;"/></div></div><p>按下键盘上的windows键，输入terminal即可。</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picdm.sunbangyan.cn/2023/11/22/a4718c0bb76ea6a428a0bb204e805c9e.jpeg" style="width:400px;"/></div></div><p>但是你会发现只有一行terminal的文字出现在左上角，随后无事发生。这里就是我遇到的第一个坑了。解决方法参考：<a href="https://www.cnblogs.com/lifuqiang/articles/17167367.html">https://www.cnblogs.com/lifuqiang/articles/17167367.html</a></p><p>我们首先输入CTRL + ALT + F3，然后会要求我们登录，用户名输入root，密码输入自己之前设的那个</p><p>随后，依次输入命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /etc/default</span><br><span class="line">sudo nano locale</span><br></pre></td></tr></table></figure><p>打开一个配置文件，将第一行改为 <code>LANG=en_US.UTF-8</code></p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picst.sunbangyan.cn/2023/11/22/37e17dd85531240de5b51dc37f3c1a6c.jpeg" style="width:800px;"/></div></div><p>按 CTRL+X，然后 Y，回车，然后再enter退出编辑</p><p>随后，再输入命令 <code>sudo locale-gen --purge</code>， 配置我们刚刚修改的设置</p><p>然后输入命令 <code>reboot</code> 重启虚拟机，现在我们就可以windows键打开搜索，输入terminal然后启动了。（原来乱码的日期也恢复了）</p><h2 id="给用户sudo权限"><a href="#给用户sudo权限" class="headerlink" title="给用户sudo权限"></a>给用户sudo权限</h2><p>参考：<a href="https://blog.csdn.net/Moelimoe/article/details/105292219">https://blog.csdn.net/Moelimoe/article/details/105292219</a></p><p>目前你创建的用户是没有超级权限的，而mininet的使用又必须要超级权限，之后会遇到各种问题。干脆直接给用户超级权限好了。</p><p>首先打开命令行，<code>su root</code> 进入root用户，然后输入命令 <code>sudo adduser &lt;user_id&gt; sudo</code> ，填入自己的用户名即可，之后你的账号就有sudo权限了，操作也不用进root。</p><h1 id="配置mininet"><a href="#配置mininet" class="headerlink" title="配置mininet"></a>配置mininet</h1><h2 id="安装git"><a href="#安装git" class="headerlink" title="安装git"></a>安装git</h2><p>刚配好的环境什么也没有，git也需要安装，命令行输入 <code>sudo apt install git</code></p><p>然后 <code>git clone https://gitee.com/derekwin/mininet.git</code> ，原仓库下载会卡住，并且有一些怪问题，这边我们利用别人的镜像源（参考：<a href="https://zhuanlan.zhihu.com/p/576832894）">https://zhuanlan.zhihu.com/p/576832894）</a></p><h2 id="安装pip"><a href="#安装pip" class="headerlink" title="安装pip"></a>安装pip</h2><p>想不到吧，pip也没有，我们还需要先安装pip，否则之后会报错。</p><p>命令： <code>sudo apt-get install python3-pip</code></p><h2 id="安装mininet"><a href="#安装mininet" class="headerlink" title="安装mininet"></a>安装mininet</h2><p>随后，我们需要输入命令 <code>PYTHON=python3 mininet/util/install.sh -a</code>，指定python3是因为较新版本的ubuntu默认只有python3环境，且没有/usr/bin/python，之后安装时会报错。</p><p>这步有可能因为网络问题卡住而获取失败，我的建议是多试几次。没什么办法。</p><p>安装过程比较长，需要等待</p><p>出现Enjoy Mininet!就是成功了</p><p>之后再输入一行命令 <code>sudo apt-get install mininet</code>，一切大功告成。</p><h2 id="下载iperf3"><a href="#下载iperf3" class="headerlink" title="下载iperf3"></a>下载iperf3</h2><p>后续实验还需要两个小插件，我们输入 <code>sudo apt-get install iperf3</code> 安装</p><p>还有一个iperf3-plotter，但是助教给的git根本download不下来（<code>git clone git://github.com/ekfoury/iperf3_plotter.git</code>），很难蚌。</p><p>可以直接去 <a href="https://github.com/ekfoury/iperf3_plotter.git">https://github.com/ekfoury/iperf3_plotter.git</a> 下载压缩包，解压</p><p>随后我们回到VirtualBox的管理页面，在我们配置的Ubuntu环境下，我们首先需要修改一下设置。修改设置需要在关闭虚拟机的情况下启动</p><p>在”设置-&gt;存储”中，我们需要对空的”控制器:IDE”下添加一个盘，点击右侧的”属性-&gt;分配光驱”右边的那个小图标，选择”选择虚拟盘”，找到VirtualBox安装时自带的一个虚拟盘即可</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picss.sunbangyan.cn/2023/11/22/e57a184a662d19a18808481cca0bc9f3.jpeg" style="width:800px;"/></div></div><p>之后退回管理页面，我们需要先启动虚拟机。然后我们就可以在右下角输入用户名密码登录，并且传文件。左边是windows下的文件系统，右边是ubuntu中的。我们选择需要的文件，点击中间的图标中下面那个，就可以把文件传到虚拟机中了。</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picss.sunbangyan.cn/2023/11/22/99c0d3389c8071cdf4d9ab1cee89cd2a.jpeg" style="width:800px;"/></div></div><p>随后在虚拟系统中命令行进入文件夹iperf3_plotter-master，然后 <code>sudo make</code>，就可以了</p><h1 id="END"><a href="#END" class="headerlink" title="END"></a>END</h1><p>之后的实验部分比较简单，就不另外记录了，引导也还可以。</p>]]></content>
    
    
    <summary type="html">保姆级教程，跟着做必成功。</summary>
    
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/categories/Study/"/>
    
    <category term="Computer Network" scheme="https://anti-entrophic.github.io/categories/Study/Computer-Network/"/>
    
    
    <category term="Computer Network" scheme="https://anti-entrophic.github.io/tags/Computer-Network/"/>
    
    <category term="Study" scheme="https://anti-entrophic.github.io/tags/Study/"/>
    
    <category term="Notes" scheme="https://anti-entrophic.github.io/tags/Notes/"/>
    
  </entry>
  
  <entry>
    <title>FDU Operating System Lab4</title>
    <link href="https://anti-entrophic.github.io/posts/10018.html"/>
    <id>https://anti-entrophic.github.io/posts/10018.html</id>
    <published>2023-11-15T07:46:42.000Z</published>
    <updated>2023-11-22T07:02:15.325Z</updated>
    
    <content type="html"><![CDATA[<h1 id="RISC-V-实验"><a href="#RISC-V-实验" class="headerlink" title="RISC-V 实验:"></a>RISC-V 实验:</h1><h3 id="1-Which-registers-contain-arguments-to-functions-For-example-which-register-holds-13-in-main’s-call-to-printf"><a href="#1-Which-registers-contain-arguments-to-functions-For-example-which-register-holds-13-in-main’s-call-to-printf" class="headerlink" title="1. Which registers contain arguments to functions? For example, which register holds 13 in main’s call to printf?"></a>1. Which registers contain arguments to functions? For example, which register holds 13 in main’s call to printf?</h3><p>在推荐阅读材料《Calling Convention》中，有提到：</p><p>“The RISC-V calling convention passes arguments in registers when possible. Up to eight integer registers, a0–a7, and up to eight floating-point registers, fa0–fa7, are used for this purpose.” </p><p>所以，向函数传递参数的寄存器是 <code>a0-a7</code> 和 <code>fa0-fa7</code>. 在调用 printf 的时候，13 被放在了寄存器 a2 中 <code>(24: 4635 li a2,13)</code></p><h3 id="2-Where-is-the-call-to-function-f-in-the-assembly-code-for-main-Where-is-the-call-to-g-Hint-the-compiler-may-inline-functions"><a href="#2-Where-is-the-call-to-function-f-in-the-assembly-code-for-main-Where-is-the-call-to-g-Hint-the-compiler-may-inline-functions" class="headerlink" title="2. Where is the call to function f in the assembly code for main? Where is the call to g? (Hint: the compiler may inline functions.)"></a>2. Where is the call to function f in the assembly code for main? Where is the call to g? (Hint: the compiler may inline functions.)</h3><p><code>main</code> 函数应该是在 <code>printf</code> 中调用了函数 <code>f</code>，但是通过查看汇编代码，我们发现本应调用函数 <code>f</code> 的地方，<code>main</code> 函数直接读取了参数 12 <code>(24: 45b1 li a1,12)</code>，可以想到是函数 f 和函数 g 太简单了，直接被编译器优化掉了。</p><h3 id="3-At-what-address-is-the-function-printf-located"><a href="#3-At-what-address-is-the-function-printf-located" class="headerlink" title="3. At what address is the function printf located?"></a>3. At what address is the function printf located?</h3><p>由<code>(34:61a080e7 jalr 1562(ra) # 64a &lt;printf&gt;)</code>这句，应该是在 <code>0x64a</code> 的位置，其中 <code>ra</code> 是上一行使用 <code>(30: 00000097 auipc ra,0x0)</code> 获取的这行代码的地址 <code>0x30</code>，然后 <code>jalr</code> 的目标地址就是 1562(十进制) + 0x30 = 0x64a，我们把代码往下翻到 <code>0x64a</code> 的地方，也确实是 <code>printf</code> 函数的起始地址。</p><h3 id="4-What-value-is-in-the-register-ra-just-after-the-jalr-to-printf-in-main"><a href="#4-What-value-is-in-the-register-ra-just-after-the-jalr-to-printf-in-main" class="headerlink" title="4. What value is in the register ra just after the jalr to printf in main?"></a>4. What value is in the register ra just after the jalr to printf in main?</h3><p>参考阅读材料：</p><p>“The indirect jump instruction JALR (jump and link register) uses the I-type encoding. The target address is obtained by adding the sign-extended 12-bit I-immediate to the register rs1, then setting the least-significant bit of the result to zero. The address of the instruction following the jump (pc+4) is written to register rd.Register x0 can be used as the destination if the result is not required.” </p><p>所以，应该 ra 里存的是 pc+4，也就是 <code>0x38</code></p><h3 id="5-Run-the-following-code-unsigned-int-i-0x00646c72-printf-“H-x-Wo-s”-57616-amp-i"><a href="#5-Run-the-following-code-unsigned-int-i-0x00646c72-printf-“H-x-Wo-s”-57616-amp-i" class="headerlink" title="5. Run the following code. unsigned int i = 0x00646c72; printf(“H%x Wo%s”, 57616, &amp;i)"></a>5. Run the following code. unsigned int i = 0x00646c72; printf(“H%x Wo%s”, 57616, &amp;i)</h3><p>输出：HE110 World</p><p>前面这个%x 是以 16 进制输出 57616(十进制)，就是 E110；后面因为 risc-v 采用 little-endian 存<br>储数据，所以输出的时候是 <code>72, 6c, 64, 00</code>，对应的字符就是 <code>r l d \0</code>，如果是 big-endian, i 就应该<br>是 <code>0x726c6400</code>，<code>57616</code> 不用变。</p><h3 id="6-In-the-following-code-what-is-going-to-be-printed-after-‘y-’-note-the-answer-is-not-a-specific-value-Why-does-this-happen-printf-“x-d-y-d”-3"><a href="#6-In-the-following-code-what-is-going-to-be-printed-after-‘y-’-note-the-answer-is-not-a-specific-value-Why-does-this-happen-printf-“x-d-y-d”-3" class="headerlink" title="6. In the following code, what is going to be printed after ‘y=’? (note: the answer is not a specific value.) Why does this happen?  printf(“x=%d y=%d”, 3);"></a>6. In the following code, what is going to be printed after ‘y=’? (note: the answer is not a specific value.) Why does this happen?  printf(“x=%d y=%d”, 3);</h3><p>输出：x=3 y=8229</p><p>前面一个正常输出 3，y 这个没传参数，会输出一个奇怪的值。看汇编码的话，printf(“x=%d y=%d”, 3) 会比 printf(“x=%d y=%d”, 3, 4) 少一句 <code>li a2,4</code>，其余汇编码都是一样的。我猜就是没传参数的话，y 也会输出 a2 寄存器里的内容，只不过寄存器里就是一个不确定的值了。</p><h1 id="BACKTRACE实验"><a href="#BACKTRACE实验" class="headerlink" title="BACKTRACE实验"></a>BACKTRACE实验</h1><p>backtrace函数的目的是递归读取每个函数调用栈，依次打印返回地址</p><p>我们先来看一下本次的测试程序 <code>/user/bttest</code> ，比较朴素，就调用了一个sleep()的system call</p><p>所以我们要在 <code>kernel/defs.h</code> 头文件中注册backtrace函数的原型，以便sys_sleep函数可以调用</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// printf.c</span></span><br><span class="line"><span class="type">void</span>            <span class="title function_">printf</span><span class="params">(<span class="type">char</span>*, ...)</span>;</span><br><span class="line"><span class="type">void</span>            <span class="title function_">panic</span><span class="params">(<span class="type">char</span>*)</span> __<span class="title function_">attribute__</span><span class="params">((<span class="keyword">noreturn</span>))</span>;</span><br><span class="line"><span class="type">void</span>            <span class="title function_">printfinit</span><span class="params">(<span class="type">void</span>)</span>;</span><br><span class="line"><span class="type">void</span>            <span class="title function_">backtrace</span><span class="params">(<span class="type">void</span>)</span>;</span><br></pre></td></tr></table></figure><p>进一步按照提示，在 <code>kernel/riscv.h</code> 添加如下函数获取s0寄存器中保存的fp的值</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">static</span> <span class="keyword">inline</span> uint64 <span class="title function_">r_fp</span><span class="params">()</span> &#123;</span><br><span class="line">  uint64 x;</span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;mv %0, s0&quot;</span> : <span class="string">&quot;=r&quot;</span> (x) )</span>;</span><br><span class="line">  <span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 <code>kernel/sysproc.c</code> 的sys_sleep()中调用backtrace()</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">uint64</span><br><span class="line"><span class="title function_">sys_sleep</span><span class="params">(<span class="type">void</span>)</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="type">int</span> n;</span><br><span class="line">  uint ticks0;</span><br><span class="line"></span><br><span class="line">  backtrace();</span><br><span class="line"></span><br><span class="line">  argint(<span class="number">0</span>, &amp;n);</span><br><span class="line">  <span class="comment">//...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="backtrace函数实现"><a href="#backtrace函数实现" class="headerlink" title="backtrace函数实现"></a>backtrace函数实现</h2><p>我们可以先看 <a href="https://pdos.csail.mit.edu/6.1810/2022/lec/l-riscv.txt">https://pdos.csail.mit.edu/6.1810/2022/lec/l-riscv.txt</a> 中对函数调用栈的结构的介绍</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">Stack</span><br><span class="line">                   .</span><br><span class="line">                   .</span><br><span class="line">      +-&gt;          .</span><br><span class="line">      |   +-----------------+   |</span><br><span class="line">      |   | return address  |   |</span><br><span class="line">      |   |   previous fp ------+</span><br><span class="line">      |   | saved registers |</span><br><span class="line">      |   | local variables |</span><br><span class="line">      |   |       ...       | &lt;-+</span><br><span class="line">      |   +-----------------+   |</span><br><span class="line">      |   | return address  |   |</span><br><span class="line">      +------ previous fp   |   |</span><br><span class="line">          | saved registers |   |</span><br><span class="line">          | local variables |   |</span><br><span class="line">      +-&gt; |       ...       |   |</span><br><span class="line">      |   +-----------------+   |</span><br><span class="line">      |   | return address  |   |</span><br><span class="line">      |   |   previous fp ------+</span><br><span class="line">      |   | saved registers |</span><br><span class="line">      |   | local variables |</span><br><span class="line">      |   |       ...       | &lt;-+</span><br><span class="line">      |   +-----------------+   |</span><br><span class="line">      |   | return address  |   |</span><br><span class="line">      +------ previous fp   |   |</span><br><span class="line">          | saved registers |   |</span><br><span class="line">          | local variables |   |</span><br><span class="line">  $fp --&gt; |       ...       |   |</span><br><span class="line">          +-----------------+   |</span><br><span class="line">          | return address  |   |</span><br><span class="line">          |   previous fp ------+</span><br><span class="line">          | saved registers |</span><br><span class="line">  $sp --&gt; | local variables |</span><br><span class="line">          +-----------------+</span><br></pre></td></tr></table></figure><p>每个函数使用一个栈帧，栈从高地址向低地址生长。由图，我们可以明白，每个栈帧的起始地址由fp给出（通过 <code>r_fp()</code> 获取），fp-8 就是返回地址，fp-16 就是前一个函数的栈帧的fp。</p><p>所以我们只要递归地打印信息就可以了，至于终止条件，根据提示我们可以参看 <code>kernel/riscv.h</code> 中的PGROUNDUP(fp)或PGROUNDDOWN(fp)函数</p><p>多个函数的栈帧都在同一个页面中，无论fp指针指向哪个函数的栈帧，PGROUNDUP(fp) 和 PGROUNDDOWN(fp)都是固定不变的，且有PGROUNDUP(fp) - PGROUNDDOWN(fp) = PGSIZE。而当 fp 指向当前页的起始地址时，会有PGROUNDUP(fp) = PGROUNDDOWN(fp)，此时循环终止。</p><p>在 <code>kernel/printf.c</code> 中<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> </span><br><span class="line"><span class="title function_">backtrace</span><span class="params">(<span class="type">void</span>)</span>&#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;barcktrace:\n&quot;</span>);</span><br><span class="line"></span><br><span class="line">  uint64 ra, fp = r_fp();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span>(PGROUNDUP(fp) - PGROUNDDOWN(fp) == PGSIZE)</span><br><span class="line">  &#123;</span><br><span class="line">    ra = *((uint64*)(fp - <span class="number">8</span>));</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%p\n&quot;</span>, ra);</span><br><span class="line">    fp = *((uint64*)(fp - <span class="number">16</span>));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在调用完bttest后退出qemu，执行addr2line -e kernel/kernel，查看返回地址在代码中的位置</p><p>执行命令：<br><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">addr2line -e kernel/kernel</span><br><span class="line"><span class="number">0</span>x000000008000212c</span><br><span class="line"><span class="number">0</span>x000000008000201e</span><br><span class="line"><span class="number">0</span>x0000000080001d14</span><br></pre></td></tr></table></figure></p><p>得到结果：<br><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/root/Desktop/xv6-labs-<span class="number">2022</span>/kernel/sysproc.c:<span class="number">59</span></span><br><span class="line">/root/Desktop/xv6-labs-<span class="number">2022</span>/kernel/syscall.c:<span class="number">141</span></span><br><span class="line">/root/Desktop/xv6-labs-<span class="number">2022</span>/kernel/trap.c:<span class="number">76</span></span><br></pre></td></tr></table></figure></p><h1 id="Alarm实验"><a href="#Alarm实验" class="headerlink" title="Alarm实验"></a>Alarm实验</h1><p>首先我们需要一些准备工作，添加两个系统调用 <code>sys_sigalarm</code> 和 <code>sys_sigreturn</code> ，可以参看Lab2的操作</p><p>修改一下Makefile文件来编译alarmtest.c</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">UPROGS=\</span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line">$U/_zombie\</span><br><span class="line">    $U/_alarmtest\</span><br></pre></td></tr></table></figure><p>在 <code>user/user.h</code> 中添加系统调用声明</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// system calls</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">sigalarm</span><span class="params">(<span class="type">int</span> ticks, <span class="type">void</span> (*handler)())</span>;</span><br><span class="line"><span class="type">int</span> <span class="title function_">sigreturn</span><span class="params">(<span class="type">void</span>)</span>;</span><br></pre></td></tr></table></figure><p>在 <code>kernel/syscall.h</code> 中，添加两个宏定义</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> SYS_sigalarm 22</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> SYS_sigreturn 23</span></span><br></pre></td></tr></table></figure><p>在 <code>kernel/syscall.c</code> 指定系统调用的主体函数</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">extern</span> uint64 <span class="title function_">sys_sigalarm</span><span class="params">(<span class="type">void</span>)</span>;</span><br><span class="line"><span class="keyword">extern</span> uint64 <span class="title function_">sys_sigreturn</span><span class="params">(<span class="type">void</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">static</span> <span class="title function_">uint64</span> <span class="params">(*syscalls[])</span><span class="params">(<span class="type">void</span>)</span> = &#123;</span><br><span class="line"><span class="comment">//...</span></span><br><span class="line">[SYS_sigalarm] sys_sigalarm,</span><br><span class="line">[SYS_sigreturn] sys_sigreturn,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 <code>user/usys.pl</code> 中添加系统调用的存根</p><figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">entry(<span class="string">&quot;sigalarm&quot;</span>);</span><br><span class="line">entry(<span class="string">&quot;sigreturn&quot;</span>);</span><br></pre></td></tr></table></figure><p>以上就做完函数调用的准备了，接下来，先实现 <code>sys_sigalarm()</code> 函数</p><p>修改 <code>kernel/proc.h</code> 中的proc结构体，添加sigalarm的两个参数(时间间隔n、调用函数fn)，还需要记录从上次调用sigalarm后经过的时间间隔ticks</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> n; <span class="comment">// the alarm interval</span></span><br><span class="line"><span class="type">int</span> ticks;</span><br><span class="line">uint64 fn; <span class="comment">// the pointer to the handler function</span></span><br></pre></td></tr></table></figure><p>在 <code>kernel/proc.c</code> 中的allocproc()函数中初始化新加的参数</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">found:</span><br><span class="line">  p-&gt;pid = allocpid();</span><br><span class="line">  p-&gt;state = USED;</span><br><span class="line">  p-&gt;n = <span class="number">0</span>;</span><br><span class="line">  p-&gt;ticks = <span class="number">0</span>;</span><br><span class="line">  p-&gt;fn = <span class="number">0</span>;</span><br></pre></td></tr></table></figure><p>在 <code>kernel/sysproc.c</code> 中实现 <code>sys_sigalarm()</code> 函数(暂时 <code>sys_sigreturn</code> 只返回0)。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">uint64</span><br><span class="line"><span class="title function_">sys_sigalarm</span><span class="params">(<span class="type">void</span>)</span>&#123;</span><br><span class="line">  <span class="type">int</span> n;</span><br><span class="line">  uint64 fn;</span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">proc</span> *<span class="title">p</span> =</span> myproc();</span><br><span class="line"></span><br><span class="line">  argint(<span class="number">0</span>,&amp;n);</span><br><span class="line">  argaddr(<span class="number">1</span>,&amp;fn);</span><br><span class="line"></span><br><span class="line">  p-&gt;n = n;</span><br><span class="line">  p-&gt;fn = fn;</span><br><span class="line">  p-&gt;ticks = <span class="number">0</span>;  </span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">uint64</span><br><span class="line"><span class="title function_">sys_sigreturn</span><span class="params">(<span class="type">void</span>)</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>修改 <code>kernel/trap.c</code> 对时间中断的处理，如果是时钟中断（which_dev==2），就增加记录的中断数。如果中断数到达指定间隔，就将处理函数fn的指针传给epc，以执行处理函数。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(which_dev == <span class="number">2</span>) </span><br><span class="line">&#123;</span><br><span class="line">  p-&gt;ticks++;</span><br><span class="line">  <span class="keyword">if</span>(p-&gt;ticks == p-&gt;n) &#123;</span><br><span class="line">    p-&gt;trapframe-&gt;epc = p-&gt;fn; </span><br><span class="line">    p-&gt;ticks = <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  yield();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="实现sys-sigreturn"><a href="#实现sys-sigreturn" class="headerlink" title="实现sys_sigreturn"></a>实现sys_sigreturn</h2><p>这个函数调用要求我们在时钟中断返回后，能够回到原来的状态。</p><p>根据hint，我们需要在sys_sigalarm()中保存中断时的寄存器值</p><p>所以，在 <code>kernel/proc.h</code> 中的proc结构体加上相关信息，不妨直接设置一个备份的trapframe（包含所有寄存器的值）：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">trapframe</span> *<span class="title">backup</span>;</span>    <span class="comment">// Save registers</span></span><br><span class="line"><span class="type">int</span> cur_fn;                  <span class="comment">// Prevent repetitive calls to the handler function(test2 要用)</span></span><br></pre></td></tr></table></figure><p>这样，在调用sigreturn时，我们只需要把backup复制回原来的trapframe即可，同时设cur_fn为0表示时钟中断处理函数结束；hint4也提醒我们要把返回值设为a0的值。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">uint64 </span><br><span class="line"><span class="title function_">sys_sigreturn</span><span class="params">(<span class="type">void</span>)</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">proc</span>* <span class="title">p</span> =</span> myproc();</span><br><span class="line">  *p-&gt;trapframe = *p-&gt;backup;</span><br><span class="line">  p-&gt;cur_fn = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">return</span> p-&gt;backup-&gt;a0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后对应的时钟中断处理函数，加一个 <code>if(p-&gt;cur_fn == 0)</code> 判断不会重复调用，还有就是复制原来的trapframe。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(which_dev == <span class="number">2</span>) </span><br><span class="line">&#123;</span><br><span class="line">  p-&gt;ticks++;</span><br><span class="line">  <span class="keyword">if</span>(p-&gt;ticks == p-&gt;n &amp;&amp; p-&gt;cur_fn == <span class="number">0</span>)</span><br><span class="line">  &#123;</span><br><span class="line">    *p-&gt;backup = *p-&gt;trapframe;</span><br><span class="line">    p-&gt;trapframe-&gt;epc = p-&gt;fn;</span><br><span class="line">    p-&gt;ticks = <span class="number">0</span>;</span><br><span class="line">    p-&gt;cur_fn = <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  yield();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其它还有一些细节部分，该分配内存和释放内存的地方，参考原本有的部分</p><p>在 <code>kernel/proc.c</code> 的 <code>allocproc</code> 中，给新加的两个变量分配内存：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">p-&gt;cur_fn = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>((p-&gt;backup = (<span class="keyword">struct</span> trapframe *)kalloc()) == <span class="number">0</span>)&#123;</span><br><span class="line">  freeproc(p);</span><br><span class="line">  release(&amp;p-&gt;lock);</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 <code>freeproc</code> 中释放内存：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(p-&gt;backup) </span><br><span class="line">  kfree((<span class="type">void</span>*) p-&gt;backup);</span><br><span class="line">p-&gt;backup = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">p-&gt;n = <span class="number">0</span>;</span><br><span class="line">p-&gt;ticks = <span class="number">0</span>;</span><br><span class="line">p-&gt;fn = <span class="number">0</span>;</span><br><span class="line">p-&gt;cur_fn = <span class="number">0</span>;</span><br><span class="line">p-&gt;state = UNUSED;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">RISC-V实验 与 Alarm实验</summary>
    
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/categories/Study/"/>
    
    <category term="Operating System" scheme="https://anti-entrophic.github.io/categories/Study/Operating-System/"/>
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/tags/Study/"/>
    
    <category term="Notes" scheme="https://anti-entrophic.github.io/tags/Notes/"/>
    
    <category term="Operating System" scheme="https://anti-entrophic.github.io/tags/Operating-System/"/>
    
  </entry>
  
  <entry>
    <title>LLaMA.Tokenizer：Byte Pair Encoding &amp; SentencePiece</title>
    <link href="https://anti-entrophic.github.io/posts/4a17b156.html"/>
    <id>https://anti-entrophic.github.io/posts/4a17b156.html</id>
    <published>2023-11-09T07:57:00.000Z</published>
    <updated>2023-11-13T10:48:23.053Z</updated>
    
    <content type="html"><![CDATA[<div class="note success simple"><p>没写完，，，勿看</p></div><p>10017</p><p>Tokenization是所有NLP任务的第一步，有许多不同的实现方法。今天主要介绍LLaMA中采用的tokenizer：Byte Pair Encoding，实现方法用的是Google提供的分词工具SentencePiece。</p><h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><p>简单来说，BPE就是，限定一个目标词表数量上限，然后在给定的语料库中进行训练，算法会不断将高词频的词组合在一起（比如说“机器”和“学习”组成“机器学习”），直到达到设定的词表数量上限。</p><p>词表的生成是从字符level开始的，假设我们有一个corpus：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[5] low_  # [5] 表示出现频数，即数量</span><br><span class="line">[2] lowest_</span><br><span class="line">[6] newer_</span><br><span class="line">[3] wider_</span><br><span class="line">[2] new_</span><br></pre></td></tr></table></figure><p>注意一定要将空格也记为一个字符，因为est_和est是不一样的，前者代表的是lowest这种词的结尾，而后者则不作为结尾，可能出现在像estate这种词中。</p><p>| 字符 | 频次 |<br>| …  | … |<br>| _ | 18|</p><h2 id="LLaMA代码"><a href="#LLaMA代码" class="headerlink" title="LLaMA代码"></a>LLaMA代码</h2><p>首先我们需要下载LLaMA的Tokenizer看一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> LlamaTokenizer</span><br><span class="line"></span><br><span class="line">base_model = <span class="string">&#x27;decapoda-research/llama-7b-hf&#x27;</span></span><br><span class="line">tokenizer = LlamaTokenizer.from_pretrained(base_model)</span><br></pre></td></tr></table></figure><p>会显示以下warning是因为，huggingface.transformers改过一次LlamaTokenizer的名字，和用之前库训练的模型中保存的信息对不上了，不用管</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. </span><br><span class="line">The tokenizer class you load from this checkpoint is &#x27;LLaMATokenizer&#x27;. </span><br><span class="line">The class this function is called from is &#x27;LlamaTokenizer&#x27;.</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">（没写完，先隐藏）最近想看看LLaMA的model，从Tokenizer开始</summary>
    
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/categories/Study/"/>
    
    <category term="NLP" scheme="https://anti-entrophic.github.io/categories/Study/NLP/"/>
    
    
    <category term="NLP" scheme="https://anti-entrophic.github.io/tags/NLP/"/>
    
    <category term="TOOLS" scheme="https://anti-entrophic.github.io/tags/TOOLS/"/>
    
  </entry>
  
  <entry>
    <title>spacy库介绍</title>
    <link href="https://anti-entrophic.github.io/posts/10016.html"/>
    <id>https://anti-entrophic.github.io/posts/10016.html</id>
    <published>2023-11-02T11:54:16.000Z</published>
    <updated>2023-11-15T15:55:28.491Z</updated>
    
    <content type="html"><![CDATA[<p>好久不写东西了，最近比较忙，还是边学边写比较好，不然事后很难再提起精神去总结。</p><p>本篇我会先介绍一下spacy的基本用法，然后尽量能够展示一些工程上的代码，方便学习使用。</p><h1 id="安装使用"><a href="#安装使用" class="headerlink" title="安装使用"></a>安装使用</h1><p>除了一般的安装spacy这个库，还需要安装一下你需要使用的模型，比如说英语任务就下载 <code>en_core_web_sm</code></p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install spacy</span><br><span class="line">python -m spacy download en_core_web_sm</span><br></pre></td></tr></table></figure><p>随后在python文件中，就可以轻松地导入使用，用法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line">text = <span class="string">&quot;SpaCy is a popular NLP library. I love it.&quot;</span></span><br><span class="line">doc = nlp(text)</span><br></pre></td></tr></table></figure><p>doc对象中会保留句子的诸多处理信息，详情见下</p><h1 id="主要功能"><a href="#主要功能" class="headerlink" title="主要功能"></a>主要功能</h1><h2 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h2><p>其中，token.text就是分词的结果，token.pos_是词性，token.dep_是依赖关系</p><p>有些可能比较难理解，之后会配合例子，如果用到了就具体介绍一下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> doc:</span><br><span class="line">    <span class="built_in">print</span>(token.text, token.pos_, token.dep_)</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">SpaCy PROPN nsubj</span><br><span class="line">is AUX ROOT</span><br><span class="line">a DET det</span><br><span class="line">popular ADJ amod</span><br><span class="line">NLP NOUN compound</span><br><span class="line">library NOUN attr</span><br><span class="line">. PUNCT punct</span><br><span class="line">I PRON nsubj</span><br><span class="line">love VERB ROOT</span><br><span class="line">it PRON dobj</span><br><span class="line">. PUNCT punct</span><br></pre></td></tr></table></figure><p>英语分词模型中一些词性标签示例：</p><ul><li>“NOUN”：名词</li><li>“VERB”：动词</li><li>“ADJ”：形容词</li><li>“ADV”：副词</li><li>“PRON”：代词</li><li>“DET”：冠词</li><li>“ADP”：介词</li><li>“CONJ”：连词</li></ul><p>依赖关系标签示例：</p><ul><li>“nsubj”：主语</li><li>“dobj”：直接宾语</li><li>“prep”：介词短语修饰</li><li>“amod”：形容词修饰</li><li>“conj”：连接词成分</li><li>“root”：句子的根</li></ul><h2 id="分句"><a href="#分句" class="headerlink" title="分句"></a>分句</h2><h3 id="sent-text"><a href="#sent-text" class="headerlink" title="sent.text"></a>sent.text</h3><p>spacy也可以把长文本分成多个短句来处理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> sent <span class="keyword">in</span> doc.sents:</span><br><span class="line">    <span class="built_in">print</span>(sent.text)</span><br></pre></td></tr></table></figure><p>结果：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SpaCy is a popular NLP library.</span><br><span class="line">I love it.</span><br></pre></td></tr></table></figure></p><h3 id="sent-root"><a href="#sent-root" class="headerlink" title="sent.root"></a>sent.root</h3><p>当然，sent中还保留了更多句子相关的信息，比如说可以使用 <code>sent.root</code> ，找到句子中的根词（通常是句子的主要动词或核心单词）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> sent <span class="keyword">in</span> doc.sents:</span><br><span class="line">    <span class="built_in">print</span>(sent.root)</span><br></pre></td></tr></table></figure><p>结果：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">is</span><br><span class="line">love</span><br></pre></td></tr></table></figure></p><p>还可以进一步通过 <code>sent.root.children</code> ，访问根词的直接子词，这每一个字词又是之前的token对象，包含了 <code>pos_</code>、<code>dep_</code> 等属性，可以拿来进一步分析句子成分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> sent <span class="keyword">in</span> doc.sents:</span><br><span class="line">    <span class="keyword">for</span> child <span class="keyword">in</span> sent.root.children:</span><br><span class="line">        <span class="built_in">print</span>(child.text, child.pos_, child.dep_)</span><br></pre></td></tr></table></figure><p>结果：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SpaCy PROPN nsubj</span><br><span class="line">library NOUN attr</span><br><span class="line">. PUNCT punct</span><br><span class="line">I PRON nsubj</span><br><span class="line">it PRON dobj</span><br><span class="line">. PUNCT punct</span><br></pre></td></tr></table></figure></p><p>那比如说我就可以提取出句子中的主要主语对吧，判断一下 <code>if child.dep_ == &quot;nsubj&quot;</code> 即可</p><h3 id="sent-noun-chunks"><a href="#sent-noun-chunks" class="headerlink" title="sent.noun_chunks"></a>sent.noun_chunks</h3><p>还有一些功能，比如说我可以找到句子中的名词短语</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> sent <span class="keyword">in</span> doc.sents:</span><br><span class="line">    <span class="keyword">for</span> phrase <span class="keyword">in</span> sent.noun_chunks:</span><br><span class="line">        <span class="built_in">print</span>(phrase)</span><br></pre></td></tr></table></figure><p>结果：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SpaCy</span><br><span class="line">a popular NLP library</span><br><span class="line">I</span><br><span class="line">it</span><br></pre></td></tr></table></figure></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>主要接触的基本上就这些，根据具体使用场景自行使用</p>]]></content>
    
    
    <summary type="html">介绍一个NLP的分词处理工具</summary>
    
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/categories/Study/"/>
    
    <category term="NLP" scheme="https://anti-entrophic.github.io/categories/Study/NLP/"/>
    
    
    <category term="NLP" scheme="https://anti-entrophic.github.io/tags/NLP/"/>
    
    <category term="TOOLS" scheme="https://anti-entrophic.github.io/tags/TOOLS/"/>
    
  </entry>
  
  <entry>
    <title>NLP论文精读（四） Transformer</title>
    <link href="https://anti-entrophic.github.io/posts/10015.html"/>
    <id>https://anti-entrophic.github.io/posts/10015.html</id>
    <published>2023-10-22T17:23:01.000Z</published>
    <updated>2023-11-08T16:05:15.045Z</updated>
    
    <content type="html"><![CDATA[<p>写得太好了，强烈建议看这篇博文：<a href="https://wmathor.com/index.php/archives/1438/">https://wmathor.com/index.php/archives/1438/</a></p><p>论文链接：<a href="https://arxiv.org/pdf/1706.03762.pdf">https://arxiv.org/pdf/1706.03762.pdf</a></p><p>太懒了一直没写完，一直在拿huggingface的transformers库做也不太关心具体的实现，感觉不太好。。</p><p>有时间了一定重头把这些模型都好好记录一下。</p><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picss.sunbangyan.cn/2023/10/23/bfebc9f96242e7d87ff581d1632f97e5.png" style="width:400px;"/></div></div><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picdm.sunbangyan.cn/2023/10/23/e01d28d62bf8197e404271711981b962.png" style="width:400px;"/></div></div><h1 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h1><p>RNN在编码上下文关系的时候，需要逐字迭代进行forward，这是非常耗时的。Transformer 摈弃了 RNN 的结构，所以首先，它需要一种别的方式来表征位置信息。</p><p>原论文中使用了数学方法来为每个字生成一个位置向量Positional Encoding，维度与词向量相同，都是embedding_dimension。这样，我们之后可以将位置向量与词向量相加，从而将位置信息编码进入向量中，让模型得以学习。</p><h2 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h2><p>Positional Encoding的设计方法比较不一般。我们先从容易想到的形式开始，看看它们有什么缺点：</p><p>第一种最容易想到的是，对每个字按位置分配一个[0,1]之间的数，其中0给第一个字，1给最后一个字。这样做的问题在于，如果有一个20字的短文本和40字的长文本，假设在短文本中有两个字的位置编码相差为0.05，而在长文本中也有两个字的位置编码相差为0.05。那么，在短文本中，这两个字其实是相邻的，而在长文本中，这两个字中间却相隔了一个字。同样的值却因为句子长度的不同而表现出不同的含义，这是不太合适的。</p><p>第二种是线性地为每个字分配1，2，3…这样的值，但是这样值可能会变得太大，过度影响词向量及模型表现。容易想到位置向量和字向量相加后，最后一个值会比第一个值大很多。</p><p>综上，Positional Encoding的理想设计应当满足以下三个条件：</p><ul><li><p>它应该为每个字输出唯一的编码</p></li><li><p>不同长度的句子之间，任何两个字之间的差值应该保持一致</p></li><li><p>它的值应该是有界的</p></li></ul><h2 id="论文设计"><a href="#论文设计" class="headerlink" title="论文设计"></a>论文设计</h2><p>论文中使用了 sin 和 cos 函数的线性变换来提供给模型位置信息:</p>$$\begin{array}{l}PE(pos,2i) = sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}}) \\\\PE(pos,2i+1) = cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\end{array}$$<p>上式中 $pos$ 指的是一句话中某个字的位置，取值范围是 <code>[0,sequence_length)</code> ；$i$ 指的是字向量的维度序号，取值范围是 <code>[0,embedding_size/2]</code> ；$d_{model}$ 就是 embedding_size</p><p>它很像是之前提到的第二种设计方法，只不过比起朴素的1，2，3，这里使用有界的三角函数。同时，把一个数字扩展到位置向量embedding_size的维度。</p><p>更具体的数学讨论可以见 <a href="https://spaces.ac.cn/archives/8231">https://spaces.ac.cn/archives/8231</a> ，还是很有收获的。</p><h2 id="未来发展"><a href="#未来发展" class="headerlink" title="未来发展"></a>未来发展</h2><p>原始Transformer中的Positional Encoding是纯数学公式计算得到的。</p><p>从BERT开始，模型会使用需要训练的embedding层。</p><p>最近的模型如LLaMA采用的是 <a href="https://arxiv.org/pdf/2104.09864.pdf">https://arxiv.org/pdf/2104.09864.pdf</a> 中提出的旋转位置编码（RoPE）来提升模型架构性能，之后有机会介绍。</p><h1 id="Self-Attention-Mechanism"><a href="#Self-Attention-Mechanism" class="headerlink" title="Self Attention Mechanism"></a>Self Attention Mechanism</h1><p>所以模型第一步，我们首先需要对于输入的整个句子 $X$ ，通过 Word Embedding 得到该句子中每个字的词向量，并通过 Positional Encoding 得到每个字的位置向量，将二者相加，得到每个字的表征向量。记第 $t$ 个字的向量为 $x_t$</p><p>回忆一下Attention的作用 <a href="https://anti-entrophic.github.io/posts/10014.html">https://anti-entrophic.github.io/posts/10014.html</a> ，在seq2seq的任务中，encoding会将整个输入句子编码成为一个背景词向量。而在decoding的过程中，我们使用attention，让其在生成内容时，能够更多地关注到当前需要生成的内容与原文本中不同位置的字的相关关系。</p><p>在Transformer中，我们使用 self-attention，在encoder中即将一句话中字与字之间的关系编码。</p><p>我们定义三个矩阵 $W_Q, W_K, W_V$ ，使用这三个矩阵分别对所有的字向量进行三次线性变换，于是所有字向量又衍生出三个新的向量 $q_t,k_t,v_t$。我们将所有的 $q_t$ 向量拼成一个大矩阵，记作查询矩阵 $Q$，将所有的 $k_t$ 向量拼成一个大矩阵，记作键矩阵 $K$，将所有的 $v_t$ 向量拼成一个大矩阵，记作值矩阵 $V$（见下图）</p><p>以处理第一个字为例，我们需要先用它的查询向量 $q_1$ 去乘以各个词的键向量，就像是在查询和各个词的关联性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">            [<span class="number">0</span>, <span class="number">4</span>, <span class="number">2</span>]</span><br><span class="line">[<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>] x [<span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>] = [<span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>]</span><br><span class="line">            [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>之后还需要将得到的值经过 softmax，使得它们的和为 1（见下图）</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picss.sunbangyan.cn/2023/11/08/d9f19e16846d90d41ee3a1d156aaa6d0.png" style="width:400px;"/></div></div><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">softmax([2, 4, 4]) = [0.0, 0.5, 0.5]</span><br></pre></td></tr></table></figure><p>有了权重之后，将权重其分别乘以对应字的值向量 $v_t$（见下图）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0.0 * [1, 2, 3] = [0.0, 0.0, 0.0]</span><br><span class="line">0.5 * [2, 8, 0] = [1.0, 4.0, 0.0]</span><br><span class="line">0.5 * [2, 6, 3] = [1.0, 3.0, 1.5]</span><br></pre></td></tr></table></figure><p>最后将这些权重化后的值向量求和，得到第一个字的输出（见下图）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">  [0.0, 0.0, 0.0]</span><br><span class="line">+ [1.0, 4.0, 0.0]</span><br><span class="line">+ [1.0, 3.0, 1.5]</span><br><span class="line">-----------------</span><br><span class="line">= [2.0, 7.0, 1.5]</span><br></pre></td></tr></table></figure><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picst.sunbangyan.cn/2023/11/08/ce6a1a93139b0e21aaa6d04827a6950d.png" style="width:400px;"/></div></div><p>对其它的输入向量也执行相同的操作，即可得到通过 self-attention 后的所有输出</p><h2 id="矩阵计算"><a href="#矩阵计算" class="headerlink" title="矩阵计算"></a>矩阵计算</h2><p>上面介绍的方法需要一个循环遍历所有的字 $x_t$ ，不过显然这个是可以转换成矩阵计算的形式的，从而一次计算出所有时刻的输出。</p><p>第一步我们即可得到一口气得到 Q、K、V 三个矩阵。其中X即为embedding后的句子字向量矩阵，第 $t$ 行是第 $t$ 个字的词向量表示。</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picss.sunbangyan.cn/2023/11/08/38a0cda5a6c39d1d2673bab8e3077369.png" style="width:400px;"/></div></div><p>随后将上述计算过程简化。注意这里有一步除以 $\sqrt d_k$ 的trick，这个 $d_k$ 就是 $k$ 向量的dimension，也就是embedding_size。这样可以帮助模型收敛，，不过不太明白这步的motivation。。。</p>]]></content>
    
    
    <summary type="html">૮₍ ˃ ⤙ ˂ ₎ა 太懒了，一直没写完</summary>
    
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/categories/Study/"/>
    
    <category term="NLP" scheme="https://anti-entrophic.github.io/categories/Study/NLP/"/>
    
    
    <category term="NLP" scheme="https://anti-entrophic.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>NLP论文精读（三） Attention机制</title>
    <link href="https://anti-entrophic.github.io/posts/10014.html"/>
    <id>https://anti-entrophic.github.io/posts/10014.html</id>
    <published>2023-10-22T15:57:17.000Z</published>
    <updated>2023-10-22T17:22:17.536Z</updated>
    
    <content type="html"><![CDATA[<p>参考：<a href="https://wmathor.com/index.php/archives/1450/">https://wmathor.com/index.php/archives/1450/</a></p><p>上回说到：<a href="https://anti-entrophic.github.io/posts/10013.html">https://anti-entrophic.github.io/posts/10013.html</a></p><p>其中的样例是英语——&gt;法语的机器翻译：</p><blockquote><p>英语：They are watching</p><p>法语：lls regardent</p></blockquote><p>我们可以想到，在生成目标语句的时候，’lls’ 应该更多地依赖于 ‘They are’ 来生成，而 ‘regardent’ 则更多地依赖于 ‘watching’</p><p>但是在seq2seq的设计中，所有的输入序列影响都被整合为了一个统一的背景词向量 $c$ ，尤其是当输入很长时，这会损失很多细节，影响模型的表现</p><p>Attention机制的目的就是希望在decoder时，能够利用不同的词背景向量，或者说，更相关的词背景向量。</p><p>相关论文：<a href="https://arxiv.org/pdf/1409.0473.pdf">https://arxiv.org/pdf/1409.0473.pdf</a></p><h1 id="模型解释"><a href="#模型解释" class="headerlink" title="模型解释"></a>模型解释</h1><p>简单来说就是，在decoder中，计算出往下传递的隐藏状态 $s_i$ 后，我们需要拿 $s_i$ 去和encoder的隐藏状态 $h_i$，计算相关性后加权平均得到一个动态变化的背景词向量 $c_i$</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picdl.sunbangyan.cn/2023/10/22/88f7bbfcd29cea0bcaffe1a0e160e62d.png" style="width:400px;"/></div></div><p>我们取 $s_0$ 简单的在数值上就等于最后一个隐状态 $h_m$</p><p>随后，我们需要这个词向量 $s_0$ 和所有隐藏状态 $h_i$ 计算一个相关性 ${\alpha}_i = align(h_i,s_0)$</p><p>计算得到 $m$ 个相关性参数 ${\alpha}_i$ 后，将这些值与 $h_i$ 作加权求和，即：</p><script type="math/tex; mode=display">c_0 = \sum_{i=1}^m {\alpha}_ih_i={\alpha}_1 h_1 + ... + {\alpha}_m h_m</script><p>这样就可以反映出不同的影响力</p><p>随后我们会算出新的隐藏状态 $s_1$ 以及输出 $x_1’$，再加权算出 $c_1$，一起送到下一个单元中，依次类推，直到生成终止符结束。</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picss.sunbangyan.cn/2023/10/22/0e2bff808b7921740feb0173ba328bc1.png" style="width:400px;"/></div></div><h1 id="相关性计算函数align-的设计"><a href="#相关性计算函数align-的设计" class="headerlink" title="相关性计算函数align()的设计"></a>相关性计算函数align()的设计</h1><p>比较主流的，目前 Transformer 中的 Attention 设计是</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picss.sunbangyan.cn/2023/10/22/143d2c67d696231324d72ef40e212d99.png" style="width:400px;"/></div></div><p>嗯，完全不懂为什么Q，K，V要这样设计。。去看看别的文章学习一下</p>]]></content>
    
    
    <summary type="html">大名鼎鼎的Attention</summary>
    
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/categories/Study/"/>
    
    <category term="NLP" scheme="https://anti-entrophic.github.io/categories/Study/NLP/"/>
    
    
    <category term="NLP" scheme="https://anti-entrophic.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>NLP论文精读（二） seq2seq</title>
    <link href="https://anti-entrophic.github.io/posts/10013.html"/>
    <id>https://anti-entrophic.github.io/posts/10013.html</id>
    <published>2023-10-22T15:03:17.000Z</published>
    <updated>2023-11-06T13:32:31.273Z</updated>
    
    <content type="html"><![CDATA[<p>这次学个简单一点的知识。参考：<a href="https://wmathor.com/index.php/archives/1432/">https://wmathor.com/index.php/archives/1432/</a></p><p>假如有一个机器翻译的任务：</p><blockquote><p>英语：They are watching</p><p>法语：lls regardent</p></blockquote><p>seq2seq的方法就是利用了两个RNN，分别是对应输入序列的编码器和输出序列的解码器。</p><p>在解码阶段有两种选择，后续会介绍</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picdl.sunbangyan.cn/2023/10/22/cafabd89e6c6a72dffeb51abd91137ed.png" style="width:400px;"/></div></div><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picss.sunbangyan.cn/2023/10/22/fb1c2b642a2633e3193ce4d4c81af572.png" style="width:400px;"/></div></div><h1 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h1><p>在encoder这步的作用是把一个不定长的输入序列转化成一个定长的背景词向量 $c$ ，该背景词向量 $c$ 包含了输入序列的信息。</p><p>设在 $t$ 时刻，输入为 $x_t$，神经元的隐藏状态为 $h_t$ ，则有 $h_t=f(h_{t-1},x_t)$ </p><p>更具体地，设 $x_t$ 的维度是 (batch_size, feature_size)，$h_t$ 的维度是 (batch_size, hidden_size)， 每一个时刻的输出 $o_t$ 的维度是 (batch_size, output_size)。这里如果是nlp任务的画，output_size就应该等于feature_size，都是字的embedding的长度；如果是别的什么分类任务的话，可以有不同的取值。</p><p>然后还要一些权重矩阵，我们这里假设 $h_t = tanh(W \cdot h_{t-1} + )$</p><p>$c$ 就是与所有隐藏状态相关的一个函数：$c=q(h_1,…,h_T)$ ，当然，最简单的函数就是直接让 $c = h_T$</p><p>通常我们会在输入序列最后加上一个特殊字符 ‘eos’ 表示序列的终结。</p><h1 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h1><p>上文提到了两种decode方式，一种是直接把背景向量 $c$ 送到decoder中，一种是让语义向量 $c$ 参与序列所有时刻的运算</p><p>不管怎么样，我们能通过decoder得到一个输出序列：$y_1, y_2, …, y_{T’}$ ，而真实的输出序列是 $y_1’, …, y_{T’}’$ ，设decoder在 $t$ 时刻的隐藏状态是 $s_t$</p><p>自然，我们想要最大化联合概率函数 $P(y_1’, …, y_{T’}’) = \prod_{i=1}^{T’} P(y_i’) = \prod_{i=1}^{T’} g(y_{i-1}’, s_i,c)$ ,其中 $g$ 是decoder的算子。</p><p>维度问题可能有点混乱。。。。</p><p>看后一种decoder方式吧，那每个RNN单元应该有三个输入，一个是背景词向量 $c$ ，一个是上一次的隐状态 $s_0$ ，一个是上一次的预测输出的embedding，拼起来</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>总之就比较简单，还有一些诸如Teacher Forcing, Beam Search的优化。可以参考：<a href="https://zhuanlan.zhihu.com/p/194308943">https://zhuanlan.zhihu.com/p/194308943</a></p><p>下一章讲attention机制 ૮₍ ˃ ⤙ ˂ ₎ა</p>]]></content>
    
    
    <summary type="html">简单的encoder-decoder</summary>
    
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/categories/Study/"/>
    
    <category term="NLP" scheme="https://anti-entrophic.github.io/categories/Study/NLP/"/>
    
    
    <category term="NLP" scheme="https://anti-entrophic.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch学习笔记（三） LSTM</title>
    <link href="https://anti-entrophic.github.io/posts/10012.html"/>
    <id>https://anti-entrophic.github.io/posts/10012.html</id>
    <published>2023-10-18T15:24:21.000Z</published>
    <updated>2023-10-19T11:31:21.348Z</updated>
    
    <content type="html"><![CDATA[<p>参考：<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a> ，写得非常详细，有精致的图例和清楚的公式，建议先阅读该篇文章</p><p class='p left logo large'>基本概念</p><p>LSTM，长短期记忆神经网络，它克服了RNN难以记录长期信息的特点。</p><p>举个例子，“天上红色的是__”，RNN可以一定程度上利用“天上”、“红色”等信息来预测当前应填的单词，可能是“太阳”。</p><p>但是，如果有效信息相隔非常远，例如：“我是一个中国人，……，我会说___，英语和日语”，这些久远的信息很难留存在RNN的hidden state中。</p><p>LSTM在结构上做出了改进，如下图所示。它包含两种hidden state，一个是上方的$C_t$，它代表着模型的记忆，在训练过程中改动较少，一个是下方的$h_t$，代表着输入内容的抽象信息（？也许，其实没有这么明确的物理意义，只是我期望可以这么认为）。它们通过一些方式互相影响并更新，下面一一介绍。</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picss.sunbangyan.cn/2023/10/18/a1e155b0ed13694af1700b3c732024b9.png" style="width:800px;"/></div></div><p class='p left logo large'>网络结构</p><h1 id="遗忘门"><a href="#遗忘门" class="headerlink" title="遗忘门"></a>遗忘门</h1><p>LSTM的第一步是决定要让长期记忆$C_{t-1}$去遗忘什么。它拼接当前输入$x_t$和上一时刻的状态$h_{t-1}$，并送入sigmoid层，得到一个介于0与1之间的输出向量。随后，和$C_{t-1}$做Hadamard积。如果sigmoid层对应输出越接近0，则对应信息遗忘得越多，反之则继续记忆。</p><p>形象化的理解可能就是，用当前状态去判断哪些记忆不再适用了，比如说之前还一直在谈论A，现在突然转到谈论B了，一些有关A的信息就不再需要了。</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picdm.sunbangyan.cn/2023/10/19/9e11a5f3e9ea43fa9bb0538dbcc869e5.png" style="width:800px;"/></div></div><h1 id="输入门"><a href="#输入门" class="headerlink" title="输入门"></a>输入门</h1><p>第二步是决定要让$C_{t-1}$记住什么。当前输入首先会经过一个sigmoid层得到$i_t$，这决定了接下来哪些信息是有必要被记住的。实际上的更新向量$\tilde{C_t}$是通过tanh激活函数得到的，至于为什么选tanh，我找到了一些说法：（来自：<a href="https://stackoverflow.com/questions/40761185/what-is-the-intuition-of-using-tanh-in-lstm）">https://stackoverflow.com/questions/40761185/what-is-the-intuition-of-using-tanh-in-lstm）</a></p><ul><li><p>为了防止梯度消失问题，我们需要一个二次导数在大范围内不为0的函数，而tanh函数可以满足这一点</p></li><li><p>为了便于凸优化，我们需要一个单调函数</p></li><li><p>tanh函数一般收敛的更快</p></li><li><p>tanh函数的求导占用系统的资源更少</p></li></ul><p>随后，$i_t$与$\tilde{C_t}$相乘，得到正式的update内容。</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picdm.sunbangyan.cn/2023/10/19/59ab5d4957ad6e60f3984257d96245f5.png" style="width:800px;"/></div></div><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picst.sunbangyan.cn/2023/10/19/65e1a37f0d9e047776f1a0b0f6e228c7.png" style="width:800px;"/></div></div><h1 id="输出门"><a href="#输出门" class="headerlink" title="输出门"></a>输出门</h1><p>我们要决定输出什么内容。总之就是输入经过一个sigmoid层，加上记忆$C_{t}$的影响（经过tanh），</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picss.sunbangyan.cn/2023/10/19/a3243ac1c491e8b184bc8912621828af.png" style="width:800px;"/></div></div><p class='p left logo large'>代码</p><h1 id="自己实现"><a href="#自己实现" class="headerlink" title="自己实现"></a>自己实现</h1><p>先不用PyTorch定义好的LSTM，直接根据网络结构一步步从头搭建一个</p><p>注意到原来公式中有一步拼接$h_{t-1}$和$x_t$的操作，然后和权重矩阵$W$相乘。由线性代数知识，我们完全可以把$W \cdot [h_{t-1},x_t]$ 变成 $U \cdot x_t + V \cdot h_{t-1}$</p><p>这样，原来LSTM中的三个sigmoid层，一个tanh层的公式就应该是：</p><script type="math/tex; mode=display">\begin{equation}\left\{    \begin{array}{ll}      f_t &= \sigma(U_f \cdot x_t + V_f \cdot h_{t-1} + b_f) \\          i_t &= \sigma(U_i \cdot x_t + V_i \cdot h_{t-1} + b_i) \\      \tilde{C_t} &= \sigma(U_c \cdot x_t + V_c \cdot h_{t-1} + b_c) \\      o_t &= \sigma(U_o \cdot x_t + V_o \cdot h_{t-1} + b_o)    \end{array}\right. \notag\end{equation}</script><p>现在需要理清楚一下维度：</p><p>输入 $x$ 应该是 [batch_size, sequence_len(时间序列长度), feature_size]</p><p>这样 $x_t$ 就是 [feature_size]， 同理 $h_t$ 是 [hidden_size]</p><p>所以需要训练的参数为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">myLstm</span>(nn.Module):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_sz, hidden_sz</span>):</span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    self.input_size = input_sz</span><br><span class="line">    self.hidden_size = hidden_sz</span><br><span class="line"></span><br><span class="line">    <span class="comment">#f_t</span></span><br><span class="line">    self.U_f = nn.Parameter(torch.Tensor(input_sz, hidden_sz))</span><br><span class="line">    self.V_f = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))</span><br><span class="line">    self.b_f = nn.Parameter(torch.Tensor(hidden_sz))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#i_t</span></span><br><span class="line">    self.U_i = nn.Parameter(torch.Tensor(input_sz,hidden_sz))</span><br><span class="line">    self.V_i = nn.Parameter(torch.Tensor(hidden_sz,hidden_sz))</span><br><span class="line">    self.b_i = nn.parameter(torch.Tensor(hidden_sz))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#c_t</span></span><br><span class="line">    self.U_c = nn.Parameter(torch.Tensor(input_sz, hidden_sz))</span><br><span class="line">    self.V_c = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))</span><br><span class="line">    self.b_c = nn.Parameter(torch.Tensor(hidden_sz))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#o_t</span></span><br><span class="line">    self.U_o = nn.Parameter(torch.Tensor(input_sz, hidden_sz))</span><br><span class="line">    self.V_o = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))</span><br><span class="line">    self.b_o = nn.Parameter(torch.Tensor(hidden_sz))</span><br></pre></td></tr></table></figure><p>前向传播定义如下：</p><p>在训练时，某些样本在时间上可能是连续的，而有些是不相干的，有时我们需要权重进行预测。我们可以通过参数init_states来决定是否要继承上次的最终输出$h_t$和$c_t$，还是直接重新初始化。</p><p>随后，遍历时间步，在整个时间序列遍历完后再更新权重。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, init_states=<span class="literal">None</span></span>):</span><br><span class="line">    batch_size,seq_sz,_=x.size()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> init_states <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      h_t,c_t=(</span><br><span class="line">          torch.zeros(batch_size, self.hidden_size).to(x.device),</span><br><span class="line">          torch.zeros(batch_size, self.hidden_size).to(x.device)</span><br><span class="line">      )</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      h_t, c_t = init_states</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(seq_sz):</span><br><span class="line">      x_t = x[:, t, :]</span><br><span class="line"></span><br><span class="line">      <span class="comment"># 和nn.parameter计算时，自动处理x的batch这一维度</span></span><br><span class="line">      i_t = torch.sigmoid(x_t @ self.U_i + h_t @ self.V_i + self.b_i)</span><br><span class="line">      f_t = torch.sigmoid(x_t @ self.U_f + h_t @ self.V_f + self.b_f)</span><br><span class="line">      g_t = torch.tanh(x_t @ self.U_c + h_t @ self.V_c + self.b_c)</span><br><span class="line">      o_t = torch.sigmoid(x_t @ self.U_o + h_t @ self.V_o + self.b_o)</span><br><span class="line">      c_t = f_t * c_t + i_t * g_t</span><br><span class="line">      h_t = o_t * torch.tanh(c_t)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> h_t, c_t</span><br></pre></td></tr></table></figure><p>这样，我们能得到最后一次输出的权重。比如说要预测接下来16个时间步的内容，我们就可以利用这些数据，进一步在模型中forward，得到新的输出 $y_{pred}$，和真实的 $y$ 计算误差。</p><h1 id="进一步优化"><a href="#进一步优化" class="headerlink" title="进一步优化"></a>进一步优化</h1><p>注意到四个门的计算都是互相并行的，我们可以用一次矩阵运算来加速操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CustomLSTM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_sz, hidden_sz</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.input_sz = input_sz</span><br><span class="line">        self.hidden_size = hidden_sz</span><br><span class="line">        self.W = nn.Parameter(torch.Tensor(input_sz, hidden_sz * <span class="number">4</span>))</span><br><span class="line">        self.U = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz * <span class="number">4</span>))</span><br><span class="line">        self.bias = nn.Parameter(torch.Tensor(hidden_sz * <span class="number">4</span>))</span><br><span class="line">        self.init_weights()</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 初始化模型权重</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">self</span>):</span><br><span class="line">        stdv = <span class="number">1.0</span> / math.sqrt(self.hidden_size)</span><br><span class="line">        <span class="keyword">for</span> weight <span class="keyword">in</span> self.parameters():</span><br><span class="line">            weight.data.uniform_(-stdv, stdv)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, </span></span><br><span class="line"><span class="params">                init_states=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Assumes x is of shape (batch, sequence, feature)&quot;&quot;&quot;</span></span><br><span class="line">        bs, seq_sz, _ = x.size()</span><br><span class="line">        hidden_seq = []</span><br><span class="line">        <span class="keyword">if</span> init_states <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            h_t, c_t = (torch.zeros(bs, self.hidden_size).to(x.device), </span><br><span class="line">                        torch.zeros(bs, self.hidden_size).to(x.device))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            h_t, c_t = init_states</span><br><span class="line"> </span><br><span class="line">        HS = self.hidden_size</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(seq_sz):</span><br><span class="line">            x_t = x[:, t, :]</span><br><span class="line">            <span class="comment"># batch the computations into a single matrix multiplication</span></span><br><span class="line">            gates = x_t @ self.W + h_t @ self.U + self.bias</span><br><span class="line">            i_t, f_t, g_t, o_t = (</span><br><span class="line">                torch.sigmoid(gates[:, :HS]), <span class="comment"># input</span></span><br><span class="line">                torch.sigmoid(gates[:, HS:HS*<span class="number">2</span>]), <span class="comment"># forget</span></span><br><span class="line">                torch.tanh(gates[:, HS*<span class="number">2</span>:HS*<span class="number">3</span>]),</span><br><span class="line">                torch.sigmoid(gates[:, HS*<span class="number">3</span>:]), <span class="comment"># output</span></span><br><span class="line">            )</span><br><span class="line">            c_t = f_t * c_t + i_t * g_t</span><br><span class="line">            h_t = o_t * torch.tanh(c_t)</span><br><span class="line">            hidden_seq.append(h_t.unsqueeze(<span class="number">0</span>))</span><br><span class="line">        hidden_seq = torch.cat(hidden_seq, dim=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># reshape from shape (sequence, batch, feature) to (batch, sequence, feature)</span></span><br><span class="line">        hidden_seq = hidden_seq.transpose(<span class="number">0</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">        <span class="keyword">return</span> hidden_seq, (h_t, c_t)</span><br></pre></td></tr></table></figure><p>我也不太清楚算出每一步的 hidden_seq 有什么用，为什么只存 $h_t$ 而不存 $C_t$ ？</p><p>问了 ChatGPT，它是这么回答 hidden_seq 的作用的：</p><ul><li><p>序列分类：如果你的任务是对整个序列进行分类，你可以使用 hidden_seq 中的最后一个时间步的隐藏状态 h_t 或者对整个序列的隐藏状态进行池化操作（例如平均池化或最大池化），然后将其传递给分类器进行分类预测。</p></li><li><p>序列标注：在自然语言处理中，你可以使用 hidden_seq 来生成每个时间步的标注结果，例如词性标注或命名实体识别。</p></li><li><p>序列生成：如果你的任务是生成新的序列，如文本生成，你可以使用 hidden_seq 作为生成器的输入，以生成接下来的序列内容。</p></li><li><p>注意力机制：hidden_seq 可用于计算注意力权重，以确定序列中不同时间步的重要性，然后对序列的不同部分进行加权汇总。</p></li><li><p>可视化和分析：hidden_seq 可以用于可视化和分析模型在输入序列上的学习过程。你可以查看隐藏状态的变化，了解模型如何处理不同时间步的信息。</p></li></ul><p>嗯，暂时还不理解。</p><h1 id="PyTorch-API"><a href="#PyTorch-API" class="headerlink" title="PyTorch API"></a>PyTorch API</h1><p>也可以直接调PyTorch的API，会方便很多</p><p>我们用 <a href="https://github.com/L1aoXingyu/code-of-learn-deep-learning-with-pytorch/blob/master/chapter5_RNN/time-series/lstm-time-series.ipynb">https://github.com/L1aoXingyu/code-of-learn-deep-learning-with-pytorch/blob/master/chapter5_RNN/time-series/lstm-time-series.ipynb</a> 中的例子来实践一下</p><p>十年飞机客流量数据：<a href="https://github.com/L1aoXingyu/code-of-learn-deep-learning-with-pytorch/blob/master/chapter5_RNN/time-series/data.csv">https://github.com/L1aoXingyu/code-of-learn-deep-learning-with-pytorch/blob/master/chapter5_RNN/time-series/data.csv</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">data_csv = pd.read_csv(<span class="string">&#x27;./data.csv&#x27;</span>, usecols=[<span class="number">1</span>])</span><br><span class="line">plt.plot(data_csv)</span><br></pre></td></tr></table></figure><p>数据如下图所示：</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picst.sunbangyan.cn/2023/10/19/ad2d62c0b67d9370b1e9fb4571081b17.png" style="width:800px;"/></div></div><p>我们的任务目标是通过前几个月的客流量数据去预测后几个月的客流量数据。比如说我们要用前两个月的数据去预测后一个月的数据。</p><p>首先进行数据处理，除掉空数据（虽然这个数据集里好像没有），然后执行归一化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line">data_csv = data_csv.dropna()</span><br><span class="line">dataset = data_csv.values</span><br><span class="line">dataset = dataset.astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">max_value = np.<span class="built_in">max</span>(dataset)</span><br><span class="line">min_value = np.<span class="built_in">min</span>(dataset)</span><br><span class="line">scalar = max_value - min_value</span><br><span class="line">dataset = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x / scalar, dataset))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_dataset</span>(<span class="params">dataset, look_back=<span class="number">2</span></span>):</span><br><span class="line">    dataX, dataY = [], []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dataset) - look_back):</span><br><span class="line">        a = dataset[i:(i + look_back)]</span><br><span class="line">        dataX.append(a)</span><br><span class="line">        dataY.append(dataset[i + look_back])</span><br><span class="line">    <span class="keyword">return</span> np.array(dataX), np.array(dataY)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建好输入输出</span></span><br><span class="line">data_X, data_Y = create_dataset(dataset)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集，70% 作为训练集</span></span><br><span class="line">train_size = <span class="built_in">int</span>(<span class="built_in">len</span>(data_X) * <span class="number">0.7</span>)</span><br><span class="line">test_size = <span class="built_in">len</span>(data_X) - train_size</span><br><span class="line">train_X = data_X[:train_size]</span><br><span class="line">train_Y = data_Y[:train_size]</span><br><span class="line">test_X = data_X[train_size:]</span><br><span class="line">test_Y = data_Y[train_size:]</span><br></pre></td></tr></table></figure><p>看下网络的定义，先来介绍一下 <code>nn.LSTM</code> 的参数：</p><ul><li>input_size – 输入的特征维度</li><li>hidden_size – 隐状态的特征维度</li><li>num_layers – 堆叠LSTM的层数</li><li>batch_first – 如果为True，那么输入和输出Tensor的形状为(batch, seq, feature)，默认为false，是(seq, batch, feature)</li><li>bidirectional – 是否双向传播，默认为False</li></ul><p>LSTM输出: output, (h_n, c_n)</p><p>output (seq_len, batch, hidden_size <em> num_directions):LSTM的输出序列，对于每个时间步，它包含了每个样本的隐藏状态。<br>h_n (num_layers </em> num_directions, batch, hidden_size):保存着LSTM最后一个时间步的隐状态<br>c_n (num_layers * num_directions, batch, hidden_size):保存着LSTM最后一个时间步的细胞状态</p><p>其中 h_n 应该就是 output[-1] （最后一项）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 例</span></span><br><span class="line">lstm = nn.LSTM(<span class="number">10</span>, <span class="number">512</span>, <span class="number">2</span>, bidirectional=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>所以，我们要把输入的数据先做一下维度变换：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">train_X = train_X.reshape(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">train_Y = train_Y.reshape(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">test_X = test_X.reshape(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">train_x = torch.from_numpy(train_X)</span><br><span class="line">train_y = torch.from_numpy(train_Y)</span><br><span class="line">test_x = torch.from_numpy(test_X)</span><br></pre></td></tr></table></figure><p>然后我们定义模型，开始训练。</p><p>注意最后一层线性层，我们需要把当前的输出hidden_size转化成我们需要的output_size。</p><p>线性层一般接收一个二维输入，忽略第一维的batch_size，转换hidden_size，不过因为前面一层是LSTM的输出，所以是三维的，我们不妨把batch_size和seq_len先“拼起来”，毕竟我们的目的只是想把隐藏层输出转化为目标输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">lstm_reg</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size=<span class="number">1</span>, num_layers=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        self.lstm = nn.LSTM(input_size, hidden_size, num_layers) <span class="comment"># lstm</span></span><br><span class="line">        self.reg = nn.Linear(hidden_size, output_size) <span class="comment"># 回归</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x, _ = self.lstm(x) <span class="comment"># (seq, batch, hidden)</span></span><br><span class="line">        s, b, h = x.shape</span><br><span class="line">        x = x.view(s*b, h) <span class="comment"># 转换成线性层的输入格式</span></span><br><span class="line">        x = self.reg(x)</span><br><span class="line">        x = x.view(s, b, -<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">model = lstm_reg(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">1e-2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    var_x = Variable(train_x)</span><br><span class="line">    var_y = Variable(train_y)</span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    out = model(var_x)</span><br><span class="line">    loss = criterion(out, var_y)</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    if (e + 1) % 100 == 0: # 每 100 次输出结果</span></span><br><span class="line"><span class="string">        print(&#x27;Epoch: &#123;&#125;, Loss: &#123;:.5f&#125;&#x27;.format(e + 1, loss.data[0]))</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> (step + <span class="number">1</span>) % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Step:&#x27;</span>, <span class="string">&#x27;%04d&#x27;</span> % (step + <span class="number">1</span>), <span class="string">&#x27;cost =&#x27;</span>, <span class="string">&#x27;&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(loss))</span><br></pre></td></tr></table></figure><p>训练好之后，我们就可以去做预测了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">net = net.<span class="built_in">eval</span>() <span class="comment"># 转换成测试模式</span></span><br><span class="line"></span><br><span class="line">data_X = data_X.reshape(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">data_X = torch.from_numpy(data_X)</span><br><span class="line">var_data = Variable(data_X)</span><br><span class="line">pred_test = net(var_data) <span class="comment"># 测试集的预测结果</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 改变输出的格式</span></span><br><span class="line">pred_test = pred_test.view(-<span class="number">1</span>).data.numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出实际结果和预测的结果</span></span><br><span class="line">plt.plot(pred_test, <span class="string">&#x27;r&#x27;</span>, label=<span class="string">&#x27;prediction&#x27;</span>)</span><br><span class="line">plt.plot(dataset, <span class="string">&#x27;b&#x27;</span>, label=<span class="string">&#x27;real&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br></pre></td></tr></table></figure><p>结果如下图所示：</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picst.sunbangyan.cn/2023/10/19/04bdfe4d9588f5405fd426ac51d32979.png" style="width:800px;"/></div></div><p class='p left logo large'>参考链接</p><p><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p><p><a href="https://zhuanlan.zhihu.com/p/451985132">https://zhuanlan.zhihu.com/p/451985132</a></p><p><a href="https://github.com/L1aoXingyu/code-of-learn-deep-learning-with-pytorch/blob/master/chapter5_RNN/time-series/lstm-time-series.ipynb">https://github.com/L1aoXingyu/code-of-learn-deep-learning-with-pytorch/blob/master/chapter5_RNN/time-series/lstm-time-series.ipynb</a></p>]]></content>
    
    
    <summary type="html">学习LSTM！</summary>
    
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/categories/Study/"/>
    
    <category term="Neural Network" scheme="https://anti-entrophic.github.io/categories/Study/Neural-Network/"/>
    
    
    <category term="Neural Network" scheme="https://anti-entrophic.github.io/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch学习笔记（二） CNN</title>
    <link href="https://anti-entrophic.github.io/posts/10011.html"/>
    <id>https://anti-entrophic.github.io/posts/10011.html</id>
    <published>2023-10-18T13:12:07.000Z</published>
    <updated>2023-10-19T07:16:07.227Z</updated>
    
    <content type="html"><![CDATA[<p>这次来用PyTorch实现一下CNN卷积神经网络, 数据我们采用 MNIST 这个手写数字识别的数据库, 完成一个多分类任务（判断是哪个数字）</p><p>不清楚PyTorch基本用法请移步 <a href="https://anti-entrophic.github.io/posts/10010.html">https://anti-entrophic.github.io/posts/10010.html</a></p><p class='p left logo large'>概述</p><p>最简单的CNN的结构是 “-&gt;卷积层-&gt;激活函数-&gt;池化层-&gt;线性层”，这里先简单介绍一下，后面会配合代码详细描述。</p><p>卷积层目标就是训练若干个卷积核，期望这些卷积核能够学到图像的某些特征。图像的各个通道会通过各个卷积核，得到卷积操作后的结果，然后经过ReLU激活函数。</p><p>池化层就如下图，目的是为了给图像降维，减少参数，并且期望能够捕捉一些关键特征，忽略不重要的细节</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://pic1.zhimg.com/80/v2-6091b01b4e85b1c23f3b7cf9f1496c90_1440w.webp" style="width:400px;"/></div></div><p>最后压缩维度后经过一个线性层，得到最终结果的概率分布，然后利用交叉熵损失函数来进行优化。</p><p>更详细的：<a href="https://zhuanlan.zhihu.com/p/630695553">https://zhuanlan.zhihu.com/p/630695553</a></p><p class='p left logo large'>数据</p><p>我们可以很方便的通过 <code>torchvision</code> 这个包下载到 MNIST 这个数据库</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取数据集</span></span><br><span class="line">train_data = torchvision.datasets.MNIST(</span><br><span class="line">    root = <span class="string">&#x27;./MNIST/&#x27;</span>,</span><br><span class="line">    train = <span class="literal">True</span>,</span><br><span class="line">    transform = torchvision.transforms.ToTensor(),</span><br><span class="line">    download = <span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_data = torchvision.datasets.MNIST(root=<span class="string">&#x27;./MNIST/&#x27;</span>, train=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>这样下载下来一个是训练集，一个是验证集。并且下载下来就是 <code>torch.utils.data.Dataset</code> 类，可以很好地适配 PyTorch 中常用的 Dataloader</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_loader = Data.DataLoader(</span><br><span class="line">    dataset = train_data,</span><br><span class="line">    batch_size = <span class="number">50</span>,</span><br><span class="line">    shuffle = <span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><code>Dataloader</code> 可以很方便地完成将数据组成batch，随机取样等操作。</p><p>可以简单看一下 MNIST 这个数据集，每张图片的大小都是 28*28，训练样本有60000个，测试样本有10000个</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(train_data.data.shape)</span><br><span class="line"><span class="built_in">print</span>(test_data.data.shape)</span><br><span class="line"><span class="comment"># -----output-----</span></span><br><span class="line"><span class="comment"># torch.Size([60000, 28, 28])</span></span><br><span class="line"><span class="comment"># torch.Size([10000, 28, 28])</span></span><br></pre></td></tr></table></figure><p class='p left logo large'>网络结构</p><p>卷积层的输入是三维的，第一维是图像的通道数。</p><p>这个 <code>nn.Conv2d</code> ，<code>in_channels</code>就是输入图像的通道数，灰度图像就是1，RGB图像就是3。<code>output_channels</code>就是卷积核数，也是输出图像的通道数。</p><p>如果<code>in_channels</code>是3的话，那对每个卷积核，都是对3个通道各自卷积，然后加起来，会得到16个卷积后的通道，最后合在一起。</p><p><code>kernel_size</code> 就是卷积核的大小，<code>stride</code> 是卷积核移动的步长，<code>padding</code> 是周围补0，控制卷积后图像的大小。</p><p><code>ReLU()</code> 就激活一下，不过我有点疑惑的是，卷积操作完之后，会不会某些点的intensity超过255？因为这在图像中应该是不可能的情况，但是好像直接就没有处理；小于0的话经过ReLU()可以调回来。</p><p><code>nn.MaxPool2d</code> 就是一个池化层，如文章开头图片所示，取2x2格中的最大值。</p><p>最终第一层的维度变化为 (batch_size, 1, 28, 28) -&gt; (batch_size, 16, 14, 14)</p><p>第二层的维度变化为 (batch_size, 16, 14, 14) -&gt; (batch_size, 32, 7, 7)</p><p>线性层的维度变化为 -&gt; (batch, 32<em>7</em>7) -&gt; (batch, 10)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__() <span class="comment"># 在Python3中，不再需要显式地传递参数，会自动地识别当前类及其继承的父类，所以写super().__init__()更好</span></span><br><span class="line"></span><br><span class="line">        self.convl = nn.Sequential(</span><br><span class="line">            nn.Conv2d(</span><br><span class="line">                in_channels=<span class="number">1</span>,</span><br><span class="line">                out_channels=<span class="number">16</span>,</span><br><span class="line">                kernel_size=<span class="number">5</span>,</span><br><span class="line">                stride=<span class="number">1</span>,</span><br><span class="line">                padding=<span class="number">2</span></span><br><span class="line">            ),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(</span><br><span class="line">                kernel_size=<span class="number">2</span></span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.conv2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(</span><br><span class="line">                in_channels=<span class="number">16</span>,</span><br><span class="line">                out_channels=<span class="number">32</span>,</span><br><span class="line">                kernel_size=<span class="number">5</span>,</span><br><span class="line">                stride=<span class="number">1</span>,</span><br><span class="line">                padding=<span class="number">2</span></span><br><span class="line">            ),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(</span><br><span class="line">                kernel_size=<span class="number">2</span></span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.out = nn.Linear(<span class="number">32</span>*<span class="number">7</span>*<span class="number">7</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.convl(x) <span class="comment"># (batch_size, 1, 28, 28) -&gt; (batch_size, 16, 14, 14)</span></span><br><span class="line">        x = self.conv2(x) <span class="comment"># (batch_size, 16, 14, 14) -&gt; (batch_size, 32, 7, 7)</span></span><br><span class="line">        x = x.view(x.shape[<span class="number">0</span>], -<span class="number">1</span>) <span class="comment"># (batch, 32*7*7)</span></span><br><span class="line">        x = self.out(x) <span class="comment"># (batch, 10)</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p class='p left logo large'>网络训练</p><p>对于分类的概率分布，损失函数用交叉熵损失，原因我在其它文章中也提到过，详细链接：<a href="https://zhuanlan.zhihu.com/p/115277553">https://zhuanlan.zhihu.com/p/115277553</a></p><p>优化器没有用SGD而是Adam，不知道具体会有什么差异</p><p>只训练一个epoch</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">model = CNN()</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> step,(batch_x,batch_y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        pred_y = model(batch_x)</span><br><span class="line">        loss = criterion(pred_y, batch_y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (step + <span class="number">1</span>) % <span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Step:&#x27;</span>, <span class="string">&#x27;%04d&#x27;</span> % (step + <span class="number">1</span>), <span class="string">&#x27;cost =&#x27;</span>, <span class="string">&#x27;&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(loss))</span><br><span class="line">        </span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure><p class='p left logo large'>结果</p><p>看一下结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这步unsqueeze让test_data从(10000,28,28)-&gt;(10000,1,28,28)，适配CNN的输入</span></span><br><span class="line">test_x = torch.unsqueeze(test_data.data,dim=<span class="number">1</span>).<span class="built_in">float</span>()[:<span class="number">2000</span>] </span><br><span class="line"><span class="comment"># 取出前2000个验证样本的标签</span></span><br><span class="line">test_y = test_data.targets[:<span class="number">2000</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 简单试一下前20个的输出结果</span></span><br><span class="line">test_output = model(test_x[:<span class="number">20</span>])</span><br><span class="line"><span class="comment"># 这里，test_output的维度是(20,10)，torch.max会返回两个值，一个是value，一个是index，index就代表着分类为哪个数字</span></span><br><span class="line"><span class="comment"># torch.max(_, 1) 表示沿着test_output的第1维也就是10这一维去找最大值，找的就是每一个概率分布中的最大值</span></span><br><span class="line">pred_y = torch.<span class="built_in">max</span>(test_output, <span class="number">1</span>)[<span class="number">1</span>].numpy()</span><br><span class="line"><span class="built_in">print</span>(pred_y, <span class="string">&#x27;prediction number&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(test_y[:<span class="number">20</span>].numpy(),<span class="string">&#x27;real number&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----output-----</span></span><br><span class="line"><span class="comment"># [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4] prediction number</span></span><br><span class="line"><span class="comment"># [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4] real number</span></span><br></pre></td></tr></table></figure><p class='p left logo large'>总结</p><p>只是最简单的CNN吧，不过结果确实挺好，网络就真的学到特征了。也没去试过换一下优化器啊激活函数会有什么效果，只是学一下基础知识顺便学习PyTorch用法吧</p>]]></content>
    
    
    <summary type="html">使用 torchvision.datasets 中的 MNIST 数据集进行训练</summary>
    
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/categories/Study/"/>
    
    <category term="Neural Network" scheme="https://anti-entrophic.github.io/categories/Study/Neural-Network/"/>
    
    
    <category term="Neural Network" scheme="https://anti-entrophic.github.io/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch学习笔记（一） 基础</title>
    <link href="https://anti-entrophic.github.io/posts/10010.html"/>
    <id>https://anti-entrophic.github.io/posts/10010.html</id>
    <published>2023-10-17T10:48:37.000Z</published>
    <updated>2023-10-18T12:19:17.302Z</updated>
    
    <content type="html"><![CDATA[<p>先尝试写一个简单的线性计算，用神经网络拟合学习。</p><p class='p left logo large'>数据生成</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成数据</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 给定权重矩阵</span></span><br><span class="line">W = torch.tensor([[<span class="number">0.3</span>, <span class="number">0.4</span>],[<span class="number">0.5</span>,<span class="number">0.6</span>],[<span class="number">0.6</span>,<span class="number">0.2</span>]])</span><br><span class="line"></span><br><span class="line">num_samples = <span class="number">100</span></span><br><span class="line">input_dim = <span class="number">3</span></span><br><span class="line">x = torch.rand((num_samples, input_dim))</span><br><span class="line"></span><br><span class="line">y = torch.matmul(W.t(), x.t()).t()</span><br></pre></td></tr></table></figure><p>一会儿我们用这些数据取拟合权重矩阵 $W$</p><p class='p left logo large'>定义网络</p><p>pytorch中的自定义神经网络类需要继承 <code>nn.Module</code> ，并且调用父类的构造函数 <code>__init__()</code> 来完成一些参数和方法的初始化。</p><p>最简单的网络我们只需定义网络中的层结构，如这里我加了一层线性层。以及前向传播的 <code>forward</code> 函数即可。</p><p>如果需要自定义损失函数等，会在之后的文章中介绍。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Simple_nn</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Simple_nn, self).__init__()</span><br><span class="line">        self.W = nn.Linear(input_dim, output_dim, bias=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># X : [batch_size, input_dim]</span></span><br><span class="line">        <span class="comment"># 在forward中的X还是带有batch_size这一维度的，不过在经过链接层的时候，pytorch会自动处理批次，不用显式考虑。</span></span><br><span class="line">        output = self.W(X)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p class='p left logo large'>进行训练</p><p>训练需要数据，我们把数据以batch的形式送进网络进行训练。目前我们不涉及dataset，dataloader的使用。</p><p>先写一个随机取batch的函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">4</span> <span class="comment"># mini-batch size</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">random_batch</span>():</span><br><span class="line">    random_x = torch.zeros((batch_size, input_dim))</span><br><span class="line">    random_y = torch.zeros((batch_size, output_dim))</span><br><span class="line">    <span class="comment"># 定义一个随机抽取batch</span></span><br><span class="line">    random_index = np.random.choice(<span class="built_in">range</span>(<span class="built_in">len</span>(x)), batch_size, replace=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, index <span class="keyword">in</span> <span class="built_in">enumerate</span>(random_index):</span><br><span class="line">        random_x[i] = x[index]</span><br><span class="line">        random_y[i] = y[index]</span><br><span class="line">                </span><br><span class="line">    <span class="keyword">return</span> random_x, random_y</span><br></pre></td></tr></table></figure><p>然后写一个训练函数</p><p>pytorch封装了很多操作，比如说反向传播 <code>backward</code> 是不需要自己写的，只需要调用 <code>loss.backward()</code> 即可</p><p>随后，调用 <code>torch.optim</code> 中被称为优化器的工具就可以更新参数</p><p>像这里的 <code>optim.SGD</code> 就是随机梯度下降的参数更新方法。</p><p>注意，模型直到 <code>optimizer.step()</code> 这步才正式更新模型参数。这两步的分离是为了提供更大的灵活性，比如可以多次调用 <code>loss.backward()</code>累积梯度，然后在特定时刻执行一次 <code>optimizer.step()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = Simple_nn()</span><br><span class="line"><span class="comment"># 定义损失函数 </span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5000</span>):</span><br><span class="line">    input_batch, output_batch = random_batch()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 清除梯度缓存</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    output_pred = model(input_batch)</span><br><span class="line">    <span class="comment"># 计算损失函数</span></span><br><span class="line">    loss = criterion(output_pred, output_batch)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Epoch:&#x27;</span>, <span class="string">&#x27;%04d&#x27;</span> % (epoch + <span class="number">1</span>), <span class="string">&#x27;cost =&#x27;</span>, <span class="string">&#x27;&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(loss))</span><br><span class="line"></span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model.W.weight.t())</span><br></pre></td></tr></table></figure><p>最后得到输出，和我们一开始定义的 $W$ 是很接近的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.2985, 0.4066],</span><br><span class="line">        [0.5058, 0.5921],</span><br><span class="line">        [0.5955, 0.2015]], grad_fn=&lt;TBackward0&gt;)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">记录Pytorch的学习过程，目标是逐步深入，从基础到完整大型的pytorch架构。</summary>
    
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/categories/Study/"/>
    
    <category term="Neural Network" scheme="https://anti-entrophic.github.io/categories/Study/Neural-Network/"/>
    
    
    <category term="Neural Network" scheme="https://anti-entrophic.github.io/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>Nunjucks Error expected variable end解决办法</title>
    <link href="https://anti-entrophic.github.io/posts/10009.html"/>
    <id>https://anti-entrophic.github.io/posts/10009.html</id>
    <published>2023-10-17T04:36:19.000Z</published>
    <updated>2023-10-17T11:57:12.674Z</updated>
    
    <content type="html"><![CDATA[<p>在文章中写latex代码的时候，可能会遇到报错：Nunjucks Error expected variable end</p><p>比如下面这段</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">$</span><span class="built_in">$</span></span><br><span class="line"><span class="keyword">\begin</span>&#123;equation&#125;</span><br><span class="line">    <span class="keyword">\begin</span>&#123;array&#125;&#123;ll&#125;</span><br><span class="line">        p(w<span class="built_in">_</span>j|w<span class="built_in">_</span>i) <span class="built_in">&amp;</span>= y<span class="built_in">_</span>j <span class="keyword">\\</span></span><br><span class="line">                   <span class="built_in">&amp;</span>= <span class="keyword">\frac</span>&#123;e<span class="built_in">^</span>&#123;u<span class="built_in">_</span>j&#125;&#125;&#123;<span class="keyword">\sum</span><span class="built_in">_</span>&#123;k <span class="keyword">\in</span> V&#125; e<span class="built_in">^</span>&#123;u<span class="built_in">_</span>k&#125;&#125;   <span class="keyword">\\</span></span><br><span class="line">                   <span class="built_in">&amp;</span>= <span class="keyword">\frac</span>&#123; e<span class="built_in">^</span>&#123;&#123;W&#x27;<span class="built_in">_</span>j&#125;<span class="built_in">^</span>T <span class="keyword">\cdot</span> W<span class="built_in">_</span>I&#125; &#125; &#123;<span class="keyword">\sum</span><span class="built_in">_</span>&#123;k <span class="keyword">\in</span> V&#125; e<span class="built_in">^</span>&#123; &#123;W&#x27;<span class="built_in">_</span>k&#125;<span class="built_in">^</span>T <span class="keyword">\cdot</span> W<span class="built_in">_</span>I&#125; &#125; <span class="keyword">\\</span></span><br><span class="line">    <span class="keyword">\end</span>&#123;array&#125; <span class="keyword">\notag</span></span><br><span class="line"><span class="keyword">\end</span>&#123;equation&#125;</span><br><span class="line"><span class="built_in">$</span><span class="built_in">$</span></span><br></pre></td></tr></table></figure><p>查阅资料后是因为，Hexo使用Nunjucks渲染帖子，用 <code>&#123; &#123; &#125; &#125;</code> 或 <code>&#123;% %&#125;</code> 包装的内容将被解析，并可能导致问题。</p><p>可以使用下列标签包裹敏感字段，避免被Nunjucks错误解析。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% raw %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% endraw%&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在文章中写latex代码的时候，可能会遇到报错：Nunjucks Error expected variable end&lt;/p&gt;
&lt;p&gt;比如下面这段&lt;/p&gt;
&lt;figure class=&quot;highlight latex&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutt</summary>
      
    
    
    
    <category term="Hexo &amp; Butterfly tutorial" scheme="https://anti-entrophic.github.io/categories/Hexo-Butterfly-tutorial/"/>
    
    
    <category term="Hexo &amp; Butterfly tutorial" scheme="https://anti-entrophic.github.io/tags/Hexo-Butterfly-tutorial/"/>
    
  </entry>
  
  <entry>
    <title>NLP论文精读（一） Word2Vec</title>
    <link href="https://anti-entrophic.github.io/posts/10008.html"/>
    <id>https://anti-entrophic.github.io/posts/10008.html</id>
    <published>2023-10-16T15:39:21.000Z</published>
    <updated>2023-10-17T15:06:32.868Z</updated>
    
    <content type="html"><![CDATA[<div class="note success simple"><p>最好的学习方法就是把知识讲给别人听</p></div><p>开个新坑，努力！封面图也换上对我意义非凡的 Chtholly</p><p class='p left logo large'>基本概念</p><p>基本概念懒得写了。</p><p>Word2Vec是Google的Mikolov在2013年提出的一种词向量的表征形式。不同于稀疏的one-hot编码，对样本空间的利用率只有坐标轴上可怜的几个点; word embedding 就可以以更少的维度表示词语，更高效地利用样本空间，并可使单词的向量表征具有一定几何意义。</p><p class='p left logo large'>网络结构</p><p>Word2Vec一共给出了两种网络结构，CBOW和Skip-gram</p><h1 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h1><p>CBOW的任务简单来说就是，给定某个单词 $w_t$ 的上下文 $w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2}$，去估计中间这个单词应该是什么，也就是要计算 $p(w_t | w_{t-2}, w_{t-2}, w_{t-2}, w_{t-2})$，词向量是这一任务的一项产物。</p><h2 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h2><p>我们先考虑较为简单的一个单词的情况，即计算 $p(w_t|w_{I})$</p><p>CBOW模型的输入是一个one-hot向量，考虑当前有一个长度为|V|的词表，不妨设 $w_{oI}$ 是词表中的第 $I$ 个词的one-hot表示（和 $w_{t+1}$ 这种原来句子中的空间位置关系作区别），也即只有第 $I$ 个元素是1</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://pic2.zhimg.com/80/v2-ca4c641f4f3c9a44e43260c04c0161d1_1440w.webp" alt="CBOW模型示例，图源知乎" style="width:400px;"/></div><span class="image-caption">CBOW模型示例，图源知乎</span></div><p>CBOW仅有一个隐层，设输入层到隐层的权重为 $W$，输入层到输出层的权重为 $W’$，隐层神经元个数为 $N$, $W_i$ 表示 $W$ 的第 $i$ 行，${W’_j}$ 表示 $W’$ 的第 $j$ 列。</p><p>因为input是one-hot向量，可以知道隐层的输入 $h$ 应该等于 $W$ 的第 $I$ 行 $W_I$ ，$h$ 的维度是 $N \times 1$。</p><p>CBOW的设计中省略了隐藏层的非线性激活函数，仅仅是做了一个加权组合。</p><p>随后经过权重 $W’$ ，得到一个向量$u$，其中 $u_i = {W’_i}^T \cdot W_I$ ，并进行一次softmax归一化得到结果 $y$</p><p>输出结果是一个 $V \times 1$ 的向量，每一个元素 $y_i$ 表示了预测词为词表中第 $i$ 个词的概率。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>对于一个输入 $w_{oi}$，我们已知它的输出应该是 $w_{oj}$，也就是说我们应该最大化目标函数$p(w_{oj}|w_{oi})$, 即 $y_j$</p>$$\begin{equation}    \begin{array}{ll}        p(w_{oj}|w_{oi}) &= y_j \\                   &= \frac{e^{u_j}}{\sum_{k \in V} e^{u_k}}   \\                   &= \frac{ e^{{W'_j}^T \cdot W_I} } {\sum_{k \in V} e^{ {W'_k}^T \cdot W_I} } \\    \end{array} \notag\end{equation}$$<p>我们对其取对数,则</p><script type="math/tex; mode=display">log \; p(w_{oj}|w_{oi}) = {W'_j}^T \cdot W_I - log{\sum_{k \in V} {W'_k}^T \cdot W_I}</script><p>损失函数就是目标函数取负就可以了。</p><p>如果有多个输入，仅需在第一步的时候，对各个输入均经过相同的权重矩阵 $W$ ，最后隐藏层输入 $h$ 平均即可。</p><p>（关于对这步多个输入求和平均，我一开始也有些困惑，感觉会不会对效果产生什么影响。不过如果是单个输入的话，可能不同输入之间的影响抵消的影响会比较大。作者应该也是实验过了现在的结果好？不懂。然后模型也抛弃了和中心词相隔距离的参数。）</p><p>考虑上下文窗口长度为 $c$（前后各 $c$ 个总共 $2c$ 个），我们要求的损失函数就是：</p><script type="math/tex; mode=display">\sum_{-c \leq j \leq c, j \neq 0}log \; p(w_t|w_{t+j});</script><p>这个其实可以直接用交叉熵损失函数算，原因在：<a href="https://zhuanlan.zhihu.com/p/115277553">https://zhuanlan.zhihu.com/p/115277553</a> ，写得特别好！值得我重新开一篇博文记录一下。 </p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>作者很厉害，那个时候没有Tensorflow、PyTorch这种深度学习框架，整个代码是C语言完成的并且所有梯度都是手算的。</p><p>其实用上深度学习框架后网络非常简单，损失函数用 <code>criterion = nn.CrossEntropyLoss()</code> 就可以了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Model</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Word2Vec</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Word2Vec, self).__init__()</span><br><span class="line">        <span class="comment"># 输入层到隐层权重矩阵</span></span><br><span class="line">        self.W = nn.Linear(voc_size, embedding_size, bias=<span class="literal">False</span>) <span class="comment"># voc_size &gt; embedding_size Weight</span></span><br><span class="line">        <span class="comment"># 隐层到输出层权重矩阵</span></span><br><span class="line">        self.WT = nn.Linear(embedding_size, voc_size, bias=<span class="literal">False</span>) <span class="comment"># embedding_size &gt; voc_size Weight</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># X : [batch_size, voc_size]</span></span><br><span class="line">        hidden_layer = self.W(X) <span class="comment"># hidden_layer : [batch_size, embedding_size]</span></span><br><span class="line">        output_layer = self.WT(hidden_layer) <span class="comment"># output_layer : [batch_size, voc_size]</span></span><br><span class="line">        <span class="keyword">return</span> output_layer</span><br></pre></td></tr></table></figure><h1 id="skip-gram"><a href="#skip-gram" class="headerlink" title="skip-gram"></a>skip-gram</h1><p>说实话我觉得就是CBOW倒过来，，，形式也是一样的。</p><p class='p left logo large'>效率优化</p><h1 id="hierarchical-softmax"><a href="#hierarchical-softmax" class="headerlink" title="hierarchical softmax"></a>hierarchical softmax</h1><p>TODO</p><p class='p left logo large'>参考链接</p><p><a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/</a> 超级保姆级教程<br><a href="https://adoni.github.io/2017/11/08/word2vec-pytorch/">https://adoni.github.io/2017/11/08/word2vec-pytorch/</a> </p>]]></content>
    
    
    <summary type="html">开始论文学习之路</summary>
    
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/categories/Study/"/>
    
    <category term="NLP" scheme="https://anti-entrophic.github.io/categories/Study/NLP/"/>
    
    
    <category term="NLP" scheme="https://anti-entrophic.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>计算机网络 Wireshark的使用</title>
    <link href="https://anti-entrophic.github.io/posts/10007.html"/>
    <id>https://anti-entrophic.github.io/posts/10007.html</id>
    <published>2023-10-16T09:30:21.000Z</published>
    <updated>2023-10-16T10:23:32.209Z</updated>
    
    <content type="html"><![CDATA[<p>随便写写</p><p>Wireshark是网络包分析工具，主要作用是在接囗实时捕捉网络包，并详细显示包的协议信息。</p><ul><li><p>可以捕捉多种网络接囗类型的包，包括无线局域网接囗。</p></li><li><p>可以支持多种协议的解码，如TCP，DNS等。</p></li></ul><h1 id="过滤器"><a href="#过滤器" class="headerlink" title="过滤器"></a>过滤器</h1><p>Wireshark的过滤器分为捕获过滤器和显示过滤器</p><ul><li><p>前者需要在捕捉前设置好，决定捕捉什么包</p></li><li><p>后者在捕获过程中及结束后可以随时修改，只是显示捕获结果符合要求的包</p></li></ul><h1 id="捕获过滤器语法"><a href="#捕获过滤器语法" class="headerlink" title="捕获过滤器语法"></a>捕获过滤器语法</h1><p>这里很全：<a href="https://blog.csdn.net/qq_39720249/article/details/128157288">https://blog.csdn.net/qq_39720249/article/details/128157288</a></p><h2 id="基于协议过滤"><a href="#基于协议过滤" class="headerlink" title="基于协议过滤"></a>基于协议过滤</h2><ul><li>例：只捕获端口为80的tcp数据包</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcp port 80</span><br></pre></td></tr></table></figure><h2 id="基于方向过滤"><a href="#基于方向过滤" class="headerlink" title="基于方向过滤"></a>基于方向过滤</h2><p>可以指定获取 源src 或是 目的dst 方向的数据包，也可以用 src and dst 或是 src or dst</p><ul><li>例：只捕获目的ip为本机ip的ipv4数据包</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dst host &lt;本机ip&gt;</span><br></pre></td></tr></table></figure><h2 id="基于类型过滤"><a href="#基于类型过滤" class="headerlink" title="基于类型过滤"></a>基于类型过滤</h2><p>可选项有 主机host，网段net，端口port，端口范围portrange 等</p><ul><li>例：只捕获端口不为80的数据包</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">not port 80</span><br></pre></td></tr></table></figure><h1 id="显示过滤器语法"><a href="#显示过滤器语法" class="headerlink" title="显示过滤器语法"></a>显示过滤器语法</h1><p>例：显示ip为本机ip的dns数据包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip.addr == &lt;本机ip&gt; &amp;&amp; dns</span><br></pre></td></tr></table></figure><p>但是会发现这样什么包都没有。</p><p>将显示过滤器条件简化为dns后发现能正确找到相关dns数据包，但是其src和dst都是我本机的临时Ipv6地址。</p><p>将表达式更改为以下式子即可正确获取dns数据包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ipv6.addr == &lt;本机ip&gt; &amp;&amp; dns</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">作业，顺便记录一下Wireshark的使用方法</summary>
    
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/categories/Study/"/>
    
    <category term="Computer Network" scheme="https://anti-entrophic.github.io/categories/Study/Computer-Network/"/>
    
    
    <category term="Computer Network" scheme="https://anti-entrophic.github.io/tags/Computer-Network/"/>
    
    <category term="Study" scheme="https://anti-entrophic.github.io/tags/Study/"/>
    
    <category term="Notes" scheme="https://anti-entrophic.github.io/tags/Notes/"/>
    
  </entry>
  
  <entry>
    <title>FDU Operating System Lab2</title>
    <link href="https://anti-entrophic.github.io/posts/10006.html"/>
    <id>https://anti-entrophic.github.io/posts/10006.html</id>
    <published>2023-10-15T10:46:34.000Z</published>
    <updated>2023-11-13T10:28:14.766Z</updated>
    
    <content type="html"><![CDATA[<div class="note success simple"><p>薪火相传</p></div><p>因为中途才开始写博客，一些配置上的问题可能没讲清楚。目前只是自己做一个记录吧。</p><p>如果能帮到你理解这个lab就好啦</p><p>环境：WSL + xv6</p><p>LAB2：<a href="https://docs.qq.com/slide/DR2VtU3Fvb2hGWEN0">https://docs.qq.com/slide/DR2VtU3Fvb2hGWEN0</a></p><h1 id="实验准备"><a href="#实验准备" class="headerlink" title="实验准备"></a>实验准备</h1><p>切换到本次实验的环境分支下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git commit -am lab0</span><br><span class="line">git fetch</span><br><span class="line">git checkout syscall</span><br><span class="line">make clean</span><br></pre></td></tr></table></figure><p>随后按要求修改相应 <code>kernel</code> 与 <code>user</code> 文件夹下的文件，以任务1的 <code>procnum</code> 为例，其余同理。</p><p>在 <code>kernel/syscall.h</code> 中，添加一个宏定义</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> SYS_procnum 22</span></span><br></pre></td></tr></table></figure><p>然后在 <code>kernel/syscall.c</code> 指定系统调用的主体函数，即第22号System call会调用 <code>sys_procnum</code> 这个指针指向的函数</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">extern</span> uint64 <span class="title function_">sys_procnum</span><span class="params">(<span class="type">void</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">static</span> <span class="title function_">uint64</span> <span class="params">(*syscalls[])</span><span class="params">(<span class="type">void</span>)</span> = &#123;</span><br><span class="line"><span class="comment">//...</span></span><br><span class="line">[SYS_procnum]  sys_procnum,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 <code>kernel/sysproc.c</code> 中添加具体实现的主体函数（在后面会介绍）</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">uint64</span><br><span class="line"><span class="title function_">sys_procnum</span><span class="params">(<span class="type">void</span>)</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">//implementation of sys_procnum</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 <code>user/usys.pl</code> 中添加系统调用的存根</p><figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">entry(<span class="string">&quot;procnum&quot;</span>);</span><br></pre></td></tr></table></figure><p>具体而言，就是 <code>Makefile</code> 会调用这个 <code>Perl</code> 脚本，生成一段汇编码在 <code>usys.S</code> 中，这是 <code>user.h</code> 中定义的函数实际的实现，即调用的地方。</p><p>点进 <code>usys.S</code> 可以看到诸如 <code>li a7, SYS_fork</code> 这种，还记得之前把 <code>SYS_fork</code> 宏定义为了数字吧，就是在这里派用场。</p><p>为了能够让用户程序访问到 <code>procnum</code> 系统调用，我们需要在 <code>user/user.h</code> 中声明该调用：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// system calls</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">procnum</span><span class="params">(<span class="type">int</span>*)</span>;</span><br></pre></td></tr></table></figure><p>你可能会注意到这里有一个参数列表上的不匹配，会发现所有 <code>kernel/sysproc.c</code> 中的系统调用实现形参列表都是void。</p><p>这是因为 <code>procnum()</code> 这样的函数是用户级别的函数，而 <code>sys_procnum()</code> 是内核级别的函数，用户空间和内核空间有不同的数据访问规则和隔离。</p><p>前者需要将参数在用户空间打包成适当的数据结构，后者会通过辅助函数 <code>argaddr()</code> 或 <code>argint()</code> 等获取用户空间的参数到内核空间，并在其中进行合法与安全性检查，确保不会引发内核的错误。</p><p>可以看看其它system call的实现方法，比如wait()，是怎么获取参数列表的</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">argaddr</span><span class="params">(<span class="type">int</span> n, uint64 *addr)</span>;</span><br><span class="line"><span class="type">int</span> <span class="title function_">argint</span><span class="params">(<span class="type">int</span> n, <span class="type">int</span> *ip)</span>;</span><br></pre></td></tr></table></figure><h1 id="任务1：process-counting"><a href="#任务1：process-counting" class="headerlink" title="任务1：process counting"></a>任务1：process counting</h1><ul><li>系统调用功能 procnum：统计系统总进程数</li></ul><p>在user文件夹下添加检验程序 procnum.c</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;kernel/types.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;kernel/riscv.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;kernel/sysinfo.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;user/user.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span></span><br><span class="line"><span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> *argv[])</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">if</span>(argc &gt;= <span class="number">2</span>) &#123;</span><br><span class="line"><span class="built_in">fprintf</span>(<span class="number">2</span>, <span class="string">&quot;procnum: Too many arguments\n&quot;</span>);</span><br><span class="line"><span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> num = <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (procnum(&amp;num) &lt; <span class="number">0</span>) &#123;</span><br><span class="line"><span class="built_in">fprintf</span>(<span class="number">2</span>, <span class="string">&quot;procnum failed!\n&quot;</span>);</span><br><span class="line"><span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;Number of process: %d\n&quot;</span>, num);</span><br><span class="line"><span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在makefile中添加编译项</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">UPROGS=\</span><br><span class="line">    ...</span><br><span class="line">$U/_procnum\</span><br></pre></td></tr></table></figure><p>完成 <code>kernel/sysproc.c</code> 中函数的具体实现。我整体是对着 <code>sys_wait()</code> 函数学的。</p><p>先用 <code>argaddr()</code> 获取传过来的参数，是变量num的地址</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">uint64</span><br><span class="line"><span class="title function_">sys_procnum</span><span class="params">(<span class="type">void</span>)</span></span><br><span class="line">&#123;</span><br><span class="line">  uint64 p;</span><br><span class="line">  argaddr(<span class="number">0</span>, &amp;p);</span><br><span class="line">  <span class="keyword">return</span> procnum(p);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>和 <code>sys_wait()</code> 函数一样，我们把实现放在了 <code>kernel/proc.c</code> 中，记得在 <code>defs.h</code> 中先定义函数</p><p>因为内核空间和用户空间存在隔离，所以只能使用 <code>copyout()</code> 这种辅助函数实现修改用户空间的变量，而不能直接拿指针去操作。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Count the total number of processes in the system.</span></span><br><span class="line"><span class="type">int</span></span><br><span class="line"><span class="title function_">procnum</span><span class="params">(uint64 addr)</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">proc</span> *<span class="title">pp</span>;</span> <span class="comment">// 声明一个指向struct proc的指针pp，用于遍历进程表中的进程。</span></span><br><span class="line">  <span class="type">int</span> proc_num = <span class="number">0</span>; <span class="comment">// 统计进程数</span></span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">proc</span> *<span class="title">p</span> =</span> myproc(); <span class="comment">//获取当前进程的指针并存储在p中。</span></span><br><span class="line">  <span class="keyword">for</span>(pp = proc; pp &lt; &amp;proc[NPROC]; pp++)&#123; <span class="comment">//遍历整个进程表，查找子进程。</span></span><br><span class="line"><span class="keyword">if</span>(pp-&gt;state != <span class="number">0</span>) &#123; <span class="comment">// 只要进程状态不是unused</span></span><br><span class="line">proc_num++;</span><br><span class="line">&#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span>(addr != <span class="number">0</span> &amp;&amp; copyout(p-&gt;pagetable, addr, (<span class="type">char</span> *)&amp;proc_num, <span class="keyword">sizeof</span>(proc_num)) &lt; <span class="number">0</span>) &#123;</span><br><span class="line"><span class="comment">// 这个比较复杂，，首先检查 addr 是否为非零值。</span></span><br><span class="line"><span class="comment">// 如果 addr 不为零就调用 copyout 函数，将数据从内核空间复制到用户空间。</span></span><br><span class="line"><span class="comment">// p 是当前进程的指针，p-&gt;pagetable 存储了当前进程的页表。页表是一种数据结构，用于将虚拟地址映射到物理地址，以便访问内存中的数据。</span></span><br><span class="line"><span class="comment">// addr 是用户程序提供的目标地址，数据将被复制到这里</span></span><br><span class="line"><span class="comment">// (char *)&amp;proc_num 是要复制的源数据的地址。需要以char*的形式传递源数据的地址</span></span><br><span class="line"><span class="comment">// sizeof(pp-&gt;xstate) 是要复制的数据的大小，以字节为单位。</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> proc_num;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其余几项任务也差不多，照猫画虎。</p><blockquote><p>下面的部分是我很久之后参照着实验报告补的，可能并不全面，仅供参考</p></blockquote><h1 id="任务2：Free-Memory-Counting"><a href="#任务2：Free-Memory-Counting" class="headerlink" title="任务2：Free Memory Counting"></a>任务2：Free Memory Counting</h1><p>要统计空闲内存块的总字节数数，可以直接看在 kalloc.c 中的 kmem.freelist，遍历可以获得空闲“页数”，最后记得答案乘以 PGSIZE</p><p>为了操作定义在 kalloc.c 中的 kmem，可以在 kalloc.c 中添加一个函数</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span></span><br><span class="line"><span class="title function_">get_freemem</span><span class="params">(<span class="type">void</span>)</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="type">int</span> freemem_num = <span class="number">0</span>; <span class="comment">// 统计空闲内存数</span></span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">run</span> *<span class="title">mem</span> =</span> kmem.freelist;</span><br><span class="line">  <span class="keyword">while</span> (mem) &#123;</span><br><span class="line">    freemem_num++;</span><br><span class="line">    mem = mem-&gt;next;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> freemem_num * PGSIZE;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最后在系统调用时调用 <code>get_freemem()</code> 这个函数，和实验 1 一样我还是把实现放在了proc.c 中。记得把这些函数都在头文件 defs.h 中定义一下。</p><h3 id="sysproc-c"><a href="#sysproc-c" class="headerlink" title="sysproc.c :"></a>sysproc.c :</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">uint64 </span><br><span class="line"><span class="title function_">sys_freemem</span><span class="params">(<span class="type">void</span>)</span></span><br><span class="line">&#123;</span><br><span class="line">  uint64 p;</span><br><span class="line">  argaddr(<span class="number">0</span>, &amp;p);</span><br><span class="line">  <span class="keyword">return</span> freemem(p);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="proc-c"><a href="#proc-c" class="headerlink" title="proc.c :"></a>proc.c :</h3><p>把答案写回 num</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span></span><br><span class="line"><span class="title function_">freemem</span><span class="params">(uint64 addr)</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">proc</span> *<span class="title">p</span> =</span> myproc();</span><br><span class="line">  <span class="type">int</span> freemem_num = get_freemem();</span><br><span class="line">  <span class="keyword">if</span>(addr != <span class="number">0</span> &amp;&amp; copyout(p-&gt;pagetable, addr, (<span class="type">char</span> *)&amp;freemem_num, <span class="keyword">sizeof</span>(freemem_num)) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> freemem_num;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="任务3：System-call-tracing"><a href="#任务3：System-call-tracing" class="headerlink" title="任务3：System call tracing"></a>任务3：System call tracing</h1><p>这次 trace.c 已经给好了在 user 文件夹里</p><p>Trace 的用法不太一样，它传入一个参数 mask，每一个二进制位表示跟踪某一个system call。若某一 system call 处于被跟踪状态，则执行它时会输出相关信息</p><p>跟着 <a href="https://pdos.csail.mit.edu/6.S081/2022/labs/syscall.html">https://pdos.csail.mit.edu/6.S081/2022/labs/syscall.html</a> 中的提示，我们还是先同样在 sysproc.c 中添<br>加系统调用 sys_trace()，它负责接收参数 mask 到当前进程。这要求我们在 proc.h 中，对<br>每一个进程添加一个参数 mask。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Per-process state </span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">proc</span> &#123;</span></span><br><span class="line">  <span class="comment">//...</span></span><br><span class="line">  <span class="type">char</span> name[<span class="number">16</span>]; <span class="comment">// Process name (debugging)</span></span><br><span class="line">  <span class="type">int</span> mask; <span class="comment">// denote which syscall is being traced </span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>随后，我们需要修改 fork，让每一个子进程也继承到 mask。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> </span><br><span class="line"><span class="title function_">fork</span><span class="params">(<span class="type">void</span>)</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">// ... </span></span><br><span class="line">  <span class="comment">// copy the trace_mask state</span></span><br><span class="line">  np-&gt;mask = p-&gt;trace_mask; </span><br><span class="line">  <span class="comment">//...</span></span><br><span class="line">  <span class="keyword">return</span> pid;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最后，在 syscall.c 中修改 void syscall(void)，每次调用时检查所有的 system call，若对应 mask 位=1，则打印相关信息，格式和要求中相同。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> </span><br><span class="line"><span class="title function_">syscall</span><span class="params">(<span class="type">void</span>)</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="type">int</span> num; </span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">proc</span> *<span class="title">p</span> =</span> myproc(); </span><br><span class="line">  <span class="type">char</span>* syscall_name; </span><br><span class="line">  num = p-&gt;trapframe-&gt;a7; </span><br><span class="line">  <span class="keyword">if</span>(num &gt; <span class="number">0</span> &amp;&amp; num &lt; NELEM(syscalls) &amp;&amp; syscalls[num]) &#123;</span><br><span class="line">    <span class="comment">// Use num to lookup the system call function for num, call it, </span></span><br><span class="line"><span class="comment">// and store its return value in p-&gt;trapframe-&gt;a0 </span></span><br><span class="line">p-&gt;trapframe-&gt;a0 = syscalls[num](); </span><br><span class="line"><span class="comment">// 遍历所有 syscall </span></span><br><span class="line"><span class="keyword">if</span> ((p-&gt;mask &amp; (<span class="number">1</span> &lt;&lt; num)) != <span class="number">0</span>) &#123; <span class="comment">// 若处于跟踪状态； </span></span><br><span class="line">  <span class="comment">// 打印相关信息</span></span><br><span class="line">      syscall_name = syscall_names[num]; </span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;%d: syscall %s -&gt; %d\n&quot;</span>, p-&gt;pid, syscall_name, p-&gt;trapframe-&gt;a0); </span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line">  <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%d %s: unknown sys call %d\n&quot;</span>, p-&gt;pid, p-&gt;name, num); </span><br><span class="line">p-&gt;trapframe-&gt;a0 = <span class="number">-1</span>; </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;div class=&quot;note success simple&quot;&gt;&lt;p&gt;薪火相传&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;因为中途才开始写博客，一些配置上的问题可能没讲清楚。目前只是自己做一个记录吧。&lt;/p&gt;
&lt;p&gt;如果能帮到你理解这个lab就好啦&lt;/p&gt;
&lt;p&gt;环境：WSL + xv6&lt;/</summary>
      
    
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/categories/Study/"/>
    
    <category term="Operating System" scheme="https://anti-entrophic.github.io/categories/Study/Operating-System/"/>
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/tags/Study/"/>
    
    <category term="Notes" scheme="https://anti-entrophic.github.io/tags/Notes/"/>
    
    <category term="Operating System" scheme="https://anti-entrophic.github.io/tags/Operating-System/"/>
    
  </entry>
  
  <entry>
    <title>BP神经网络与反向传播算法</title>
    <link href="https://anti-entrophic.github.io/posts/10005.html"/>
    <id>https://anti-entrophic.github.io/posts/10005.html</id>
    <published>2023-10-14T08:31:31.000Z</published>
    <updated>2023-10-17T03:22:36.506Z</updated>
    
    <content type="html"><![CDATA[<p>本文还不是很完善</p><p>简单推导一下反向传播算法的公式</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picst.sunbangyan.cn/2023/10/14/r3efwk.png" alt="多层前馈神经网络示例。" style="width:400px;"/></div><span class="image-caption">多层前馈神经网络示例。</span></div><p>设激活函数为sigmoid函数，误差函数采用均方误差，正向传播得到的结果为 $\hat{\boldsymbol{y}}$，真实值为y。</p><h1 id="最后一层参数"><a href="#最后一层参数" class="headerlink" title="最后一层参数"></a>最后一层参数</h1><p>设隐层 $b_i$ 到输出层 $y_j$ 的参数为 $w_{ij} $</p><p>以更新 $w_{11}$ 为例，现在已经得到了均方误差 $E$ :</p><script type="math/tex; mode=display">E = \frac{1}{2} \sum_{i=1}^n (\hat{y_i} - y_i)^2</script><p>用梯度下降最小化均方误差更新隐层到输出层参数，由链式法则</p><script type="math/tex; mode=display">\frac{\partial E}{\partial w_{11}} = \frac{\partial E}{\partial \hat{y_1}} \cdot \frac{\partial \hat{y_1}}{\partial \beta_{1}} \cdot \frac{\partial \beta_1}{\partial w_{11}}</script><script type="math/tex; mode=display">\begin{equation}\left\{    \begin{array}{ll}        \frac{\partial E}{\partial \hat{y_1}} = \hat{y_1}-y_1 & \\        \frac{\partial \hat{y_1}}{\partial \beta_{1}} = \hat{y_1} (1- \hat{y_1}) & 对sigmoid求导 \\    \frac{\partial \beta_1}{\partial w_{11}} = b_1    \end{array}\right. \notag\end{equation}</script><p>所以 $\Delta w_{11} = b_1\hat{y_1}(1-\hat{y_1})(\hat{y_1}-y_1)$ </p><p>令 $\boldsymbol{g_i} = \hat{y_1}(1-\hat{y_1})(\hat{y_1}-y_1)$</p><h1 id="隐层之间的参数"><a href="#隐层之间的参数" class="headerlink" title="隐层之间的参数"></a>隐层之间的参数</h1><p>考虑</p><script type="math/tex; mode=display">\frac{\partial E}{\partial v_{11}} = \frac{\partial E}{\partial b_1} \cdot \frac{\partial b_1}{\partial \alpha_{1}} \cdot \frac{\partial \alpha_1}{\partial v_{11}}</script><p>其中</p><script type="math/tex; mode=display">\begin{equation}\left\{    \begin{array}{ll}        \frac{\partial E}{\partial b_1} & =  \frac{\partial E}{\partial \beta_1} \cdot \frac{\partial \beta_1}{\partial b_1} + \frac{\partial E}{\partial \beta_2} \cdot \frac{\partial \beta_2}{\partial b_1} + \frac{\partial E}{\partial \beta_3} \cdot \frac{\partial \beta_3}{\partial b_1}\\        \frac{\partial b_1}{\partial \alpha_{1}} & = b_1 (1- b_1) & sigmoid \\    \frac{\partial \alpha_1}{\partial v_{11}} & = x_1    \end{array}\right. \notag\end{equation}</script><p>代入可得 $\Delta v_{11} = x_1b_1(1-b_1)\sum_{i=1}^3 g_iw_{1i}$</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>最后隐层到输出层和隐层之间梯度更新公式是不一样的，都是正常使用链式法则求导出来的。</p><p>隐层之间的公式是相同的，如果是多层网络的话也是同样的推导过程，把变量换成对应的前一层的内容即可。</p><p>有时间的话，希望能推一个一般性的递推公式出来。网上都找不到完整证明。。</p><p>希望能有一个向量化的公式啊</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本文还不是很完善&lt;/p&gt;
&lt;p&gt;简单推导一下反向传播算法的公式&lt;/p&gt;
&lt;div class=&quot;img-wrap&quot;&gt;&lt;div class=&quot;img-bg&quot;&gt;&lt;img class=&quot;img&quot; src=&quot;https://picst.sunbangyan.cn/2023/10/1</summary>
      
    
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/categories/Study/"/>
    
    <category term="Neural Network" scheme="https://anti-entrophic.github.io/categories/Study/Neural-Network/"/>
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/tags/Study/"/>
    
    <category term="Notes" scheme="https://anti-entrophic.github.io/tags/Notes/"/>
    
    <category term="Neural Network" scheme="https://anti-entrophic.github.io/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>自定义页面</title>
    <link href="https://anti-entrophic.github.io/posts/10004.html"/>
    <id>https://anti-entrophic.github.io/posts/10004.html</id>
    <published>2023-10-14T05:17:52.000Z</published>
    <updated>2023-10-14T05:37:46.175Z</updated>
    
    <content type="html"><![CDATA[<p>Hexo &amp; butterfly 会自动按照其设置指定的方式对markdown文件进行渲染，生成对应的博客页面。</p><p>不过可能除了写博客，我们还希望能在博客上部署一些小页面小功能，butterfly也是支持的。</p><p>我们可以在 <code>/source</code> 文件夹下新建一个html文件夹，将我们写好的页面（html、css、javascript）一起放进去。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">source</span><br><span class="line">    |-- html</span><br><span class="line">          |-- index.html  # 主要的小工具总览页面</span><br><span class="line">          |-- Tool1  # 各种不同的小工具</span><br><span class="line">          |-- Tool2</span><br><span class="line">          |-- ...</span><br></pre></td></tr></table></figure><p>在 <code>_config.yml</code> ，我们需要指定hexo让其跳过对html文件夹下内容的默认渲染：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">skip_render:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&quot;html/**&quot;</span>  </span><br></pre></td></tr></table></figure><p>最后，我们可以在menu栏添加一个Tool图标，会首先跳转到一个总览页面，专门用于存放自己的小工具：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">menu:</span></span><br><span class="line">  <span class="attr">Tools:</span> <span class="string">/html/index.html</span> <span class="string">||</span> <span class="string">fas</span> <span class="string">fa-archive</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">介绍如何在博客上部署自定义页面</summary>
    
    
    
    <category term="Hexo &amp; Butterfly tutorial" scheme="https://anti-entrophic.github.io/categories/Hexo-Butterfly-tutorial/"/>
    
    
    <category term="Hexo &amp; Butterfly tutorial" scheme="https://anti-entrophic.github.io/tags/Hexo-Butterfly-tutorial/"/>
    
  </entry>
  
  <entry>
    <title>博客书写指南</title>
    <link href="https://anti-entrophic.github.io/posts/10003.html"/>
    <id>https://anti-entrophic.github.io/posts/10003.html</id>
    <published>2023-10-13T14:59:34.000Z</published>
    <updated>2023-10-17T04:35:15.334Z</updated>
    
    <content type="html"><![CDATA[<p>每篇博客都是一个markdown文件，由Butterfly渲染生成页面，下面介绍一些文章的设置以及写法：</p><p class='p left logo large'>文章属性</p><p>每篇文章的开头都有一个<code>Front-matter</code>，用于提供这篇文章的相关信息，详情请看 <a href="https://butterfly.js.org/posts/dc584b87/">https://butterfly.js.org/posts/dc584b87/</a></p><p>以本篇文章的设置为例，讲一下各个属性的意义：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 博客书写指南</span><br><span class="line">cover: &#x27;https://picst.sunbangyan.cn/2023/10/13/oynsmu.jpg&#x27;</span><br><span class="line">abbrlink: 10003</span><br><span class="line">date: 2023-10-13 22:59:32</span><br><span class="line">tags: [Hexo &amp; Butterfly tutorial]</span><br><span class="line">categories: </span><br><span class="line"><span class="bullet">  -</span> [Hexo &amp; Butterfly tutorial]</span><br><span class="line">updated:</span><br><span class="line">type:</span><br><span class="line">comments:</span><br><span class="line">description: 介绍博客文章的书写方法</span><br><span class="line">keywords:</span><br><span class="line">top<span class="emphasis">_img:</span></span><br><span class="line"><span class="emphasis">mathjax:</span></span><br><span class="line"><span class="emphasis">katex:</span></span><br><span class="line"><span class="emphasis">aside:</span></span><br><span class="line"><span class="emphasis">aplayer:</span></span><br><span class="line"><span class="emphasis">highlight_</span>shrink:</span><br><span class="line"><span class="section">random:</span></span><br><span class="line"><span class="section">---</span></span><br></pre></td></tr></table></figure><ul><li><p>title</p><p>博客的标题</p></li><li><p>cover </p><p>博客的封面（显示在首页）</p></li><li><p>abbrlink</p><p>由插件 <code>hexo-abbrlink</code> 生成，替换hexo原生的页面url生成方案。该值可自定义，唯一指定一篇文章（不可重复），同时也是该页面的url地址。</p></li><li><p>date</p><p>文章的发表时间</p></li><li><p>tags</p><p>该篇文章的标签，语法为 tags:[tag1, tag2, …]</p></li><li><p>categories</p><p>该篇文章的分类，推荐语法为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">categories: </span><br><span class="line">  - [A, B]</span><br></pre></td></tr></table></figure><p>表示该文章属于分类A下的子类B</p></li><li><p>description</p><p>在首页看到的文章内容简介。如果设置了 <code>description</code> ，则会显示这个，否则会显示文章内容节选</p><p>这一项可以在 <code>_config.butterfly.yml</code> 中的 <code>index_post_content</code> 一栏修改（详见：<a href="https://anti-entrophic.github.io/posts/10002.html）">https://anti-entrophic.github.io/posts/10002.html）</a></p></li><li><p>待补充</p></li></ul><p class='p left logo large'>自定义页面字体</p><h1 id="创建css文件"><a href="#创建css文件" class="headerlink" title="创建css文件"></a>创建css文件</h1><p>以我目前使用的字体为例，首先需要在 <code>/&#123;root&#125;/source/css</code> 目录下新建一个文件，暂且命名为 <code>custom.css</code> ，内容如下：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*感谢安知鱼大佬的教程！*/</span></span><br><span class="line"><span class="keyword">@font-face</span> &#123;</span><br><span class="line">    <span class="attribute">font-family</span>: ZhuZiAYuanJWD;</span><br><span class="line">    <span class="attribute">src</span>: <span class="built_in">url</span>(<span class="string">https://npm.elemecdn.com/anzhiyu-blog@1.1.6/fonts/ZhuZiAWan.woff2</span>);</span><br><span class="line">    <span class="attribute">font-display</span>: swap;</span><br><span class="line">    <span class="attribute">font-weight</span>: lighter;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="selector-tag">body</span> &#123;</span><br><span class="line">    <span class="attribute">font-family</span>: <span class="string">&quot;ZhuZiAYuanJWD&quot;</span>;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>当然，你也可以不指定全局使用这一个字体，可以换，比如说仅指定导航栏当然也是可以的，可以自己魔改：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">div</span><span class="selector-id">#menus</span> &#123;</span><br><span class="line">  <span class="attribute">font-family</span>: <span class="string">&quot;ZhuZiAYuanJWD&quot;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="引入css文件"><a href="#引入css文件" class="headerlink" title="引入css文件"></a>引入css文件</h1><p>在 <code>_config.butterfly.yml</code> 中找到 <code>inject</code> 这一栏，就像html链接css一样引入：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">inject:</span></span><br><span class="line">  <span class="attr">head:</span></span><br><span class="line">    <span class="comment"># 自定义CSS</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&lt;link</span> <span class="string">rel=&quot;stylesheet&quot;</span> <span class="string">href=&quot;/css/custom.css&quot;</span> <span class="string">media=&quot;defer&quot;</span> <span class="string">onload=&quot;this.media=&#x27;all&#x27;&quot;&gt;</span></span><br><span class="line">    <span class="comment"># 暂时不清楚media和onload有什么用</span></span><br></pre></td></tr></table></figure><p>之后，在 <code>_config.butterfly.yml</code> 中指定渲染时使用我们新引入的字体：（这步不清楚需不需要）</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">font:</span></span><br><span class="line">  <span class="attr">global-font-size:</span></span><br><span class="line">  <span class="attr">code-font-size:</span></span><br><span class="line">  <span class="attr">font-family:</span> <span class="string">ZhuZiAYuanJWD</span></span><br><span class="line">  <span class="attr">code-font-family:</span> <span class="string">ZhuZiAYuanJWD</span></span><br></pre></td></tr></table></figure><p>其中，<code>font-family</code> 是全局的字体， <code>code-font-family</code> 是代码块中的字体</p>]]></content>
    
    
    <summary type="html">介绍博客文章的书写方法</summary>
    
    
    
    <category term="Hexo &amp; Butterfly tutorial" scheme="https://anti-entrophic.github.io/categories/Hexo-Butterfly-tutorial/"/>
    
    
    <category term="Hexo &amp; Butterfly tutorial" scheme="https://anti-entrophic.github.io/tags/Hexo-Butterfly-tutorial/"/>
    
  </entry>
  
  <entry>
    <title>hexo&amp;butterfly配置教程</title>
    <link href="https://anti-entrophic.github.io/posts/10002.html"/>
    <id>https://anti-entrophic.github.io/posts/10002.html</id>
    <published>2023-10-13T14:59:33.000Z</published>
    <updated>2023-10-14T05:55:38.309Z</updated>
    
    <content type="html"><![CDATA[<div class="note success simple"><p>介绍一些博客的 Hexo &amp; butterfly 配置修改方法</p></div><p class='p left logo large'>主页文章简介</p><p>在主页中看到的文章内容的简介，<code>butterfly</code> 共提供了四种选择：</p><ol><li>description： 只显示description</li><li>both： 优先选择description；如果没有配置description，则显示文章节选内容</li><li>auto_excerpt： 只显示文章节选内容</li><li>false： 不显示文章内容</li></ol><p>在 <code>_config.butterfly.yml</code> 修改条目如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">index_post_content:</span></span><br><span class="line">  <span class="attr">method:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">length:</span> <span class="number">500</span>  <span class="comment"># if you set method to 2 or 3, the length need to config</span></span><br></pre></td></tr></table></figure><p class='p left logo large'>跳过渲染</p><p>hexo会自动把source下的文件识别渲染为博客页面，但有时我们并不希望自动渲染，而是想要其维持我们设计的界面。</p><p>在 <code>_config.yml</code> 修改条目如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">skip_render:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&quot;html/**&quot;</span>  <span class="comment"># &quot;**&quot;：通配符，html文件夹下的所有文件；&quot;*&quot;，html文件夹下一层的所有文件</span></span><br></pre></td></tr></table></figure><p class='p left logo large'>侧边栏</p><p>在 <code>_config.butterfly.yml</code> 相关条目：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">aside:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">hide:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">button:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">mobile:</span> <span class="literal">true</span> <span class="comment"># display on mobile</span></span><br><span class="line">  <span class="attr">position:</span> <span class="string">right</span> <span class="comment"># left or right</span></span><br><span class="line">  <span class="attr">display:</span></span><br><span class="line">    <span class="attr">archive:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">tag:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">category:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">card_author:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">description:</span></span><br><span class="line">    <span class="attr">button:</span></span><br><span class="line">      <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">icon:</span> <span class="string">fab</span> <span class="string">fa-github</span></span><br><span class="line">      <span class="attr">text:</span> <span class="string">Follow</span> <span class="string">Me</span></span><br><span class="line">      <span class="attr">link:</span> <span class="string">https://github.com/Anti-Entrophic</span></span><br><span class="line">  <span class="attr">card_announcement:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">content:</span> <span class="string">This</span> <span class="string">is</span> <span class="string">my</span> <span class="string">Blog</span></span><br><span class="line">  <span class="attr">card_recent_post:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">limit:</span> <span class="number">5</span> <span class="comment"># if set 0 will show all</span></span><br><span class="line">    <span class="attr">sort:</span> <span class="string">date</span> <span class="comment"># date or updated</span></span><br><span class="line">    <span class="attr">sort_order:</span> <span class="comment"># Don&#x27;t modify the setting unless you know how it works</span></span><br><span class="line">  <span class="attr">card_categories:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">limit:</span> <span class="number">8</span> <span class="comment"># if set 0 will show all</span></span><br><span class="line">    <span class="attr">expand:</span> <span class="string">none</span> <span class="comment"># none/true/false</span></span><br><span class="line">    <span class="attr">sort_order:</span> <span class="comment"># Don&#x27;t modify the setting unless you know how it works</span></span><br><span class="line">  <span class="attr">card_tags:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">limit:</span> <span class="number">40</span> <span class="comment"># if set 0 will show all</span></span><br><span class="line">    <span class="attr">color:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">orderby:</span> <span class="string">random</span> <span class="comment"># Order of tags, random/name/length</span></span><br><span class="line">    <span class="attr">order:</span> <span class="number">1</span> <span class="comment"># Sort of order. 1, asc for ascending; -1, desc for descending</span></span><br><span class="line">    <span class="attr">sort_order:</span> <span class="comment"># Don&#x27;t modify the setting unless you know how it works</span></span><br><span class="line">  <span class="attr">card_archives:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">monthly</span> <span class="comment"># yearly or monthly</span></span><br><span class="line">    <span class="attr">format:</span> <span class="string">MMMM</span> <span class="string">YYYY</span> <span class="comment"># eg: YYYY年MM月</span></span><br><span class="line">    <span class="attr">order:</span> <span class="number">-1</span> <span class="comment"># Sort of order. 1, asc for ascending; -1, desc for descending</span></span><br><span class="line">    <span class="attr">limit:</span> <span class="number">8</span> <span class="comment"># if set 0 will show all</span></span><br><span class="line">    <span class="attr">sort_order:</span> <span class="comment"># Don&#x27;t modify the setting unless you know how it works</span></span><br><span class="line">  <span class="attr">card_webinfo:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">post_count:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">last_push_date:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">sort_order:</span> <span class="comment"># Don&#x27;t modify the setting unless you know how it works</span></span><br><span class="line">  <span class="attr">card_post_series:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">orderBy:</span> <span class="string">&#x27;date&#x27;</span> <span class="comment"># Order by title or date</span></span><br><span class="line">    <span class="attr">order:</span> <span class="number">-1</span> <span class="comment"># Sort of order. 1, asc for ascending; -1, desc for descending</span></span><br><span class="line"></span><br><span class="line"><span class="attr">social:</span></span><br><span class="line">  <span class="comment"># fab fa-github: https://github.com/Anti-Entrophic || Github || &#x27;#24292e&#x27;</span></span><br></pre></td></tr></table></figure><p><code>aside</code> 的注释很详细，对照着看就知道功能了。<code>social</code> 指的是在 Follow me 下面出现的图标。</p>]]></content>
    
    
    <summary type="html">hexo&amp;butterfly 配置个性化魔改教程</summary>
    
    
    
    <category term="Hexo &amp; Butterfly tutorial" scheme="https://anti-entrophic.github.io/categories/Hexo-Butterfly-tutorial/"/>
    
    
    <category term="Hexo &amp; Butterfly tutorial" scheme="https://anti-entrophic.github.io/tags/Hexo-Butterfly-tutorial/"/>
    
  </entry>
  
  <entry>
    <title>博客时间线</title>
    <link href="https://anti-entrophic.github.io/posts/d87f7e0c.html"/>
    <id>https://anti-entrophic.github.io/posts/d87f7e0c.html</id>
    <published>2023-10-12T12:05:47.000Z</published>
    <updated>2023-10-14T05:15:50.815Z</updated>
    
    <content type="html"><![CDATA[<div class="timeline undefined"><div class='timeline-item headline'><div class='timeline-item-title'><div class='item-circle'><p>2023</p></div></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>10-13</p></div></div><div class='timeline-item-content'><p>完成博客基础建设与部署</p></div></div></div>]]></content>
    
    
    <summary type="html">建站时间线~</summary>
    
    
    
    
  </entry>
  
</feed>
