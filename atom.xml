<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>不会魔法的小圆</title>
  
  <subtitle>世界如此可爱</subtitle>
  <link href="https://anti-entrophic.github.io/atom.xml" rel="self"/>
  
  <link href="https://anti-entrophic.github.io/"/>
  <updated>2023-10-19T06:36:08.236Z</updated>
  <id>https://anti-entrophic.github.io/</id>
  
  <author>
    <name>不会魔法的小圆</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>PyTorch学习笔记（三） LSTM</title>
    <link href="https://anti-entrophic.github.io/posts/10012.html"/>
    <id>https://anti-entrophic.github.io/posts/10012.html</id>
    <published>2023-10-18T15:24:21.000Z</published>
    <updated>2023-10-19T06:36:08.236Z</updated>
    
    <content type="html"><![CDATA[<p>参考：<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a> ，写得非常详细，有精致的图例和清楚的公式，建议先阅读该篇文章</p><p class='p left logo large'>基本概念</p><p>LSTM，长短期记忆神经网络，它克服了RNN难以记录长期信息的特点。</p><p>举个例子，“天上红色的是__”，RNN可以一定程度上利用“天上”、“红色”等信息来预测当前应填的单词，可能是“太阳”。</p><p>但是，如果有效信息相隔非常远，例如：“我是一个中国人，……，我会说___，英语和日语”，这些久远的信息很难留存在RNN的hidden state中。</p><p>LSTM在结构上做出了改进，如下图所示。它包含两种hidden state，一个是上方的$C_t$，它代表着长期记忆，在训练过程中改动较少，一个是下方的$h_t$，代表着短期记忆（？也许，其实没有这么明确的物理意义，只是我期望可以这么认为）。它们通过一些方式互相影响并更新，下面一一介绍。</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picss.sunbangyan.cn/2023/10/18/a1e155b0ed13694af1700b3c732024b9.png" style="width:800px;"/></div></div><p class='p left logo large'>网络结构</p><h1 id="遗忘门"><a href="#遗忘门" class="headerlink" title="遗忘门"></a>遗忘门</h1><p>LSTM的第一步是决定要让长期记忆$C_{t-1}$去遗忘什么。它拼接当前输入$x_t$和上一时刻的状态$h_{t-1}$，并送入sigmoid层，得到一个介于0与1之间的输出向量。随后，和$C_{t-1}$做Hadamard积。如果sigmoid层对应输出越接近0，则对应信息遗忘得越多，反之则继续记忆。</p><p>形象化的理解可能就是，用当前状态去判断哪些记忆不再适用了，比如说之前还一直在谈论A，现在突然转到谈论B了，一些有关A的信息就不再需要了。</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picdm.sunbangyan.cn/2023/10/19/9e11a5f3e9ea43fa9bb0538dbcc869e5.png" style="width:800px;"/></div></div><h1 id="输入门"><a href="#输入门" class="headerlink" title="输入门"></a>输入门</h1><p>第二步是决定要让$C_{t-1}$记住什么。当前输入首先会经过一个sigmoid层得到$i_t$，这决定了接下来哪些信息是有必要被记住的。实际上的更新向量$\tilde{C_t}$是通过tanh激活函数得到的，至于为什么选tanh，我找到了一些说法：（来自：<a href="https://stackoverflow.com/questions/40761185/what-is-the-intuition-of-using-tanh-in-lstm）">https://stackoverflow.com/questions/40761185/what-is-the-intuition-of-using-tanh-in-lstm）</a></p><ul><li><p>为了防止梯度消失问题，我们需要一个二次导数在大范围内不为0的函数，而tanh函数可以满足这一点</p></li><li><p>为了便于凸优化，我们需要一个单调函数</p></li><li><p>tanh函数一般收敛的更快</p></li><li><p>tanh函数的求导占用系统的资源更少</p></li></ul><p>随后，$i_t$与$\tilde{C_t}$相乘，得到正式的update内容。</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picdl.sunbangyan.cn/2023/10/19/9cb764099821843a91cd0197a98e1824.png" style="width:800px;"/></div></div><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picdm.sunbangyan.cn/2023/10/19/59ab5d4957ad6e60f3984257d96245f5.png" style="width:800px;"/></div></div><h1 id="输出门"><a href="#输出门" class="headerlink" title="输出门"></a>输出门</h1><p>我们要决定输出什么内容。总之就是输入经过一个sigmoid层，加上记忆$C_{t}$的影响（经过tanh），</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picss.sunbangyan.cn/2023/10/19/a3243ac1c491e8b184bc8912621828af.png" style="width:800px;"/></div></div><p class='p left logo large'>代码</p>]]></content>
    
    
    <summary type="html">学习LSTM！</summary>
    
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/categories/Study/"/>
    
    <category term="Neural Network" scheme="https://anti-entrophic.github.io/categories/Study/Neural-Network/"/>
    
    
    <category term="Neural Network" scheme="https://anti-entrophic.github.io/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch学习笔记（二） CNN</title>
    <link href="https://anti-entrophic.github.io/posts/10011.html"/>
    <id>https://anti-entrophic.github.io/posts/10011.html</id>
    <published>2023-10-18T13:12:07.000Z</published>
    <updated>2023-10-18T13:15:15.089Z</updated>
    
    <content type="html"><![CDATA[<p>这次来用PyTorch实现一下CNN卷积神经网络, 数据我们采用 MNIST 这个手写数字识别的数据库, 完成一个多分类任务（判断是哪个数字）</p><p>不清楚PyTorch基本用法请移步 <a href="https://anti-entrophic.github.io/posts/10010.html">https://anti-entrophic.github.io/posts/10010.html</a></p><p class='p left logo large'>概述</p><p>最简单的CNN的结构是 “-&gt;卷积层-&gt;激活函数-&gt;池化层-&gt;线性层”，这里先简单介绍一下，后面会配合代码详细描述。</p><p>卷积层目标就是训练若干个卷积核，期望这些卷积核能够学到图像的某些特征。图像的各个通道会通过各个卷积核，得到卷积操作后的结果，然后经过ReLU激活函数。</p><p>池化层就如下图，目的是为了给图像降维，减少参数，并且期望能够捕捉一些关键特征，忽略不重要的细节</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://pic1.zhimg.com/80/v2-6091b01b4e85b1c23f3b7cf9f1496c90_1440w.webp" style="width:400px;"/></div></div><p>最后压缩维度后经过一个线性层，得到最终结果的概率分布，然后利用交叉熵损失函数来进行优化。</p><p>更详细的：<a href="https://zhuanlan.zhihu.com/p/630695553">https://zhuanlan.zhihu.com/p/630695553</a></p><p class='p left logo large'>数据</p><p>我们可以很方便的通过 <code>torchvision</code> 这个包下载到 MNIST 这个数据库</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取数据集</span></span><br><span class="line">train_data = torchvision.datasets.MNIST(</span><br><span class="line">    root = <span class="string">&#x27;./MNIST/&#x27;</span>,</span><br><span class="line">    train = <span class="literal">True</span>,</span><br><span class="line">    transform = torchvision.transforms.ToTensor(),</span><br><span class="line">    download = <span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_data = torchvision.datasets.MNIST(root=<span class="string">&#x27;./MNIST/&#x27;</span>, train=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>这样下载下来一个是训练集，一个是验证集。并且下载下来就是 <code>torch.utils.data.Dataset</code> 类，可以很好地适配 PyTorch 中常用的 Dataloader</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_loader = Data.DataLoader(</span><br><span class="line">    dataset = train_data,</span><br><span class="line">    batch_size = <span class="number">50</span>,</span><br><span class="line">    shuffle = <span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><code>Dataloader</code> 可以很方便地完成将数据组成batch，随机取样等操作。</p><p>可以简单看一下 MNIST 这个数据集，每张图片的大小都是 28*28，训练样本有60000个，测试样本有10000个</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(train_data.data.shape)</span><br><span class="line"><span class="built_in">print</span>(test_data.data.shape)</span><br><span class="line"><span class="comment"># -----output-----</span></span><br><span class="line"><span class="comment"># torch.Size([60000, 28, 28])</span></span><br><span class="line"><span class="comment"># torch.Size([10000, 28, 28])</span></span><br></pre></td></tr></table></figure><p class='p left logo large'>网络结构</p><p>卷积层的输入是三维的，第一维是图像的通道数。</p><p>这个 <code>nn.Conv2d</code> ，<code>in_channels</code>就是输入图像的通道数，灰度图像就是1，RGB图像就是3。<code>output_channels</code>就是卷积核数，也是输出图像的通道数。</p><p>如果<code>in_channels</code>是3的话，那对每个卷积核，都是对3个通道各自卷积，然后加起来，会得到16个卷积后的通道，最后合在一起。</p><p><code>kernel_size</code> 就是卷积核的大小，<code>stride</code> 是卷积核移动的步长，<code>padding</code> 是周围补0，控制卷积后图像的大小。</p><p><code>ReLU()</code> 就激活一下，不过我有点疑惑的是，卷积操作完之后，会不会某些点的intensity超过255？因为这在图像中应该是不可能的情况，但是好像直接就没有处理；小于0的话经过ReLU()可以调回来。</p><p><code>nn.MaxPool2d</code> 就是一个池化层，如文章开头图片所示，取2x2格中的最大值。</p><p>最终第一层的维度变化为 (batch_size, 1, 28, 28) -&gt; (batch_size, 16, 14, 14)</p><p>第二层的维度变化为 (batch_size, 16, 14, 14) -&gt; (batch_size, 32, 7, 7)</p><p>线性层的维度变化为 -&gt; (batch, 32<em>7</em>7) -&gt; (batch, 10)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.convl = nn.Sequential(</span><br><span class="line">            nn.Conv2d(</span><br><span class="line">                in_channels=<span class="number">1</span>,</span><br><span class="line">                out_channels=<span class="number">16</span>,</span><br><span class="line">                kernel_size=<span class="number">5</span>,</span><br><span class="line">                stride=<span class="number">1</span>,</span><br><span class="line">                padding=<span class="number">2</span></span><br><span class="line">            ),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(</span><br><span class="line">                kernel_size=<span class="number">2</span></span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.conv2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(</span><br><span class="line">                in_channels=<span class="number">16</span>,</span><br><span class="line">                out_channels=<span class="number">32</span>,</span><br><span class="line">                kernel_size=<span class="number">5</span>,</span><br><span class="line">                stride=<span class="number">1</span>,</span><br><span class="line">                padding=<span class="number">2</span></span><br><span class="line">            ),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(</span><br><span class="line">                kernel_size=<span class="number">2</span></span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.out = nn.Linear(<span class="number">32</span>*<span class="number">7</span>*<span class="number">7</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.convl(x) <span class="comment"># (batch_size, 1, 28, 28) -&gt; (batch_size, 16, 14, 14)</span></span><br><span class="line">        x = self.conv2(x) <span class="comment"># (batch_size, 16, 14, 14) -&gt; (batch_size, 32, 7, 7)</span></span><br><span class="line">        x = x.view(x.shape[<span class="number">0</span>], -<span class="number">1</span>) <span class="comment"># (batch, 32*7*7)</span></span><br><span class="line">        x = self.out(x) <span class="comment"># (batch, 10)</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p class='p left logo large'>网络训练</p><p>对于分类的概率分布，损失函数用交叉熵损失，原因我在其它文章中也提到过，详细链接：<a href="https://zhuanlan.zhihu.com/p/115277553">https://zhuanlan.zhihu.com/p/115277553</a></p><p>优化器没有用SGD而是Adam，不知道具体会有什么差异</p><p>只训练一个epoch</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">model = CNN()</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> step,(batch_x,batch_y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        pred_y = model(batch_x)</span><br><span class="line">        loss = criterion(pred_y, batch_y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (step + <span class="number">1</span>) % <span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Step:&#x27;</span>, <span class="string">&#x27;%04d&#x27;</span> % (step + <span class="number">1</span>), <span class="string">&#x27;cost =&#x27;</span>, <span class="string">&#x27;&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(loss))</span><br><span class="line">        </span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure><p class='p left logo large'>结果</p><p>看一下结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这步unsqueeze让test_data从(10000,28,28)-&gt;(10000,1,28,28)，适配CNN的输入</span></span><br><span class="line">test_x = torch.unsqueeze(test_data.data,dim=<span class="number">1</span>).<span class="built_in">float</span>()[:<span class="number">2000</span>] </span><br><span class="line"><span class="comment"># 取出前2000个验证样本的标签</span></span><br><span class="line">test_y = test_data.targets[:<span class="number">2000</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 简单试一下前20个的输出结果</span></span><br><span class="line">test_output = model(test_x[:<span class="number">20</span>])</span><br><span class="line"><span class="comment"># 这里，test_output的维度是(20,10)，torch.max会返回两个值，一个是value，一个是index，index就代表着分类为哪个数字</span></span><br><span class="line"><span class="comment"># torch.max(_, 1) 表示沿着test_output的第1维也就是10这一维去找最大值，找的就是每一个概率分布中的最大值</span></span><br><span class="line">pred_y = torch.<span class="built_in">max</span>(test_output, <span class="number">1</span>)[<span class="number">1</span>].numpy()</span><br><span class="line"><span class="built_in">print</span>(pred_y, <span class="string">&#x27;prediction number&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(test_y[:<span class="number">20</span>].numpy(),<span class="string">&#x27;real number&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----output-----</span></span><br><span class="line"><span class="comment"># [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4] prediction number</span></span><br><span class="line"><span class="comment"># [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4] real number</span></span><br></pre></td></tr></table></figure><p class='p left logo large'>总结</p><p>只是最简单的CNN吧，不过结果确实挺好，网络就真的学到特征了。也没去试过换一下优化器啊激活函数会有什么效果，只是学一下基础知识顺便学习PyTorch用法吧</p>]]></content>
    
    
    <summary type="html">使用 torchvision.datasets 中的 MNIST 数据集进行训练</summary>
    
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/categories/Study/"/>
    
    <category term="Neural Network" scheme="https://anti-entrophic.github.io/categories/Study/Neural-Network/"/>
    
    
    <category term="Neural Network" scheme="https://anti-entrophic.github.io/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch学习笔记（一） 基础</title>
    <link href="https://anti-entrophic.github.io/posts/10010.html"/>
    <id>https://anti-entrophic.github.io/posts/10010.html</id>
    <published>2023-10-17T10:48:37.000Z</published>
    <updated>2023-10-18T12:19:17.302Z</updated>
    
    <content type="html"><![CDATA[<p>先尝试写一个简单的线性计算，用神经网络拟合学习。</p><p class='p left logo large'>数据生成</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成数据</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 给定权重矩阵</span></span><br><span class="line">W = torch.tensor([[<span class="number">0.3</span>, <span class="number">0.4</span>],[<span class="number">0.5</span>,<span class="number">0.6</span>],[<span class="number">0.6</span>,<span class="number">0.2</span>]])</span><br><span class="line"></span><br><span class="line">num_samples = <span class="number">100</span></span><br><span class="line">input_dim = <span class="number">3</span></span><br><span class="line">x = torch.rand((num_samples, input_dim))</span><br><span class="line"></span><br><span class="line">y = torch.matmul(W.t(), x.t()).t()</span><br></pre></td></tr></table></figure><p>一会儿我们用这些数据取拟合权重矩阵 $W$</p><p class='p left logo large'>定义网络</p><p>pytorch中的自定义神经网络类需要继承 <code>nn.Module</code> ，并且调用父类的构造函数 <code>__init__()</code> 来完成一些参数和方法的初始化。</p><p>最简单的网络我们只需定义网络中的层结构，如这里我加了一层线性层。以及前向传播的 <code>forward</code> 函数即可。</p><p>如果需要自定义损失函数等，会在之后的文章中介绍。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Simple_nn</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Simple_nn, self).__init__()</span><br><span class="line">        self.W = nn.Linear(input_dim, output_dim, bias=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># X : [batch_size, input_dim]</span></span><br><span class="line">        <span class="comment"># 在forward中的X还是带有batch_size这一维度的，不过在经过链接层的时候，pytorch会自动处理批次，不用显式考虑。</span></span><br><span class="line">        output = self.W(X)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p class='p left logo large'>进行训练</p><p>训练需要数据，我们把数据以batch的形式送进网络进行训练。目前我们不涉及dataset，dataloader的使用。</p><p>先写一个随机取batch的函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">4</span> <span class="comment"># mini-batch size</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">random_batch</span>():</span><br><span class="line">    random_x = torch.zeros((batch_size, input_dim))</span><br><span class="line">    random_y = torch.zeros((batch_size, output_dim))</span><br><span class="line">    <span class="comment"># 定义一个随机抽取batch</span></span><br><span class="line">    random_index = np.random.choice(<span class="built_in">range</span>(<span class="built_in">len</span>(x)), batch_size, replace=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, index <span class="keyword">in</span> <span class="built_in">enumerate</span>(random_index):</span><br><span class="line">        random_x[i] = x[index]</span><br><span class="line">        random_y[i] = y[index]</span><br><span class="line">                </span><br><span class="line">    <span class="keyword">return</span> random_x, random_y</span><br></pre></td></tr></table></figure><p>然后写一个训练函数</p><p>pytorch封装了很多操作，比如说反向传播 <code>backward</code> 是不需要自己写的，只需要调用 <code>loss.backward()</code> 即可</p><p>随后，调用 <code>torch.optim</code> 中被称为优化器的工具就可以更新参数</p><p>像这里的 <code>optim.SGD</code> 就是随机梯度下降的参数更新方法。</p><p>注意，模型直到 <code>optimizer.step()</code> 这步才正式更新模型参数。这两步的分离是为了提供更大的灵活性，比如可以多次调用 <code>loss.backward()</code>累积梯度，然后在特定时刻执行一次 <code>optimizer.step()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = Simple_nn()</span><br><span class="line"><span class="comment"># 定义损失函数 </span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5000</span>):</span><br><span class="line">    input_batch, output_batch = random_batch()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 清除梯度缓存</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    output_pred = model(input_batch)</span><br><span class="line">    <span class="comment"># 计算损失函数</span></span><br><span class="line">    loss = criterion(output_pred, output_batch)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Epoch:&#x27;</span>, <span class="string">&#x27;%04d&#x27;</span> % (epoch + <span class="number">1</span>), <span class="string">&#x27;cost =&#x27;</span>, <span class="string">&#x27;&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(loss))</span><br><span class="line"></span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model.W.weight.t())</span><br></pre></td></tr></table></figure><p>最后得到输出，和我们一开始定义的 $W$ 是很接近的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.2985, 0.4066],</span><br><span class="line">        [0.5058, 0.5921],</span><br><span class="line">        [0.5955, 0.2015]], grad_fn=&lt;TBackward0&gt;)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">记录Pytorch的学习过程，目标是逐步深入，从基础到完整大型的pytorch架构。</summary>
    
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/categories/Study/"/>
    
    <category term="Neural Network" scheme="https://anti-entrophic.github.io/categories/Study/Neural-Network/"/>
    
    
    <category term="Neural Network" scheme="https://anti-entrophic.github.io/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>Nunjucks Error expected variable end解决办法</title>
    <link href="https://anti-entrophic.github.io/posts/10009.html"/>
    <id>https://anti-entrophic.github.io/posts/10009.html</id>
    <published>2023-10-17T04:36:19.000Z</published>
    <updated>2023-10-17T11:57:12.674Z</updated>
    
    <content type="html"><![CDATA[<p>在文章中写latex代码的时候，可能会遇到报错：Nunjucks Error expected variable end</p><p>比如下面这段</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">$</span><span class="built_in">$</span></span><br><span class="line"><span class="keyword">\begin</span>&#123;equation&#125;</span><br><span class="line">    <span class="keyword">\begin</span>&#123;array&#125;&#123;ll&#125;</span><br><span class="line">        p(w<span class="built_in">_</span>j|w<span class="built_in">_</span>i) <span class="built_in">&amp;</span>= y<span class="built_in">_</span>j <span class="keyword">\\</span></span><br><span class="line">                   <span class="built_in">&amp;</span>= <span class="keyword">\frac</span>&#123;e<span class="built_in">^</span>&#123;u<span class="built_in">_</span>j&#125;&#125;&#123;<span class="keyword">\sum</span><span class="built_in">_</span>&#123;k <span class="keyword">\in</span> V&#125; e<span class="built_in">^</span>&#123;u<span class="built_in">_</span>k&#125;&#125;   <span class="keyword">\\</span></span><br><span class="line">                   <span class="built_in">&amp;</span>= <span class="keyword">\frac</span>&#123; e<span class="built_in">^</span>&#123;&#123;W&#x27;<span class="built_in">_</span>j&#125;<span class="built_in">^</span>T <span class="keyword">\cdot</span> W<span class="built_in">_</span>I&#125; &#125; &#123;<span class="keyword">\sum</span><span class="built_in">_</span>&#123;k <span class="keyword">\in</span> V&#125; e<span class="built_in">^</span>&#123; &#123;W&#x27;<span class="built_in">_</span>k&#125;<span class="built_in">^</span>T <span class="keyword">\cdot</span> W<span class="built_in">_</span>I&#125; &#125; <span class="keyword">\\</span></span><br><span class="line">    <span class="keyword">\end</span>&#123;array&#125; <span class="keyword">\notag</span></span><br><span class="line"><span class="keyword">\end</span>&#123;equation&#125;</span><br><span class="line"><span class="built_in">$</span><span class="built_in">$</span></span><br></pre></td></tr></table></figure><p>查阅资料后是因为，Hexo使用Nunjucks渲染帖子，用 <code>&#123; &#123; &#125; &#125;</code> 或 <code>&#123;% %&#125;</code> 包装的内容将被解析，并可能导致问题。</p><p>可以使用下列标签包裹敏感字段，避免被Nunjucks错误解析。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% raw %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% endraw%&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在文章中写latex代码的时候，可能会遇到报错：Nunjucks Error expected variable end&lt;/p&gt;
&lt;p&gt;比如下面这段&lt;/p&gt;
&lt;figure class=&quot;highlight latex&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutt</summary>
      
    
    
    
    <category term="Hexo &amp; Butterfly tutorial" scheme="https://anti-entrophic.github.io/categories/Hexo-Butterfly-tutorial/"/>
    
    
    <category term="Hexo &amp; Butterfly tutorial" scheme="https://anti-entrophic.github.io/tags/Hexo-Butterfly-tutorial/"/>
    
  </entry>
  
  <entry>
    <title>NLP论文精读（一） Word2Vec</title>
    <link href="https://anti-entrophic.github.io/posts/10008.html"/>
    <id>https://anti-entrophic.github.io/posts/10008.html</id>
    <published>2023-10-16T15:39:21.000Z</published>
    <updated>2023-10-17T15:06:32.868Z</updated>
    
    <content type="html"><![CDATA[<div class="note success simple"><p>最好的学习方法就是把知识讲给别人听</p></div><p>开个新坑，努力！封面图也换上对我意义非凡的 Chtholly</p><p class='p left logo large'>基本概念</p><p>基本概念懒得写了。</p><p>Word2Vec是Google的Mikolov在2013年提出的一种词向量的表征形式。不同于稀疏的one-hot编码，对样本空间的利用率只有坐标轴上可怜的几个点; word embedding 就可以以更少的维度表示词语，更高效地利用样本空间，并可使单词的向量表征具有一定几何意义。</p><p class='p left logo large'>网络结构</p><p>Word2Vec一共给出了两种网络结构，CBOW和Skip-gram</p><h1 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h1><p>CBOW的任务简单来说就是，给定某个单词 $w_t$ 的上下文 $w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2}$，去估计中间这个单词应该是什么，也就是要计算 $p(w_t | w_{t-2}, w_{t-2}, w_{t-2}, w_{t-2})$，词向量是这一任务的一项产物。</p><h2 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h2><p>我们先考虑较为简单的一个单词的情况，即计算 $p(w_t|w_{I})$</p><p>CBOW模型的输入是一个one-hot向量，考虑当前有一个长度为|V|的词表，不妨设 $w_{oI}$ 是词表中的第 $I$ 个词的one-hot表示（和 $w_{t+1}$ 这种原来句子中的空间位置关系作区别），也即只有第 $I$ 个元素是1</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://pic2.zhimg.com/80/v2-ca4c641f4f3c9a44e43260c04c0161d1_1440w.webp" alt="CBOW模型示例，图源知乎" style="width:400px;"/></div><span class="image-caption">CBOW模型示例，图源知乎</span></div><p>CBOW仅有一个隐层，设输入层到隐层的权重为 $W$，输入层到输出层的权重为 $W’$，隐层神经元个数为 $N$, $W_i$ 表示 $W$ 的第 $i$ 行，${W’_j}$ 表示 $W’$ 的第 $j$ 列。</p><p>因为input是one-hot向量，可以知道隐层的输入 $h$ 应该等于 $W$ 的第 $I$ 行 $W_I$ ，$h$ 的维度是 $N \times 1$。</p><p>CBOW的设计中省略了隐藏层的非线性激活函数，仅仅是做了一个加权组合。</p><p>随后经过权重 $W’$ ，得到一个向量$u$，其中 $u_i = {W’_i}^T \cdot W_I$ ，并进行一次softmax归一化得到结果 $y$</p><p>输出结果是一个 $V \times 1$ 的向量，每一个元素 $y_i$ 表示了预测词为词表中第 $i$ 个词的概率。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>对于一个输入 $w_{oi}$，我们已知它的输出应该是 $w_{oj}$，也就是说我们应该最大化目标函数$p(w_{oj}|w_{oi})$, 即 $y_j$</p>$$\begin{equation}    \begin{array}{ll}        p(w_{oj}|w_{oi}) &= y_j \\                   &= \frac{e^{u_j}}{\sum_{k \in V} e^{u_k}}   \\                   &= \frac{ e^{{W'_j}^T \cdot W_I} } {\sum_{k \in V} e^{ {W'_k}^T \cdot W_I} } \\    \end{array} \notag\end{equation}$$<p>我们对其取对数,则</p><script type="math/tex; mode=display">log \; p(w_{oj}|w_{oi}) = {W'_j}^T \cdot W_I - log{\sum_{k \in V} {W'_k}^T \cdot W_I}</script><p>损失函数就是目标函数取负就可以了。</p><p>如果有多个输入，仅需在第一步的时候，对各个输入均经过相同的权重矩阵 $W$ ，最后隐藏层输入 $h$ 平均即可。</p><p>（关于对这步多个输入求和平均，我一开始也有些困惑，感觉会不会对效果产生什么影响。不过如果是单个输入的话，可能不同输入之间的影响抵消的影响会比较大。作者应该也是实验过了现在的结果好？不懂。然后模型也抛弃了和中心词相隔距离的参数。）</p><p>考虑上下文窗口长度为 $c$（前后各 $c$ 个总共 $2c$ 个），我们要求的损失函数就是：</p><script type="math/tex; mode=display">\sum_{-c \leq j \leq c, j \neq 0}log \; p(w_t|w_{t+j});</script><p>这个其实可以直接用交叉熵损失函数算，原因在：<a href="https://zhuanlan.zhihu.com/p/115277553">https://zhuanlan.zhihu.com/p/115277553</a> ，写得特别好！值得我重新开一篇博文记录一下。 </p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>作者很厉害，那个时候没有Tensorflow、PyTorch这种深度学习框架，整个代码是C语言完成的并且所有梯度都是手算的。</p><p>其实用上深度学习框架后网络非常简单，损失函数用 <code>criterion = nn.CrossEntropyLoss()</code> 就可以了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Model</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Word2Vec</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Word2Vec, self).__init__()</span><br><span class="line">        <span class="comment"># 输入层到隐层权重矩阵</span></span><br><span class="line">        self.W = nn.Linear(voc_size, embedding_size, bias=<span class="literal">False</span>) <span class="comment"># voc_size &gt; embedding_size Weight</span></span><br><span class="line">        <span class="comment"># 隐层到输出层权重矩阵</span></span><br><span class="line">        self.WT = nn.Linear(embedding_size, voc_size, bias=<span class="literal">False</span>) <span class="comment"># embedding_size &gt; voc_size Weight</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># X : [batch_size, voc_size]</span></span><br><span class="line">        hidden_layer = self.W(X) <span class="comment"># hidden_layer : [batch_size, embedding_size]</span></span><br><span class="line">        output_layer = self.WT(hidden_layer) <span class="comment"># output_layer : [batch_size, voc_size]</span></span><br><span class="line">        <span class="keyword">return</span> output_layer</span><br></pre></td></tr></table></figure><h1 id="skip-gram"><a href="#skip-gram" class="headerlink" title="skip-gram"></a>skip-gram</h1><p>说实话我觉得就是CBOW倒过来，，，形式也是一样的。</p><p class='p left logo large'>效率优化</p><h1 id="hierarchical-softmax"><a href="#hierarchical-softmax" class="headerlink" title="hierarchical softmax"></a>hierarchical softmax</h1><p>TODO</p><p class='p left logo large'>参考链接</p><p><a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/</a> 超级保姆级教程<br><a href="https://adoni.github.io/2017/11/08/word2vec-pytorch/">https://adoni.github.io/2017/11/08/word2vec-pytorch/</a> </p>]]></content>
    
    
    <summary type="html">开始论文学习之路</summary>
    
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/categories/Study/"/>
    
    <category term="NLP" scheme="https://anti-entrophic.github.io/categories/Study/NLP/"/>
    
    
    <category term="NLP" scheme="https://anti-entrophic.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>计算机网络 Wireshark的使用</title>
    <link href="https://anti-entrophic.github.io/posts/10007.html"/>
    <id>https://anti-entrophic.github.io/posts/10007.html</id>
    <published>2023-10-16T09:30:21.000Z</published>
    <updated>2023-10-16T10:23:32.209Z</updated>
    
    <content type="html"><![CDATA[<p>随便写写</p><p>Wireshark是网络包分析工具，主要作用是在接囗实时捕捉网络包，并详细显示包的协议信息。</p><ul><li><p>可以捕捉多种网络接囗类型的包，包括无线局域网接囗。</p></li><li><p>可以支持多种协议的解码，如TCP，DNS等。</p></li></ul><h1 id="过滤器"><a href="#过滤器" class="headerlink" title="过滤器"></a>过滤器</h1><p>Wireshark的过滤器分为捕获过滤器和显示过滤器</p><ul><li><p>前者需要在捕捉前设置好，决定捕捉什么包</p></li><li><p>后者在捕获过程中及结束后可以随时修改，只是显示捕获结果符合要求的包</p></li></ul><h1 id="捕获过滤器语法"><a href="#捕获过滤器语法" class="headerlink" title="捕获过滤器语法"></a>捕获过滤器语法</h1><p>这里很全：<a href="https://blog.csdn.net/qq_39720249/article/details/128157288">https://blog.csdn.net/qq_39720249/article/details/128157288</a></p><h2 id="基于协议过滤"><a href="#基于协议过滤" class="headerlink" title="基于协议过滤"></a>基于协议过滤</h2><ul><li>例：只捕获端口为80的tcp数据包</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcp port 80</span><br></pre></td></tr></table></figure><h2 id="基于方向过滤"><a href="#基于方向过滤" class="headerlink" title="基于方向过滤"></a>基于方向过滤</h2><p>可以指定获取 源src 或是 目的dst 方向的数据包，也可以用 src and dst 或是 src or dst</p><ul><li>例：只捕获目的ip为本机ip的ipv4数据包</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dst host &lt;本机ip&gt;</span><br></pre></td></tr></table></figure><h2 id="基于类型过滤"><a href="#基于类型过滤" class="headerlink" title="基于类型过滤"></a>基于类型过滤</h2><p>可选项有 主机host，网段net，端口port，端口范围portrange 等</p><ul><li>例：只捕获端口不为80的数据包</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">not port 80</span><br></pre></td></tr></table></figure><h1 id="显示过滤器语法"><a href="#显示过滤器语法" class="headerlink" title="显示过滤器语法"></a>显示过滤器语法</h1><p>例：显示ip为本机ip的dns数据包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip.addr == &lt;本机ip&gt; &amp;&amp; dns</span><br></pre></td></tr></table></figure><p>但是会发现这样什么包都没有。</p><p>将显示过滤器条件简化为dns后发现能正确找到相关dns数据包，但是其src和dst都是我本机的临时Ipv6地址。</p><p>将表达式更改为以下式子即可正确获取dns数据包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ipv6.addr == &lt;本机ip&gt; &amp;&amp; dns</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">作业，顺便记录一下Wireshark的使用方法</summary>
    
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/categories/Study/"/>
    
    <category term="Computer Network" scheme="https://anti-entrophic.github.io/categories/Study/Computer-Network/"/>
    
    
    <category term="Computer Network" scheme="https://anti-entrophic.github.io/tags/Computer-Network/"/>
    
    <category term="Study" scheme="https://anti-entrophic.github.io/tags/Study/"/>
    
    <category term="Notes" scheme="https://anti-entrophic.github.io/tags/Notes/"/>
    
  </entry>
  
  <entry>
    <title>FDU Operating System Lab2</title>
    <link href="https://anti-entrophic.github.io/posts/10006.html"/>
    <id>https://anti-entrophic.github.io/posts/10006.html</id>
    <published>2023-10-15T10:46:34.000Z</published>
    <updated>2023-10-16T15:29:59.424Z</updated>
    
    <content type="html"><![CDATA[<p>因为中途才开始写博客，一些配置上的问题可能没讲清楚。目前只是自己做一个记录吧。</p><p>这项作业还没截止，，，应该不会有同学搜到吧？等截止了再把剩下几块补上</p><p>环境：WSL + xv6</p><p>LAB2：<a href="https://docs.qq.com/slide/DR2VtU3Fvb2hGWEN0">https://docs.qq.com/slide/DR2VtU3Fvb2hGWEN0</a></p><h1 id="实验准备"><a href="#实验准备" class="headerlink" title="实验准备"></a>实验准备</h1><p>切换到本次实验的环境分支下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git commit -am lab0</span><br><span class="line">git fetch</span><br><span class="line">git checkout syscall</span><br><span class="line">make clean</span><br></pre></td></tr></table></figure><p>随后按要求修改相应 <code>kernel</code> 与 <code>user</code> 文件夹下的文件，以任务1的 <code>procnum</code> 为例，其余同理。</p><p>在 <code>kernel/syscall.h</code> 中，添加一个宏定义</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> SYS_procnum 22</span></span><br></pre></td></tr></table></figure><p>然后在 <code>kernel/syscall.c</code> 指定系统调用的主体函数，即第22号System call会调用 <code>sys_procnum</code> 这个指针指向的函数</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">extern</span> uint64 <span class="title function_">sys_procnum</span><span class="params">(<span class="type">void</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">static</span> <span class="title function_">uint64</span> <span class="params">(*syscalls[])</span><span class="params">(<span class="type">void</span>)</span> = &#123;</span><br><span class="line"><span class="comment">//...</span></span><br><span class="line">[SYS_procnum]  sys_procnum,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 <code>kernel/sysproc.c</code> 中添加具体实现的主体函数（在后面会介绍）</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">uint64</span><br><span class="line"><span class="title function_">sys_procnum</span><span class="params">(<span class="type">void</span>)</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">//implementation of sys_procnum</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 <code>user/usys.pl</code> 中添加系统调用的存根</p><figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">entry(<span class="string">&quot;procnum&quot;</span>);</span><br></pre></td></tr></table></figure><p>具体而言，就是 <code>Makefile</code> 会调用这个 <code>Perl</code> 脚本，生成一段汇编码在 <code>usys.S</code> 中，这是 <code>user.h</code> 中定义的函数实际的实现，即调用的地方。</p><p>点进 <code>usys.S</code> 可以看到诸如 <code>li a7, SYS_fork</code> 这种，还记得之前把 <code>SYS_fork</code> 宏定义为了数字吧，就是在这里派用场。</p><p>为了能够让用户程序访问到 <code>procnum</code> 系统调用，我们需要在 <code>user/user.h</code> 中声明该调用：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// system calls</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">procnum</span><span class="params">(<span class="type">int</span>*)</span>;</span><br></pre></td></tr></table></figure><p>你可能会注意到这里有一个参数列表上的不匹配，会发现所有 <code>kernel/sysproc.c</code> 中的系统调用实现形参列表都是void。</p><p>这是因为 <code>procnum()</code> 这样的函数是用户级别的函数，而 <code>sys_procnum()</code> 是内核级别的函数，用户空间和内核空间有不同的数据访问规则和隔离。</p><p>前者需要将参数在用户空间打包成适当的数据结构，后者会通过辅助函数 <code>argaddr()</code> 或 <code>argint()</code> 等获取用户空间的参数到内核空间，并在其中进行合法与安全性检查，确保不会引发内核的错误。</p><p>可以看看其它system call的实现方法，比如wait()，是怎么获取参数列表的</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">argaddr</span><span class="params">(<span class="type">int</span> n, uint64 *addr)</span>;</span><br><span class="line"><span class="type">int</span> <span class="title function_">argint</span><span class="params">(<span class="type">int</span> n, <span class="type">int</span> *ip)</span>;</span><br></pre></td></tr></table></figure><h1 id="任务1：process-counting"><a href="#任务1：process-counting" class="headerlink" title="任务1：process counting"></a>任务1：process counting</h1><ul><li>系统调用功能 procnum：统计系统总进程数</li></ul><p>在user文件夹下添加检验程序 procnum.c</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;kernel/types.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;kernel/riscv.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;kernel/sysinfo.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;user/user.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span></span><br><span class="line"><span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> *argv[])</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">if</span>(argc &gt;= <span class="number">2</span>) &#123;</span><br><span class="line"><span class="built_in">fprintf</span>(<span class="number">2</span>, <span class="string">&quot;procnum: Too many arguments\n&quot;</span>);</span><br><span class="line"><span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> num = <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (procnum(&amp;num) &lt; <span class="number">0</span>) &#123;</span><br><span class="line"><span class="built_in">fprintf</span>(<span class="number">2</span>, <span class="string">&quot;procnum failed!\n&quot;</span>);</span><br><span class="line"><span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;Number of process: %d\n&quot;</span>, num);</span><br><span class="line"><span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在makefile中添加编译项</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">UPROGS=\</span><br><span class="line">    ...</span><br><span class="line">$U/_procnum\</span><br></pre></td></tr></table></figure><p>完成 <code>kernel/sysproc.c</code> 中函数的具体实现。我整体是对着 <code>sys_wait()</code> 函数学的。</p><p>先用 <code>argaddr()</code> 获取传过来的参数，是变量num的地址</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">uint64</span><br><span class="line"><span class="title function_">sys_procnum</span><span class="params">(<span class="type">void</span>)</span></span><br><span class="line">&#123;</span><br><span class="line">  uint64 p;</span><br><span class="line">  argaddr(<span class="number">0</span>, &amp;p);</span><br><span class="line">  <span class="keyword">return</span> procnum(p);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>和 <code>sys_wait()</code> 函数一样，我们把实现放在了 <code>kernel/proc.c</code> 中，记得在 <code>defs.h</code> 中先定义函数</p><p>因为内核空间和用户空间存在隔离，所以只能使用 <code>copyout()</code> 这种辅助函数实现修改用户空间的变量，而不能直接拿指针去操作。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Count the total number of processes in the system.</span></span><br><span class="line"><span class="type">int</span></span><br><span class="line"><span class="title function_">procnum</span><span class="params">(uint64 addr)</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">proc</span> *<span class="title">pp</span>;</span> <span class="comment">// 声明一个指向struct proc的指针pp，用于遍历进程表中的进程。</span></span><br><span class="line">  <span class="type">int</span> proc_num = <span class="number">0</span>; <span class="comment">// 统计进程数</span></span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">proc</span> *<span class="title">p</span> =</span> myproc(); <span class="comment">//获取当前进程的指针并存储在p中。</span></span><br><span class="line">  <span class="keyword">for</span>(pp = proc; pp &lt; &amp;proc[NPROC]; pp++)&#123; <span class="comment">//遍历整个进程表，查找子进程。</span></span><br><span class="line"><span class="keyword">if</span>(pp-&gt;state != <span class="number">0</span>) &#123; <span class="comment">// 只要进程状态不是unused</span></span><br><span class="line">proc_num++;</span><br><span class="line">&#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span>(addr != <span class="number">0</span> &amp;&amp; copyout(p-&gt;pagetable, addr, (<span class="type">char</span> *)&amp;proc_num, <span class="keyword">sizeof</span>(proc_num)) &lt; <span class="number">0</span>) &#123;</span><br><span class="line"><span class="comment">// 这个比较复杂，，首先检查 addr 是否为非零值。</span></span><br><span class="line"><span class="comment">// 如果 addr 不为零就调用 copyout 函数，将数据从内核空间复制到用户空间。</span></span><br><span class="line"><span class="comment">// p 是当前进程的指针，p-&gt;pagetable 存储了当前进程的页表。页表是一种数据结构，用于将虚拟地址映射到物理地址，以便访问内存中的数据。</span></span><br><span class="line"><span class="comment">// addr 是用户程序提供的目标地址，数据将被复制到这里</span></span><br><span class="line"><span class="comment">// (char *)&amp;proc_num 是要复制的源数据的地址。需要以char*的形式传递源数据的地址</span></span><br><span class="line"><span class="comment">// sizeof(pp-&gt;xstate) 是要复制的数据的大小，以字节为单位。</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> proc_num;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其余几项任务也差不多，照猫画虎。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;因为中途才开始写博客，一些配置上的问题可能没讲清楚。目前只是自己做一个记录吧。&lt;/p&gt;
&lt;p&gt;这项作业还没截止，，，应该不会有同学搜到吧？等截止了再把剩下几块补上&lt;/p&gt;
&lt;p&gt;环境：WSL + xv6&lt;/p&gt;
&lt;p&gt;LAB2：&lt;a href=&quot;https://docs.q</summary>
      
    
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/categories/Study/"/>
    
    <category term="Operating System" scheme="https://anti-entrophic.github.io/categories/Study/Operating-System/"/>
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/tags/Study/"/>
    
    <category term="Notes" scheme="https://anti-entrophic.github.io/tags/Notes/"/>
    
    <category term="Operating System" scheme="https://anti-entrophic.github.io/tags/Operating-System/"/>
    
  </entry>
  
  <entry>
    <title>BP神经网络与反向传播算法</title>
    <link href="https://anti-entrophic.github.io/posts/10005.html"/>
    <id>https://anti-entrophic.github.io/posts/10005.html</id>
    <published>2023-10-14T08:31:31.000Z</published>
    <updated>2023-10-17T03:22:36.506Z</updated>
    
    <content type="html"><![CDATA[<p>本文还不是很完善</p><p>简单推导一下反向传播算法的公式</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://picst.sunbangyan.cn/2023/10/14/r3efwk.png" alt="多层前馈神经网络示例。" style="width:400px;"/></div><span class="image-caption">多层前馈神经网络示例。</span></div><p>设激活函数为sigmoid函数，误差函数采用均方误差，正向传播得到的结果为 $\hat{\boldsymbol{y}}$，真实值为y。</p><h1 id="最后一层参数"><a href="#最后一层参数" class="headerlink" title="最后一层参数"></a>最后一层参数</h1><p>设隐层 $b_i$ 到输出层 $y_j$ 的参数为 $w_{ij} $</p><p>以更新 $w_{11}$ 为例，现在已经得到了均方误差 $E$ :</p><script type="math/tex; mode=display">E = \frac{1}{2} \sum_{i=1}^n (\hat{y_i} - y_i)^2</script><p>用梯度下降最小化均方误差更新隐层到输出层参数，由链式法则</p><script type="math/tex; mode=display">\frac{\partial E}{\partial w_{11}} = \frac{\partial E}{\partial \hat{y_1}} \cdot \frac{\partial \hat{y_1}}{\partial \beta_{1}} \cdot \frac{\partial \beta_1}{\partial w_{11}}</script><script type="math/tex; mode=display">\begin{equation}\left\{    \begin{array}{ll}        \frac{\partial E}{\partial \hat{y_1}} = \hat{y_1}-y_1 & \\        \frac{\partial \hat{y_1}}{\partial \beta_{1}} = \hat{y_1} (1- \hat{y_1}) & 对sigmoid求导 \\    \frac{\partial \beta_1}{\partial w_{11}} = b_1    \end{array}\right. \notag\end{equation}</script><p>所以 $\Delta w_{11} = b_1\hat{y_1}(1-\hat{y_1})(\hat{y_1}-y_1)$ </p><p>令 $\boldsymbol{g_i} = \hat{y_1}(1-\hat{y_1})(\hat{y_1}-y_1)$</p><h1 id="隐层之间的参数"><a href="#隐层之间的参数" class="headerlink" title="隐层之间的参数"></a>隐层之间的参数</h1><p>考虑</p><script type="math/tex; mode=display">\frac{\partial E}{\partial v_{11}} = \frac{\partial E}{\partial b_1} \cdot \frac{\partial b_1}{\partial \alpha_{1}} \cdot \frac{\partial \alpha_1}{\partial v_{11}}</script><p>其中</p><script type="math/tex; mode=display">\begin{equation}\left\{    \begin{array}{ll}        \frac{\partial E}{\partial b_1} & =  \frac{\partial E}{\partial \beta_1} \cdot \frac{\partial \beta_1}{\partial b_1} + \frac{\partial E}{\partial \beta_2} \cdot \frac{\partial \beta_2}{\partial b_1} + \frac{\partial E}{\partial \beta_3} \cdot \frac{\partial \beta_3}{\partial b_1}\\        \frac{\partial b_1}{\partial \alpha_{1}} & = b_1 (1- b_1) & sigmoid \\    \frac{\partial \alpha_1}{\partial v_{11}} & = x_1    \end{array}\right. \notag\end{equation}</script><p>代入可得 $\Delta v_{11} = x_1b_1(1-b_1)\sum_{i=1}^3 g_iw_{1i}$</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>最后隐层到输出层和隐层之间梯度更新公式是不一样的，都是正常使用链式法则求导出来的。</p><p>隐层之间的公式是相同的，如果是多层网络的话也是同样的推导过程，把变量换成对应的前一层的内容即可。</p><p>有时间的话，希望能推一个一般性的递推公式出来。网上都找不到完整证明。。</p><p>希望能有一个向量化的公式啊</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本文还不是很完善&lt;/p&gt;
&lt;p&gt;简单推导一下反向传播算法的公式&lt;/p&gt;
&lt;div class=&quot;img-wrap&quot;&gt;&lt;div class=&quot;img-bg&quot;&gt;&lt;img class=&quot;img&quot; src=&quot;https://picst.sunbangyan.cn/2023/10/1</summary>
      
    
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/categories/Study/"/>
    
    <category term="Neural Network" scheme="https://anti-entrophic.github.io/categories/Study/Neural-Network/"/>
    
    
    <category term="Study" scheme="https://anti-entrophic.github.io/tags/Study/"/>
    
    <category term="Notes" scheme="https://anti-entrophic.github.io/tags/Notes/"/>
    
    <category term="Neural Network" scheme="https://anti-entrophic.github.io/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>自定义页面</title>
    <link href="https://anti-entrophic.github.io/posts/10004.html"/>
    <id>https://anti-entrophic.github.io/posts/10004.html</id>
    <published>2023-10-14T05:17:52.000Z</published>
    <updated>2023-10-14T05:37:46.175Z</updated>
    
    <content type="html"><![CDATA[<p>Hexo &amp; butterfly 会自动按照其设置指定的方式对markdown文件进行渲染，生成对应的博客页面。</p><p>不过可能除了写博客，我们还希望能在博客上部署一些小页面小功能，butterfly也是支持的。</p><p>我们可以在 <code>/source</code> 文件夹下新建一个html文件夹，将我们写好的页面（html、css、javascript）一起放进去。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">source</span><br><span class="line">    |-- html</span><br><span class="line">          |-- index.html  # 主要的小工具总览页面</span><br><span class="line">          |-- Tool1  # 各种不同的小工具</span><br><span class="line">          |-- Tool2</span><br><span class="line">          |-- ...</span><br></pre></td></tr></table></figure><p>在 <code>_config.yml</code> ，我们需要指定hexo让其跳过对html文件夹下内容的默认渲染：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">skip_render:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&quot;html/**&quot;</span>  </span><br></pre></td></tr></table></figure><p>最后，我们可以在menu栏添加一个Tool图标，会首先跳转到一个总览页面，专门用于存放自己的小工具：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">menu:</span></span><br><span class="line">  <span class="attr">Tools:</span> <span class="string">/html/index.html</span> <span class="string">||</span> <span class="string">fas</span> <span class="string">fa-archive</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">介绍如何在博客上部署自定义页面</summary>
    
    
    
    <category term="Hexo &amp; Butterfly tutorial" scheme="https://anti-entrophic.github.io/categories/Hexo-Butterfly-tutorial/"/>
    
    
    <category term="Hexo &amp; Butterfly tutorial" scheme="https://anti-entrophic.github.io/tags/Hexo-Butterfly-tutorial/"/>
    
  </entry>
  
  <entry>
    <title>博客书写指南</title>
    <link href="https://anti-entrophic.github.io/posts/10003.html"/>
    <id>https://anti-entrophic.github.io/posts/10003.html</id>
    <published>2023-10-13T14:59:34.000Z</published>
    <updated>2023-10-17T04:35:15.334Z</updated>
    
    <content type="html"><![CDATA[<p>每篇博客都是一个markdown文件，由Butterfly渲染生成页面，下面介绍一些文章的设置以及写法：</p><p class='p left logo large'>文章属性</p><p>每篇文章的开头都有一个<code>Front-matter</code>，用于提供这篇文章的相关信息，详情请看 <a href="https://butterfly.js.org/posts/dc584b87/">https://butterfly.js.org/posts/dc584b87/</a></p><p>以本篇文章的设置为例，讲一下各个属性的意义：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 博客书写指南</span><br><span class="line">cover: &#x27;https://picst.sunbangyan.cn/2023/10/13/oynsmu.jpg&#x27;</span><br><span class="line">abbrlink: 10003</span><br><span class="line">date: 2023-10-13 22:59:32</span><br><span class="line">tags: [Hexo &amp; Butterfly tutorial]</span><br><span class="line">categories: </span><br><span class="line"><span class="bullet">  -</span> [Hexo &amp; Butterfly tutorial]</span><br><span class="line">updated:</span><br><span class="line">type:</span><br><span class="line">comments:</span><br><span class="line">description: 介绍博客文章的书写方法</span><br><span class="line">keywords:</span><br><span class="line">top<span class="emphasis">_img:</span></span><br><span class="line"><span class="emphasis">mathjax:</span></span><br><span class="line"><span class="emphasis">katex:</span></span><br><span class="line"><span class="emphasis">aside:</span></span><br><span class="line"><span class="emphasis">aplayer:</span></span><br><span class="line"><span class="emphasis">highlight_</span>shrink:</span><br><span class="line"><span class="section">random:</span></span><br><span class="line"><span class="section">---</span></span><br></pre></td></tr></table></figure><ul><li><p>title</p><p>博客的标题</p></li><li><p>cover </p><p>博客的封面（显示在首页）</p></li><li><p>abbrlink</p><p>由插件 <code>hexo-abbrlink</code> 生成，替换hexo原生的页面url生成方案。该值可自定义，唯一指定一篇文章（不可重复），同时也是该页面的url地址。</p></li><li><p>date</p><p>文章的发表时间</p></li><li><p>tags</p><p>该篇文章的标签，语法为 tags:[tag1, tag2, …]</p></li><li><p>categories</p><p>该篇文章的分类，推荐语法为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">categories: </span><br><span class="line">  - [A, B]</span><br></pre></td></tr></table></figure><p>表示该文章属于分类A下的子类B</p></li><li><p>description</p><p>在首页看到的文章内容简介。如果设置了 <code>description</code> ，则会显示这个，否则会显示文章内容节选</p><p>这一项可以在 <code>_config.butterfly.yml</code> 中的 <code>index_post_content</code> 一栏修改（详见：<a href="https://anti-entrophic.github.io/posts/10002.html）">https://anti-entrophic.github.io/posts/10002.html）</a></p></li><li><p>待补充</p></li></ul><p class='p left logo large'>自定义页面字体</p><h1 id="创建css文件"><a href="#创建css文件" class="headerlink" title="创建css文件"></a>创建css文件</h1><p>以我目前使用的字体为例，首先需要在 <code>/&#123;root&#125;/source/css</code> 目录下新建一个文件，暂且命名为 <code>custom.css</code> ，内容如下：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*感谢安知鱼大佬的教程！*/</span></span><br><span class="line"><span class="keyword">@font-face</span> &#123;</span><br><span class="line">    <span class="attribute">font-family</span>: ZhuZiAYuanJWD;</span><br><span class="line">    <span class="attribute">src</span>: <span class="built_in">url</span>(<span class="string">https://npm.elemecdn.com/anzhiyu-blog@1.1.6/fonts/ZhuZiAWan.woff2</span>);</span><br><span class="line">    <span class="attribute">font-display</span>: swap;</span><br><span class="line">    <span class="attribute">font-weight</span>: lighter;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="selector-tag">body</span> &#123;</span><br><span class="line">    <span class="attribute">font-family</span>: <span class="string">&quot;ZhuZiAYuanJWD&quot;</span>;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>当然，你也可以不指定全局使用这一个字体，可以换，比如说仅指定导航栏当然也是可以的，可以自己魔改：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">div</span><span class="selector-id">#menus</span> &#123;</span><br><span class="line">  <span class="attribute">font-family</span>: <span class="string">&quot;ZhuZiAYuanJWD&quot;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="引入css文件"><a href="#引入css文件" class="headerlink" title="引入css文件"></a>引入css文件</h1><p>在 <code>_config.butterfly.yml</code> 中找到 <code>inject</code> 这一栏，就像html链接css一样引入：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">inject:</span></span><br><span class="line">  <span class="attr">head:</span></span><br><span class="line">    <span class="comment"># 自定义CSS</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&lt;link</span> <span class="string">rel=&quot;stylesheet&quot;</span> <span class="string">href=&quot;/css/custom.css&quot;</span> <span class="string">media=&quot;defer&quot;</span> <span class="string">onload=&quot;this.media=&#x27;all&#x27;&quot;&gt;</span></span><br><span class="line">    <span class="comment"># 暂时不清楚media和onload有什么用</span></span><br></pre></td></tr></table></figure><p>之后，在 <code>_config.butterfly.yml</code> 中指定渲染时使用我们新引入的字体：（这步不清楚需不需要）</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">font:</span></span><br><span class="line">  <span class="attr">global-font-size:</span></span><br><span class="line">  <span class="attr">code-font-size:</span></span><br><span class="line">  <span class="attr">font-family:</span> <span class="string">ZhuZiAYuanJWD</span></span><br><span class="line">  <span class="attr">code-font-family:</span> <span class="string">ZhuZiAYuanJWD</span></span><br></pre></td></tr></table></figure><p>其中，<code>font-family</code> 是全局的字体， <code>code-font-family</code> 是代码块中的字体</p>]]></content>
    
    
    <summary type="html">介绍博客文章的书写方法</summary>
    
    
    
    <category term="Hexo &amp; Butterfly tutorial" scheme="https://anti-entrophic.github.io/categories/Hexo-Butterfly-tutorial/"/>
    
    
    <category term="Hexo &amp; Butterfly tutorial" scheme="https://anti-entrophic.github.io/tags/Hexo-Butterfly-tutorial/"/>
    
  </entry>
  
  <entry>
    <title>hexo&amp;butterfly配置教程</title>
    <link href="https://anti-entrophic.github.io/posts/10002.html"/>
    <id>https://anti-entrophic.github.io/posts/10002.html</id>
    <published>2023-10-13T14:59:33.000Z</published>
    <updated>2023-10-14T05:55:38.309Z</updated>
    
    <content type="html"><![CDATA[<div class="note success simple"><p>介绍一些博客的 Hexo &amp; butterfly 配置修改方法</p></div><p class='p left logo large'>主页文章简介</p><p>在主页中看到的文章内容的简介，<code>butterfly</code> 共提供了四种选择：</p><ol><li>description： 只显示description</li><li>both： 优先选择description；如果没有配置description，则显示文章节选内容</li><li>auto_excerpt： 只显示文章节选内容</li><li>false： 不显示文章内容</li></ol><p>在 <code>_config.butterfly.yml</code> 修改条目如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">index_post_content:</span></span><br><span class="line">  <span class="attr">method:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">length:</span> <span class="number">500</span>  <span class="comment"># if you set method to 2 or 3, the length need to config</span></span><br></pre></td></tr></table></figure><p class='p left logo large'>跳过渲染</p><p>hexo会自动把source下的文件识别渲染为博客页面，但有时我们并不希望自动渲染，而是想要其维持我们设计的界面。</p><p>在 <code>_config.yml</code> 修改条目如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">skip_render:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&quot;html/**&quot;</span>  <span class="comment"># &quot;**&quot;：通配符，html文件夹下的所有文件；&quot;*&quot;，html文件夹下一层的所有文件</span></span><br></pre></td></tr></table></figure><p class='p left logo large'>侧边栏</p><p>在 <code>_config.butterfly.yml</code> 相关条目：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">aside:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">hide:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">button:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">mobile:</span> <span class="literal">true</span> <span class="comment"># display on mobile</span></span><br><span class="line">  <span class="attr">position:</span> <span class="string">right</span> <span class="comment"># left or right</span></span><br><span class="line">  <span class="attr">display:</span></span><br><span class="line">    <span class="attr">archive:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">tag:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">category:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">card_author:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">description:</span></span><br><span class="line">    <span class="attr">button:</span></span><br><span class="line">      <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">icon:</span> <span class="string">fab</span> <span class="string">fa-github</span></span><br><span class="line">      <span class="attr">text:</span> <span class="string">Follow</span> <span class="string">Me</span></span><br><span class="line">      <span class="attr">link:</span> <span class="string">https://github.com/Anti-Entrophic</span></span><br><span class="line">  <span class="attr">card_announcement:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">content:</span> <span class="string">This</span> <span class="string">is</span> <span class="string">my</span> <span class="string">Blog</span></span><br><span class="line">  <span class="attr">card_recent_post:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">limit:</span> <span class="number">5</span> <span class="comment"># if set 0 will show all</span></span><br><span class="line">    <span class="attr">sort:</span> <span class="string">date</span> <span class="comment"># date or updated</span></span><br><span class="line">    <span class="attr">sort_order:</span> <span class="comment"># Don&#x27;t modify the setting unless you know how it works</span></span><br><span class="line">  <span class="attr">card_categories:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">limit:</span> <span class="number">8</span> <span class="comment"># if set 0 will show all</span></span><br><span class="line">    <span class="attr">expand:</span> <span class="string">none</span> <span class="comment"># none/true/false</span></span><br><span class="line">    <span class="attr">sort_order:</span> <span class="comment"># Don&#x27;t modify the setting unless you know how it works</span></span><br><span class="line">  <span class="attr">card_tags:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">limit:</span> <span class="number">40</span> <span class="comment"># if set 0 will show all</span></span><br><span class="line">    <span class="attr">color:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">orderby:</span> <span class="string">random</span> <span class="comment"># Order of tags, random/name/length</span></span><br><span class="line">    <span class="attr">order:</span> <span class="number">1</span> <span class="comment"># Sort of order. 1, asc for ascending; -1, desc for descending</span></span><br><span class="line">    <span class="attr">sort_order:</span> <span class="comment"># Don&#x27;t modify the setting unless you know how it works</span></span><br><span class="line">  <span class="attr">card_archives:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">monthly</span> <span class="comment"># yearly or monthly</span></span><br><span class="line">    <span class="attr">format:</span> <span class="string">MMMM</span> <span class="string">YYYY</span> <span class="comment"># eg: YYYY年MM月</span></span><br><span class="line">    <span class="attr">order:</span> <span class="number">-1</span> <span class="comment"># Sort of order. 1, asc for ascending; -1, desc for descending</span></span><br><span class="line">    <span class="attr">limit:</span> <span class="number">8</span> <span class="comment"># if set 0 will show all</span></span><br><span class="line">    <span class="attr">sort_order:</span> <span class="comment"># Don&#x27;t modify the setting unless you know how it works</span></span><br><span class="line">  <span class="attr">card_webinfo:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">post_count:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">last_push_date:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">sort_order:</span> <span class="comment"># Don&#x27;t modify the setting unless you know how it works</span></span><br><span class="line">  <span class="attr">card_post_series:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">orderBy:</span> <span class="string">&#x27;date&#x27;</span> <span class="comment"># Order by title or date</span></span><br><span class="line">    <span class="attr">order:</span> <span class="number">-1</span> <span class="comment"># Sort of order. 1, asc for ascending; -1, desc for descending</span></span><br><span class="line"></span><br><span class="line"><span class="attr">social:</span></span><br><span class="line">  <span class="comment"># fab fa-github: https://github.com/Anti-Entrophic || Github || &#x27;#24292e&#x27;</span></span><br></pre></td></tr></table></figure><p><code>aside</code> 的注释很详细，对照着看就知道功能了。<code>social</code> 指的是在 Follow me 下面出现的图标。</p>]]></content>
    
    
    <summary type="html">hexo&amp;butterfly 配置个性化魔改教程</summary>
    
    
    
    <category term="Hexo &amp; Butterfly tutorial" scheme="https://anti-entrophic.github.io/categories/Hexo-Butterfly-tutorial/"/>
    
    
    <category term="Hexo &amp; Butterfly tutorial" scheme="https://anti-entrophic.github.io/tags/Hexo-Butterfly-tutorial/"/>
    
  </entry>
  
  <entry>
    <title>博客时间线</title>
    <link href="https://anti-entrophic.github.io/posts/d87f7e0c.html"/>
    <id>https://anti-entrophic.github.io/posts/d87f7e0c.html</id>
    <published>2023-10-12T12:05:47.000Z</published>
    <updated>2023-10-14T05:15:50.815Z</updated>
    
    <content type="html"><![CDATA[<div class="timeline undefined"><div class='timeline-item headline'><div class='timeline-item-title'><div class='item-circle'><p>2023</p></div></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>10-13</p></div></div><div class='timeline-item-content'><p>完成博客基础建设与部署</p></div></div></div>]]></content>
    
    
    <summary type="html">建站时间线~</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://anti-entrophic.github.io/posts/4a17b156.html"/>
    <id>https://anti-entrophic.github.io/posts/4a17b156.html</id>
    <published>2023-10-12T12:01:19.973Z</published>
    <updated>2023-10-13T06:03:23.139Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
