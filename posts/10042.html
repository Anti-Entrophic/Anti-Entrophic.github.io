<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Triton Tutorial | 不会魔法的小圆</title><meta name="author" content="不会魔法的小圆"><meta name="copyright" content="不会魔法的小圆"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="都什么年代，还在用传统pytorch">
<meta property="og:type" content="article">
<meta property="og:title" content="Triton Tutorial">
<meta property="og:url" content="https://anti-entrophic.github.io/posts/10042.html">
<meta property="og:site_name" content="不会魔法的小圆">
<meta property="og:description" content="都什么年代，还在用传统pytorch">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.072333.xyz/file/AgACAgEAAyEGAASMaMWHAAJ0kmgkMqLWOl2GofJ-fllk5LkIlR8pAALnrjEbdlsgRScFcviKiHNwAQADAgADeQADNgQ.jpg">
<meta property="article:published_time" content="2025-05-14T06:06:06.000Z">
<meta property="article:modified_time" content="2025-05-14T06:10:52.963Z">
<meta property="article:author" content="不会魔法的小圆">
<meta property="article:tag" content="CUDA">
<meta property="article:tag" content="triton">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.072333.xyz/file/AgACAgEAAyEGAASMaMWHAAJ0kmgkMqLWOl2GofJ-fllk5LkIlR8pAALnrjEbdlsgRScFcviKiHNwAQADAgADeQADNgQ.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://anti-entrophic.github.io/posts/10042.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Triton Tutorial',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-05-14 14:10:52'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="不会魔法的小圆" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/icon.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">20</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">12</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/index.html"><i class="fa-fw fas fa-heart"></i><span> 追番</span></a></div><div class="menus_item"><a class="site-page" href="/html/index.html"><i class="fa-fw fas fa-archive"></i><span> Tools</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.072333.xyz/file/AgACAgEAAyEGAASMaMWHAAJ0kmgkMqLWOl2GofJ-fllk5LkIlR8pAALnrjEbdlsgRScFcviKiHNwAQADAgADeQADNgQ.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="不会魔法的小圆"><span class="site-name">不会魔法的小圆</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/index.html"><i class="fa-fw fas fa-heart"></i><span> 追番</span></a></div><div class="menus_item"><a class="site-page" href="/html/index.html"><i class="fa-fw fas fa-archive"></i><span> Tools</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Triton Tutorial</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-05-14T06:06:06.000Z" title="发表于 2025-05-14 14:06:06">2025-05-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-05-14T06:10:52.963Z" title="更新于 2025-05-14 14:10:52">2025-05-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/cuda/">cuda</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Triton Tutorial"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h1><p>Triton是一门适配python的高性能GPU编程语言（暂时只认为是语言），学习路线可以从完成官方的<a target="_blank" rel="noopener" href="https://triton-lang.org/main/index.html," title="Triton Tutorial">tutorial</a>开始。我的博客里主要想讲一些不一样的。</p>
<p>CUDA Version: 12.2</p>
<p>Triton Version: 3.1.0</p>
<h1 id="GPU相关知识"><a href="#GPU相关知识" class="headerlink" title="GPU相关知识"></a>GPU相关知识</h1><p>想必大家上来跑tutorial遇到的第一个问题是，获取DEVICE的接口报错了！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> triton.runtime <span class="keyword">import</span> driver</span><br><span class="line">DEVICE = driver.active.get_active_torch_device()</span><br><span class="line"></span><br><span class="line"><span class="comment"># &gt;&gt;&gt; AttributeError: &#x27;CudaDriver&#x27; object has no attribute &#x27;get_active_torch_device&#x27;</span></span><br></pre></td></tr></table></figure>
<p>查阅源码后发现，应该是nvidia那边的接口变掉了，导致triton中无法重载：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># triton/python/triton/backends/driver.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DriverBase</span>(metaclass=ABCMeta):</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">is_active</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_current_target</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_active_torch_device</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_benchmarker</span>(<span class="params">self</span>) -&gt; Benchmarker:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Return the benchmarking function that this backend should use by default.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">driver.active</span><br><span class="line"><span class="comment"># &gt;&gt;&gt; &lt;nvi.CudaDriver object at 0x7ff294e43d30&gt;</span></span><br></pre></td></tr></table></figure>
<p>因此，我们可以用别的API来代替：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">driver.active.get_current_target()</span><br><span class="line"><span class="comment"># &gt;&gt;&gt; GPUTarget(backend=&#x27;cuda&#x27;, arch=90, warp_size=32)</span></span><br><span class="line">DEVICE = driver.active.get_current_target().backend</span><br><span class="line"><span class="comment"># &gt;&gt;&gt; &#x27;cuda&#x27;</span></span><br><span class="line">DEVICE_ID = driver.active.get_current_device()</span><br><span class="line"><span class="comment"># &gt;&gt;&gt; 0</span></span><br></pre></td></tr></table></figure>
<p>在有些时候，我们还需要更多GPU的信息来辅助并行编程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">properties = driver.active.utils.get_device_properties(DEVICE_ID)</span><br><span class="line"><span class="comment"># &gt;&gt;&gt; &#123;&#x27;max_shared_mem&#x27;: 232448, &#x27;max_num_regs&#x27;: 65536, &#x27;multiprocessor_count&#x27;: 132, &#x27;warpSize&#x27;: 32, &#x27;sm_clock_rate&#x27;: 1980000, &#x27;mem_clock_rate&#x27;: 2619000, &#x27;mem_bus_width&#x27;: 5120&#125;</span></span><br><span class="line">NUM_SM = properties[<span class="string">&quot;multiprocessor_count&quot;</span>]</span><br><span class="line">NUM_REGS = properties[<span class="string">&quot;max_num_regs&quot;</span>]</span><br><span class="line">SIZE_SMEM = properties[<span class="string">&quot;max_shared_mem&quot;</span>]</span><br><span class="line">WARP_SIZE = properties[<span class="string">&quot;warpSize&quot;</span>]</span><br></pre></td></tr></table></figure>
<p>解释一下这四个主要的GPU参数：</p>
<ul>
<li><p><code>NUM_SM</code> 是指的GPU中Streaming Multiprocessor（SM）的数量。SM是GPU上的核心处理单元，包含完整的内存、寄存器等。整个CUDA编程的核心就是将任务分成多个BLOCKs，然后这些BLOCKs会被均匀地分给所有SM执行。我使用的GPU型号是H800，总共有132个SM。</p>
</li>
<li><p><code>NUM_REGS</code> 是指每个SM中可用的寄存器（registers）的最大数量。寄存器每个线程不共享，如果单个线程所需要使用的寄存器很多，则同时在一个SM上运行的线程数量就会减少。</p>
</li>
<li><p><code>SIZE_SMEM</code> 是指每个SM可用的共享内存（Shared Memory）的大小，通常以字节（Bytes）为单位。SMEM访问速度远快于显存，同一个SM中的所有线程均可共享，可用于数据交换等。linux系统中也有共享内存的概念，可以看作是“基于内存的文件系统”，通常指的是 <code>/dev/shm</code> 这块区域，用于进程间通信等操作，就不用把数据写到硬盘里去了。</p>
</li>
</ul>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df -h /dev/shm</span><br><span class="line">&gt;&gt;&gt; 文件系统        大小  已用  可用 已用% 挂载点</span><br><span class="line">&gt;&gt;&gt; tmpfs            <span class="number">64</span>G     <span class="number">0</span>   <span class="number">64</span>G    <span class="number">0</span>% /dev/shm</span><br></pre></td></tr></table></figure>
<ul>
<li><code>WARP_SIZE</code> 比较复杂一点，我们首先需要理解Warp的概念。Warp是GPU上线程调度的基本单元，一个Warp中的所有线程会执行相同的命令。这并不代表一个Warp中所有线程是完全一样的，而是说，如果Warp中有一半的指令做的是A，而另一半的指令做的是A-&gt;B，则在第一阶段所有线程会同时处理，而在第二阶段有一半的线程会陪着另一半空转。因此，避免Warp分歧也是一个很重要的优化点。<code>WARP_SIZE</code> 则表示一个Warp中包含的线程数量，基本上是32。</li>
</ul>
<h1 id="Vector-Addition"><a href="#Vector-Addition" class="headerlink" title="Vector Addition"></a>Vector Addition</h1><p>源代码很简单就不解释了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> triton</span><br><span class="line"><span class="keyword">import</span> triton.language <span class="keyword">as</span> tl</span><br><span class="line"><span class="keyword">from</span> triton.runtime <span class="keyword">import</span> autotune</span><br><span class="line"></span><br><span class="line">DEVICE = triton.runtime.driver.active.get_current_target().backend</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@triton.jit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_kernel</span>(<span class="params"></span></span><br><span class="line"><span class="params">    x_ptr,</span></span><br><span class="line"><span class="params">    y_ptr,</span></span><br><span class="line"><span class="params">    output_ptr,</span></span><br><span class="line"><span class="params">    n_elements,</span></span><br><span class="line"><span class="params">    BLOCK_SIZE: tl.constexpr</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    pid = tl.program_id(axis=<span class="number">0</span>)</span><br><span class="line">    block_start = pid * BLOCK_SIZE</span><br><span class="line">    offsets = block_start + tl.arange(<span class="number">0</span>, BLOCK_SIZE)</span><br><span class="line">    mask = offsets &lt; n_elements</span><br><span class="line">    x = tl.load(x_ptr + offsets, mask=mask)</span><br><span class="line">    y = tl.load(y_ptr + offsets, mask=mask)</span><br><span class="line">    output = x + y</span><br><span class="line">    tl.store(output_ptr + offsets, output, mask=mask)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@autotune(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="meta">    configs=[</span></span></span><br><span class="line"><span class="params"><span class="meta">        triton.Config(<span class="params">&#123;<span class="string">&#x27;BLOCK_SIZE&#x27;</span>: <span class="number">256</span>&#125;, num_warps=<span class="number">4</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        triton.Config(<span class="params">&#123;<span class="string">&#x27;BLOCK_SIZE&#x27;</span>: <span class="number">512</span>&#125;, num_warps=<span class="number">4</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        triton.Config(<span class="params">&#123;<span class="string">&#x27;BLOCK_SIZE&#x27;</span>: <span class="number">512</span>&#125;, num_warps=<span class="number">8</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    ],</span></span></span><br><span class="line"><span class="params"><span class="meta">    key=[<span class="string">&#x27;n_elements&#x27;</span>],</span></span></span><br><span class="line"><span class="params"><span class="meta"></span>)</span></span><br><span class="line"><span class="meta">@triton.jit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_kernel_autotune</span>(<span class="params"></span></span><br><span class="line"><span class="params">    x_ptr,</span></span><br><span class="line"><span class="params">    y_ptr,</span></span><br><span class="line"><span class="params">    output_ptr,</span></span><br><span class="line"><span class="params">    n_elements,</span></span><br><span class="line"><span class="params">    BLOCK_SIZE: tl.constexpr</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    pid = tl.program_id(axis=<span class="number">0</span>)</span><br><span class="line">    block_start = pid * BLOCK_SIZE</span><br><span class="line">    offsets = block_start + tl.arange(<span class="number">0</span>, BLOCK_SIZE)</span><br><span class="line">    mask = offsets &lt; n_elements</span><br><span class="line">    x = tl.load(x_ptr + offsets, mask=mask)</span><br><span class="line">    y = tl.load(y_ptr + offsets, mask=mask)</span><br><span class="line">    output = x + y</span><br><span class="line">    tl.store(output_ptr + offsets, output, mask=mask)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add</span>(<span class="params"></span></span><br><span class="line"><span class="params">    x: torch.Tensor,</span></span><br><span class="line"><span class="params">    y: torch.Tensor,</span></span><br><span class="line"><span class="params">    block_size: <span class="built_in">int</span> = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    num_warps: <span class="built_in">int</span> = <span class="number">4</span>,</span></span><br><span class="line"><span class="params">    autotune: <span class="built_in">bool</span> = <span class="literal">False</span></span></span><br><span class="line"><span class="params"></span>) -&gt; torch.Tensor:</span><br><span class="line">    output = torch.empty_like(x)</span><br><span class="line">    <span class="keyword">assert</span> x.device.<span class="built_in">type</span> == DEVICE <span class="keyword">and</span> y.device.<span class="built_in">type</span> == DEVICE <span class="keyword">and</span> output.device.<span class="built_in">type</span> == DEVICE</span><br><span class="line">    n_elements = output.numel()</span><br><span class="line"></span><br><span class="line">    grid = <span class="keyword">lambda</span> meta: (triton.cdiv(n_elements, meta[<span class="string">&#x27;BLOCK_SIZE&#x27;</span>]), )   <span class="comment"># noqa: E731</span></span><br><span class="line">    <span class="keyword">if</span> autotune:</span><br><span class="line">        add_kernel_autotune[grid](x, y, output, n_elements)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=block_size, num_warps=num_warps)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    torch.manual_seed(<span class="number">0</span>)</span><br><span class="line">    size = <span class="number">98432</span>  </span><br><span class="line">    x = torch.randn(size, device=DEVICE)</span><br><span class="line">    y = torch.randn(size, device=DEVICE)</span><br><span class="line"></span><br><span class="line">    output_triton = add(x, y, block_size=<span class="number">256</span>)</span><br><span class="line">    output_pytorch = x + y</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(output_triton)</span><br><span class="line">    <span class="built_in">print</span>(output_pytorch)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;torch.<span class="built_in">max</span>(torch.<span class="built_in">abs</span>(output_triton - output_pytorch))&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@triton.testing.perf_report(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="meta">    triton.testing.Benchmark(<span class="params"></span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">        x_names=[<span class="string">&#x27;size&#x27;</span>],</span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">        x_vals=[<span class="number">2</span>**i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="params"><span class="number">12</span>, <span class="number">28</span>, <span class="number">1</span></span>)],</span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">        x_log=<span class="literal">True</span>,</span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">        line_arg=<span class="string">&#x27;provider&#x27;</span>,</span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">        line_vals=[<span class="string">&#x27;torch&#x27;</span>, <span class="string">&#x27;triton_bs128&#x27;</span>, <span class="string">&#x27;triton_bs256&#x27;</span>, <span class="string">&#x27;triton_bs512&#x27;</span>, <span class="string">&#x27;triton_nw4&#x27;</span>, <span class="string">&#x27;triton_nw8&#x27;</span>, <span class="string">&#x27;triton_nw16&#x27;</span>, <span class="string">&#x27;triton_autotune&#x27;</span>],</span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">        line_names=[<span class="string">&#x27;Torch&#x27;</span>, <span class="string">&#x27;Triton BS=128&#x27;</span>, <span class="string">&#x27;Triton BS=256&#x27;</span>, <span class="string">&#x27;Triton BS=512&#x27;</span>, <span class="string">&#x27;Triton NW=4&#x27;</span>, <span class="string">&#x27;Triton NW=8&#x27;</span>, <span class="string">&#x27;Triton NW=16&#x27;</span>, <span class="string">&#x27;Triton Autotune&#x27;</span>],</span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">        styles=[(<span class="params"><span class="string">&#x27;green&#x27;</span>, <span class="string">&#x27;-&#x27;</span></span>), (<span class="params"><span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;-&#x27;</span></span>), (<span class="params"><span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;-&#x27;</span></span>), (<span class="params"><span class="string">&#x27;purple&#x27;</span>, <span class="string">&#x27;-&#x27;</span></span>), (<span class="params"><span class="string">&#x27;orange&#x27;</span>, <span class="string">&#x27;-&#x27;</span></span>), (<span class="params"><span class="string">&#x27;cyan&#x27;</span>, <span class="string">&#x27;-&#x27;</span></span>), (<span class="params"><span class="string">&#x27;magenta&#x27;</span>, <span class="string">&#x27;-&#x27;</span></span>), (<span class="params"><span class="string">&#x27;yellow&#x27;</span>, <span class="string">&#x27;-&#x27;</span></span>)],</span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">        ylabel=<span class="string">&#x27;GB/s&#x27;</span>,</span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">        plot_name=<span class="string">&#x27;vector-add-performance&#x27;</span>,</span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">        args=&#123;&#125;,</span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">    </span>)</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">benchmark</span>(<span class="params">size, provider</span>):</span><br><span class="line">    x = torch.rand(size, device=DEVICE, dtype=torch.float32)</span><br><span class="line">    y = torch.rand(size, device=DEVICE, dtype=torch.float32)</span><br><span class="line">    quantiles = [<span class="number">0.5</span>, <span class="number">0.2</span>, <span class="number">0.8</span>]</span><br><span class="line">    <span class="keyword">if</span> provider == <span class="string">&#x27;torch&#x27;</span>:</span><br><span class="line">        ms, min_ms, max_ms = triton.testing.do_bench(<span class="keyword">lambda</span>: x + y, quantiles=quantiles)</span><br><span class="line">    <span class="keyword">elif</span> provider == <span class="string">&#x27;triton_bs128&#x27;</span>:</span><br><span class="line">        ms, min_ms, max_ms = triton.testing.do_bench(<span class="keyword">lambda</span>: add(x, y, block_size=<span class="number">128</span>), quantiles=quantiles)</span><br><span class="line">    <span class="keyword">elif</span> provider == <span class="string">&#x27;triton_bs256&#x27;</span>:</span><br><span class="line">        ms, min_ms, max_ms = triton.testing.do_bench(<span class="keyword">lambda</span>: add(x, y, block_size=<span class="number">256</span>), quantiles=quantiles)</span><br><span class="line">    <span class="keyword">elif</span> provider == <span class="string">&#x27;triton_bs512&#x27;</span>:</span><br><span class="line">        ms, min_ms, max_ms = triton.testing.do_bench(<span class="keyword">lambda</span>: add(x, y, block_size=<span class="number">512</span>), quantiles=quantiles)</span><br><span class="line">    <span class="keyword">elif</span> provider == <span class="string">&#x27;triton_nw4&#x27;</span>:</span><br><span class="line">        ms, min_ms, max_ms = triton.testing.do_bench(<span class="keyword">lambda</span>: add(x, y, block_size=<span class="number">512</span>, num_warps=<span class="number">4</span>), quantiles=quantiles)</span><br><span class="line">    <span class="keyword">elif</span> provider == <span class="string">&#x27;triton_nw8&#x27;</span>:</span><br><span class="line">        ms, min_ms, max_ms = triton.testing.do_bench(<span class="keyword">lambda</span>: add(x, y, block_size=<span class="number">512</span>, num_warps=<span class="number">8</span>), quantiles=quantiles)</span><br><span class="line">    <span class="keyword">elif</span> provider == <span class="string">&#x27;triton_nw16&#x27;</span>:</span><br><span class="line">        ms, min_ms, max_ms = triton.testing.do_bench(<span class="keyword">lambda</span>: add(x, y, block_size=<span class="number">512</span>, num_warps=<span class="number">16</span>), quantiles=quantiles)</span><br><span class="line">    <span class="keyword">elif</span> provider == <span class="string">&#x27;triton_autotune&#x27;</span>:</span><br><span class="line">        ms, min_ms, max_ms = triton.testing.do_bench(<span class="keyword">lambda</span>: add(x, y, autotune=<span class="literal">True</span>), quantiles=quantiles)</span><br><span class="line">    gbps = <span class="keyword">lambda</span> ms: <span class="number">3</span> * x.numel() * x.element_size() * <span class="number">1e-9</span> / (ms * <span class="number">1e-3</span>)   <span class="comment"># noqa: E731</span></span><br><span class="line">    <span class="keyword">return</span> gbps(ms), gbps(max_ms), gbps(min_ms)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># main()</span></span><br><span class="line">    benchmark.run(print_data=<span class="literal">True</span>, show_plots=<span class="literal">True</span>, save_path=<span class="string">&quot;./results/01_vector_addition&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="性能调优"><a href="#性能调优" class="headerlink" title="性能调优"></a>性能调优</h2><p>比起源码，我额外增加了 <code>from triton.runtime import autotune</code> ，它的作用就是对于不同size的输入，会在首次执行时搜一遍所有可能的配置，找到其中效率最高的，后续对同样的size就会用固定的配置。</p>
<p>简单尝试一下的话，就会发现影响程序性能的因素有两个 <code>BLOCK_SIZE</code> 与 <code>num_warps</code>。其实理论上应该是 <code>BLOCK_SIZE</code> 、 <code>num_warps</code> 和 <code>input_size</code> 三者的关系决定了性能。我们用控制变量的方式来测一下它们的影响。</p>
<center>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.072333.xyz/file/AgACAgEAAyEGAASMaMWHAAJ0iWgkKUDeOQt5_bzZKZovwbS5PBBHAALYrjEbdlsgRTJ8-RKWuAOmAQADAgADeAADNgQ.png" width="1000px" />

<p style="font-size: 10px;">Vector Addition Triton Kernel Performance。测BS时默认NW为4；测NW时默认BS为512</p>

</center>

<p><br></p>
<p>从图中我们能看出两个明显掉点的曲线，一个是 <code>BLOCK_SIZE</code> 为128时，此时的 <code>BLOCK_SIZE</code> 太小，分成的warps太多，导致管理调度 $2^{27} / 128 / 32$ 个块成了开销瓶颈；另一个是 <code>num_warps</code> 为16时，过大的线程块导致了对寄存器等资源的竞争更加剧烈，甚至可能发生寄存器溢出，显著影响效率。其实我也还不太会分析具体的原因，下面是详细的测试表格：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>size</th>
<th>Torch</th>
<th>Triton BS=128</th>
<th>Triton BS=256</th>
<th>Triton BS=512</th>
<th>Triton NW=4</th>
<th>Triton NW=8</th>
<th>Triton NW=16</th>
<th>Triton Autotune</th>
</tr>
</thead>
<tbody>
<tr>
<td>4096.000000</td>
<td>9.035294</td>
<td>9.197604</td>
<td>9.142857</td>
<td>9.142857</td>
<td>9.142857</td>
<td>9.142857</td>
<td>9.088757</td>
<td>8.982456</td>
</tr>
<tr>
<td>8192.000000</td>
<td>17.964912</td>
<td>18.070588</td>
<td>17.860465</td>
<td>17.757226</td>
<td>18.070588</td>
<td>17.757226</td>
<td>18.070588</td>
<td>17.757226</td>
</tr>
<tr>
<td>16384.000000</td>
<td>35.514452</td>
<td>35.310345</td>
<td>35.310345</td>
<td>35.310345</td>
<td>35.310345</td>
<td>35.310345</td>
<td>35.720930</td>
<td>35.310345</td>
</tr>
<tr>
<td>32768.000000</td>
<td>68.266666</td>
<td>68.266666</td>
<td>69.423731</td>
<td>69.033707</td>
<td>69.033707</td>
<td>70.217145</td>
<td>69.818181</td>
<td>69.818181</td>
</tr>
<tr>
<td>65536.000000</td>
<td>132.843245</td>
<td>135.779009</td>
<td>135.779009</td>
<td>133.565214</td>
<td>133.565214</td>
<td>133.565214</td>
<td>135.032965</td>
<td>136.533331</td>
</tr>
<tr>
<td>131072.000000</td>
<td>253.360834</td>
<td>252.061538</td>
<td>255.999991</td>
<td>258.694729</td>
<td>260.063494</td>
<td>252.061538</td>
<td>253.360834</td>
<td>258.694729</td>
</tr>
<tr>
<td>262144.000000</td>
<td>457.227922</td>
<td>444.814490</td>
<td>455.111110</td>
<td>465.895721</td>
<td>463.698115</td>
<td>461.521112</td>
<td>453.013839</td>
<td>465.895721</td>
</tr>
<tr>
<td>524288.000000</td>
<td>750.412251</td>
<td>747.558951</td>
<td>774.047204</td>
<td>771.011790</td>
<td>771.011790</td>
<td>765.011652</td>
<td>741.916954</td>
<td>777.106702</td>
</tr>
<tr>
<td>1048576.000000</td>
<td>1228.800031</td>
<td>1159.929234</td>
<td>1187.963788</td>
<td>1221.167675</td>
<td>1228.800031</td>
<td>1184.385557</td>
<td>1156.517652</td>
<td>1217.387051</td>
</tr>
<tr>
<td>2097152.000000</td>
<td>1687.622326</td>
<td>1569.724635</td>
<td>1676.827323</td>
<td>1698.557221</td>
<td>1687.622326</td>
<td>1684.008546</td>
<td>1569.724635</td>
<td>1694.896509</td>
</tr>
<tr>
<td>4194304.000000</td>
<td>2154.608134</td>
<td>1927.529447</td>
<td>2148.721353</td>
<td>2160.527432</td>
<td>2163.499294</td>
<td>2145.789924</td>
<td>1852.607766</td>
<td>2163.499294</td>
</tr>
<tr>
<td>8388608.000000</td>
<td>2532.792214</td>
<td>2204.434417</td>
<td>2538.924930</td>
<td>2534.833158</td>
<td>2532.792214</td>
<td>2543.029933</td>
<td>2160.527432</td>
<td>2545.087416</td>
</tr>
<tr>
<td>16777216.000000</td>
<td>2755.784589</td>
<td>2349.311512</td>
<td>2763.045981</td>
<td>2756.388411</td>
<td>2755.784589</td>
<td>2763.045981</td>
<td>2394.920460</td>
<td>2758.200902</td>
</tr>
<tr>
<td>33554432.000000</td>
<td>2941.994716</td>
<td>2428.196121</td>
<td>2949.580950</td>
<td>2944.059933</td>
<td>2942.682906</td>
<td>2950.618366</td>
<td>2558.022378</td>
<td>2949.580950</td>
</tr>
<tr>
<td>67108864.000000</td>
<td>3021.832823</td>
<td>2487.232999</td>
<td>3032.757656</td>
<td>3021.832823</td>
<td>3022.740106</td>
<td>3032.392472</td>
<td>2644.858132</td>
<td>3032.027035</td>
</tr>
<tr>
<td>134217728.000000</td>
<td>3067.880595</td>
<td>2514.570784</td>
<td>3071.437476</td>
<td>3066.945668</td>
<td>3067.506556</td>
<td>3072.937670</td>
<td>2676.788108</td>
<td>3072.562746</td>
</tr>
</tbody>
</table>
</div>
<div class="note warning flat"><p>如果采用 <code>BS=4096，NW=16</code> 的配置的话，性能并不会显著下降，看起来似乎不大符合原来“寄存器溢出导致性能下降”的假设</p>
<p>gemini的回答是：</p>
<ul>
<li><p>当 Kernel 需要处理一个包含4096个元素的向量 (如 offsets = tl.arange(0, BLOCK_SIZE)) 时，编译器清楚地知道，不可能一次性将所有4096个元素（对于每个线程来说，是它负责的那部分）都完整地、同时地放在寄存器中进行操作。这远超出了单个线程或 Warp 的实际寄存器容量。因此，编译器很可能会生成一种高度流水线化 (pipelined) 或分块 (tiled/chunked) 的执行代码。它会将这4096个元素的操作分解成更小的、可管理的批次。例如，加载一小批数据到寄存器，计算，存储结果，然后再处理下一小批。<br>这种流水线化的处理方式，其瞬时 (instantaneous) 寄存器需求可能相对较低。也就是说，在任何特定时刻，每个线程为正在处理的小数据块所活跃使用的寄存器数量可能不多。</p>
</li>
<li><p>编译器处理512个元素时，它可能尝试一种不同的优化策略。也许它认为可以将更大部分的“向量”同时保持活跃在寄存器中，或者采用一种不那么积极分解成小块的策略，因为它认为这在某些情况下（如有足够寄存器时）可能更有效。<br>这种策略如果本身对寄存器的需求就比较高，那么当 num_warps 强制一个较紧的寄存器预算时，就更容易导致寄存器溢出。</p>
</li>
</ul>
<p>暂时不知道对错，感觉我还需要一些性能调优工具。</p>
</div>
<p>我们可以发现一个有趣的现象，即固定 <code>num_warps</code> 时，<code>BLOCK_SIZE</code> 设为256是最佳的；固定 <code>BLOCK_SIZE</code> 时，<code>num_warps</code> 设为8是最佳的。</p>
<p>由于性能受到多种因素的影响，因此影响程序效率的参数往往存在着被称为 <strong>sweet spot（甜点区</strong> 的最佳范围，高了也不行，低了也不行。</p>
<p>一个比较好的方法是采用autotune，试几种可选参数后丢进去。这样程序会在初次运行时自动测试几种配置，后续对于同样的size均采用最佳配置即可。</p>
<p>值得一提的是，在计算非常简单的 Vector Addition 的场景下，性能瓶颈不在计算而在于带宽，因此最佳性能差距不会太大。</p>
<h1 id="Fused-Softmax"><a href="#Fused-Softmax" class="headerlink" title="Fused Softmax"></a>Fused Softmax</h1><p>遇到的一个神人问题是，源代码中的 <code>kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)</code> 报错</p>
<p>报错原因是，代码中已经预先编译过内核了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel = softmax_kernel.warmup(y, x, x.stride(<span class="number">0</span>), y.stride(<span class="number">0</span>), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE, num_stages=num_stages, num_warps=num_warps, grid=(<span class="number">1</span>, ))</span><br></pre></td></tr></table></figure>
<p>而在 <code>softmax_kernel</code> 的原始定义中，<code>BLOCK_SIZE</code> 和 <code>num_stages</code> 被声明为了 <code>tl.constexpr</code> 类型，表示编译时常量，即在内核中编译的过程中已经按照这个常量编译了，不应该在运行时再次传入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@triton.jit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax_kernel</span>(<span class="params">output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr, num_stages: tl.constexpr</span>):</span><br></pre></td></tr></table></figure>
<p>因此，源代码应该修改为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel[(num_programs, <span class="number">1</span>, <span class="number">1</span>)](y, x, x.stride(<span class="number">0</span>), y.stride(<span class="number">0</span>), n_rows, n_cols)</span><br></pre></td></tr></table></figure>
<p>最后，完整的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> triton</span><br><span class="line"><span class="keyword">import</span> triton.language <span class="keyword">as</span> tl</span><br><span class="line"><span class="keyword">from</span> triton.runtime <span class="keyword">import</span> driver</span><br><span class="line"></span><br><span class="line">target = driver.active.get_current_target()</span><br><span class="line">DEVICE = target.backend</span><br><span class="line">DEVICE_ID = driver.active.get_current_device()</span><br><span class="line"></span><br><span class="line">properties = driver.active.utils.get_device_properties(DEVICE_ID)</span><br><span class="line">NUM_SM = properties[<span class="string">&quot;multiprocessor_count&quot;</span>]</span><br><span class="line">NUM_REGS = properties[<span class="string">&quot;max_num_regs&quot;</span>]</span><br><span class="line">SIZE_SMEM = properties[<span class="string">&quot;max_shared_mem&quot;</span>]</span><br><span class="line">WARP_SIZE = properties[<span class="string">&quot;warpSize&quot;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">naive_softmax</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="comment"># read MN elements, write M elements</span></span><br><span class="line">    x_max = x.<span class="built_in">max</span>(dim=<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># read MN+M elements, write MN elements</span></span><br><span class="line">    z = x - x_max[:, <span class="literal">None</span>]</span><br><span class="line">    <span class="comment"># read MN elements, write MN elements</span></span><br><span class="line">    numerator = torch.exp(z)</span><br><span class="line">    <span class="comment"># read MN elements, write M elements</span></span><br><span class="line">    denominator = numerator.<span class="built_in">sum</span>(dim=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># read MN + M elements, write MN elements</span></span><br><span class="line">    ret = numerator / denominator[:, <span class="literal">None</span>]</span><br><span class="line">    <span class="comment"># in total: read 5MN + 2M elements ; wrote 3MN + 2M elements</span></span><br><span class="line">    <span class="keyword">return</span> ret</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@triton.jit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax_kernel</span>(<span class="params">output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr, num_stages: tl.constexpr</span>):</span><br><span class="line">    row_start = tl.program_id(<span class="number">0</span>)</span><br><span class="line">    row_step = tl.num_programs(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> row_idx <span class="keyword">in</span> tl.<span class="built_in">range</span>(row_start, n_rows, row_step, num_stages=num_stages):</span><br><span class="line">        row_start_ptr = input_ptr + row_idx * input_row_stride</span><br><span class="line">        col_offsets = tl.arange(<span class="number">0</span>, BLOCK_SIZE)</span><br><span class="line"></span><br><span class="line">        input_ptrs = row_start_ptr + col_offsets</span><br><span class="line">        mask = col_offsets &lt; n_cols</span><br><span class="line">        row = tl.load(input_ptrs, mask=mask, other=-<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>))</span><br><span class="line"></span><br><span class="line">        row_minus_max = row - tl.<span class="built_in">max</span>(row, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        numerator = tl.exp(row_minus_max)</span><br><span class="line">        denominator = tl.<span class="built_in">sum</span>(numerator, axis=<span class="number">0</span>)</span><br><span class="line">        softmax_output = numerator / denominator</span><br><span class="line"></span><br><span class="line">        output_row_start_ptr = output_ptr + row_idx * output_row_stride</span><br><span class="line">        output_ptrs = output_row_start_ptr + col_offsets</span><br><span class="line">        tl.store(output_ptrs, softmax_output, mask=mask)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">x, num_stages_to_use</span>):</span><br><span class="line">    n_rows, n_cols = x.shape</span><br><span class="line">    BLOCK_SIZE = triton.next_power_of_2(n_cols)</span><br><span class="line"></span><br><span class="line">    num_warps = <span class="number">8</span></span><br><span class="line">    <span class="comment"># num_stages = 4 if SIZE_SMEM &gt; 200000 else 2</span></span><br><span class="line">    num_stages = num_stages_to_use</span><br><span class="line"></span><br><span class="line">    y = torch.empty_like(x)</span><br><span class="line">    kernel = softmax_kernel.warmup(y, x, x.stride(<span class="number">0</span>), y.stride(<span class="number">0</span>), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE, num_stages=num_stages, num_warps=num_warps, grid=(<span class="number">1</span>, ))</span><br><span class="line">    kernel._init_handles()</span><br><span class="line"></span><br><span class="line">    n_regs = kernel.n_regs</span><br><span class="line">    size_smem = kernel.metadata.shared</span><br><span class="line"></span><br><span class="line">    occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)</span><br><span class="line">    occupancy = <span class="built_in">min</span>(occupancy, SIZE_SMEM // size_smem)</span><br><span class="line">    num_programs = NUM_SM * occupancy</span><br><span class="line"></span><br><span class="line">    num_programs = <span class="built_in">min</span>(num_programs, n_rows)</span><br><span class="line"></span><br><span class="line">    kernel[(num_programs, <span class="number">1</span>, <span class="number">1</span>)](y, x, x.stride(<span class="number">0</span>), y.stride(<span class="number">0</span>), n_rows, n_cols)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    torch.manual_seed(<span class="number">0</span>)</span><br><span class="line">    x = torch.randn(<span class="number">1823</span>, <span class="number">781</span>, device=DEVICE)</span><br><span class="line">    y_triton = softmax(x, <span class="number">4</span>)</span><br><span class="line">    y_torch = torch.softmax(x, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.allclose(y_triton, y_torch), (y_triton, y_torch)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@triton.testing.perf_report(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="meta">    triton.testing.Benchmark(<span class="params"></span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">        x_names=[<span class="string">&#x27;N&#x27;</span>],  <span class="comment"># argument names to use as an x-axis for the plot</span></span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">        x_vals=[<span class="number">128</span> * i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="params"><span class="number">2</span>, <span class="number">129</span></span>)],  <span class="comment"># different possible values for `x_name`</span></span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">        line_arg=<span class="string">&#x27;provider&#x27;</span>,  <span class="comment"># argument name whose value corresponds to a different line in the plot</span></span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">        line_vals=[<span class="string">&#x27;torch&#x27;</span>, <span class="string">&#x27;triton_ns2&#x27;</span>, <span class="string">&#x27;triton_ns3&#x27;</span>, <span class="string">&#x27;triton_ns4&#x27;</span>],</span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">        line_names=[</span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">            <span class="string">&quot;Torch&quot;</span>,</span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">            <span class="string">&quot;Triton (NS=2)&quot;</span>,</span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">            <span class="string">&quot;Triton (NS=3)&quot;</span>,</span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">            <span class="string">&quot;Triton (NS=4)&quot;</span>,</span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">        ],</span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">        styles=[(<span class="params"><span class="string">&#x27;green&#x27;</span>, <span class="string">&#x27;-&#x27;</span></span>), (<span class="params"><span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;-&#x27;</span></span>), (<span class="params"><span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;-&#x27;</span></span>), (<span class="params"><span class="string">&#x27;purple&#x27;</span>, <span class="string">&#x27;-&#x27;</span></span>)],</span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">        ylabel=<span class="string">&quot;GB/s&quot;</span>,</span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">        plot_name=<span class="string">&quot;softmax-performance-vs-num_stages&quot;</span>,</span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">        args=&#123;<span class="string">&#x27;M&#x27;</span>: <span class="number">4096</span>&#125;,</span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">    </span>)</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">benchmark</span>(<span class="params">M, N, provider</span>):</span><br><span class="line">    x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)</span><br><span class="line">    stream = <span class="built_in">getattr</span>(torch, DEVICE).Stream()</span><br><span class="line">    <span class="built_in">getattr</span>(torch, DEVICE).set_stream(stream)</span><br><span class="line">    <span class="keyword">if</span> provider == <span class="string">&#x27;torch&#x27;</span>:</span><br><span class="line">        ms = triton.testing.do_bench(<span class="keyword">lambda</span>: torch.softmax(x, axis=-<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">elif</span> provider == <span class="string">&#x27;triton_ns2&#x27;</span>:</span><br><span class="line">        ms = triton.testing.do_bench(<span class="keyword">lambda</span>: softmax(x, num_stages_to_use=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">elif</span> provider == <span class="string">&#x27;triton_ns3&#x27;</span>:</span><br><span class="line">        ms = triton.testing.do_bench(<span class="keyword">lambda</span>: softmax(x, num_stages_to_use=<span class="number">3</span>))</span><br><span class="line">    <span class="keyword">elif</span> provider == <span class="string">&#x27;triton_ns4&#x27;</span>:</span><br><span class="line">        ms = triton.testing.do_bench(<span class="keyword">lambda</span>: softmax(x, num_stages_to_use=<span class="number">4</span>))</span><br><span class="line">    gbps = <span class="keyword">lambda</span> ms: <span class="number">2</span> * x.numel() * x.element_size() * <span class="number">1e-9</span> / (ms * <span class="number">1e-3</span>)  <span class="comment"># noqa: E731</span></span><br><span class="line">    <span class="keyword">return</span> gbps(ms)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># main()</span></span><br><span class="line">    benchmark.run(show_plots=<span class="literal">True</span>, print_data=<span class="literal">True</span>, save_path=<span class="string">&quot;./results/02_fused_softmax&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="代码说明"><a href="#代码说明" class="headerlink" title="代码说明"></a>代码说明</h2><p>这个kernel比 <code>Vector Addition</code> 要稍微复杂一点，因为它涉及一个伪2D的并行。</p>
<p>说是伪2D是因为，它的 <code>BLOCK_SIZE</code> 取值是 <code>triton.next_power_of_2(n_cols)</code>，因此，每行不需要再单独划分。</p>
<p>主要的循环 <code>for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):</code> 的功能是：</p>
<ul>
<li><p>如果总共有 P 个程序并行，当前程序为 i，则取出第 $[i, P+i, 2P+i, \cdots]$ 行处理</p>
</li>
<li><p>后续每行取 <code>BLOCK_SIZE</code> 其实就已经取完了，并没有在行上并行</p>
</li>
</ul>
<p>对于 <code>@triton.jit</code> 装饰的函数，其中的中间变量，会放在寄存器或者共享内存中，直到 <code>tl.store()</code> 才会写回（如果占太多了也有可能自动offload到显存）。共享内存一般是SRAM，硬件特性决定了比一般的显存（HBM）要快很多。</p>
<p><code>num_stages</code> 是一个流水线处理的参数，表示并行程度。如果 <code>num_stages&gt;1</code>，则在程序处理第一条数据时，会同时开始加载第二条数据，这样可以加速，但是对SRAM的大小又有了要求。</p>
<p>因此有 <code>num_stages = 4 if SIZE_SMEM &gt; 200000 else 2</code></p>
<h2 id="benchmark"><a href="#benchmark" class="headerlink" title="benchmark"></a>benchmark</h2><p>从图中可以看出，还是要快不少的，不同 <code>num_stages</code> 之间差距不大。</p>
<center>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.072333.xyz/file/AgACAgEAAyEGAASMaMWHAAJ0j2gkMOUL6w_k6baDcCVISyal1o4NAALjrjEbdlsgRaE4ZMJECk62AQADAgADeAADNgQ.png" width="1000px" />

<p style="font-size: 10px;">Fused Softmax Triton Kernel Performance</p>

</center>

<p><br></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://anti-entrophic.github.io">不会魔法的小圆</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://anti-entrophic.github.io/posts/10042.html">https://anti-entrophic.github.io/posts/10042.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://anti-entrophic.github.io" target="_blank">不会魔法的小圆</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/CUDA/">CUDA</a><a class="post-meta__tags" href="/tags/triton/">triton</a></div><div class="post_share"><div class="social-share" data-image="https://i.072333.xyz/file/AgACAgEAAyEGAASMaMWHAAJ0kmgkMqLWOl2GofJ-fllk5LkIlR8pAALnrjEbdlsgRScFcviKiHNwAQADAgADeQADNgQ.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/posts/10040.html" title="Part III of Mathematical Structure of Mamba - S4D"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.072333.xyz/file/AgACAgEAAyEGAASMaMWHAAJjeGgM5CzgO8uqzR6RZuom-6BcNKaRAAKXsDEbNu1pRK8DbWzKH4FKAQADAgADeAADNgQ.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Part III of Mathematical Structure of Mamba - S4D</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/icon.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">不会魔法的小圆</div><div class="author-info__description">不会魔法的小圆，博客</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">20</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">12</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Anti-Entrophic"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">图床爆炸了 会马上修的呜呜</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%AA%E8%AE%BA"><span class="toc-number">1.</span> <span class="toc-text">绪论</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#GPU%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86"><span class="toc-number">2.</span> <span class="toc-text">GPU相关知识</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Vector-Addition"><span class="toc-number">3.</span> <span class="toc-text">Vector Addition</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98"><span class="toc-number">3.1.</span> <span class="toc-text">性能调优</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Fused-Softmax"><span class="toc-number">4.</span> <span class="toc-text">Fused Softmax</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E8%AF%B4%E6%98%8E"><span class="toc-number">4.1.</span> <span class="toc-text">代码说明</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#benchmark"><span class="toc-number">4.2.</span> <span class="toc-text">benchmark</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/10042.html" title="Triton Tutorial"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.072333.xyz/file/AgACAgEAAyEGAASMaMWHAAJ0kmgkMqLWOl2GofJ-fllk5LkIlR8pAALnrjEbdlsgRScFcviKiHNwAQADAgADeQADNgQ.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Triton Tutorial"/></a><div class="content"><a class="title" href="/posts/10042.html" title="Triton Tutorial">Triton Tutorial</a><time datetime="2025-05-14T06:06:06.000Z" title="发表于 2025-05-14 14:06:06">2025-05-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/10040.html" title="Part III of Mathematical Structure of Mamba - S4D"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.072333.xyz/file/AgACAgEAAyEGAASMaMWHAAJjeGgM5CzgO8uqzR6RZuom-6BcNKaRAAKXsDEbNu1pRK8DbWzKH4FKAQADAgADeAADNgQ.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Part III of Mathematical Structure of Mamba - S4D"/></a><div class="content"><a class="title" href="/posts/10040.html" title="Part III of Mathematical Structure of Mamba - S4D">Part III of Mathematical Structure of Mamba - S4D</a><time datetime="2025-04-25T08:28:02.000Z" title="发表于 2025-04-25 16:28:02">2025-04-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/10035.html" title="Sparsemax"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.072333.xyz/file/AgACAgEAAyEGAASMaMWHAAJTTmf1Ek0-hwAB6K-MTYKhFAS65kiTkgACb7ExG8W0qEcgM-UKzdhEWQEAAwIAA3gAAzYE.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Sparsemax"/></a><div class="content"><a class="title" href="/posts/10035.html" title="Sparsemax">Sparsemax</a><time datetime="2025-04-11T06:20:42.000Z" title="发表于 2025-04-11 14:20:42">2025-04-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/10039.html" title="Part II of Mathematical Structure of Mamba - S4"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.072333.xyz/file/AgACAgEAAyEGAASMaMWHAAJjSWgMzguLW7n4NF7aOmF9xf3dDw4aAAJgsDEbNu1pRH2bEOtt7JOdAQADAgADdwADNgQ.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Part II of Mathematical Structure of Mamba - S4"/></a><div class="content"><a class="title" href="/posts/10039.html" title="Part II of Mathematical Structure of Mamba - S4">Part II of Mathematical Structure of Mamba - S4</a><time datetime="2025-03-23T08:50:12.000Z" title="发表于 2025-03-23 16:50:12">2025-03-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/10038.html" title="Part I of Mathematical Structure of Mamba - Hippo"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.072333.xyz/file/AgACAgEAAyEGAASMaMWHAAJG82ff1w5esUgOIv2_I3Ec26GjClAYAALosTEbTk4BR1QAAYifvJODJAEAAwIAA3cAAzYE.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Part I of Mathematical Structure of Mamba - Hippo"/></a><div class="content"><a class="title" href="/posts/10038.html" title="Part I of Mathematical Structure of Mamba - Hippo">Part I of Mathematical Structure of Mamba - Hippo</a><time datetime="2025-03-23T08:50:11.000Z" title="发表于 2025-03-23 16:50:11">2025-03-23</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By 不会魔法的小圆</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- hexo injector body_end start --><script async src="/js/ali_font.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/"});</script></body></html>